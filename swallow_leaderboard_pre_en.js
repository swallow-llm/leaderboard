const g_models = {"Qwen/Qwen2.5-0.5B": {"active_params": 0.5, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-0.5B", "is_post": false, "model_id": "Qwen/Qwen2.5-0.5B", "name": "Qwen2.5-0.5B", "params": 0.5, "results": {"en_pre": {"BBH": 0.299, "GSM8K": 0.341, "HellaSwag": 0.399, "HumanEval": 0.277, "MATH": 0.148, "MMLU": 0.479, "OpenBookQA": 0.266, "SQuAD2": 0.501, "TriviaQA": 0.19, "XWINO": 0.768, "avg": 0.367}, "ja_pre": {"JComQA": 0.369, "JEMHopQA": 0.389, "JHumanEval": 0.203, "JMMLU": 0.304, "JSQuAD": 0.635, "MGSM": 0.076, "NIILC": 0.139, "WMT20enja": 0.058, "WMT20jaen": 0.064, "XLSum": 0.101, "avg": 0.234}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-0.5B"}, "Qwen/Qwen2.5-1.5B": {"active_params": 1.5, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-1.5B", "is_post": false, "model_id": "Qwen/Qwen2.5-1.5B", "name": "Qwen2.5-1.5B", "params": 1.5, "results": {"en_pre": {"BBH": 0.438, "GSM8K": 0.611, "HellaSwag": 0.499, "HumanEval": 0.356, "MATH": 0.314, "MMLU": 0.61, "OpenBookQA": 0.342, "SQuAD2": 0.506, "TriviaQA": 0.397, "XWINO": 0.851, "avg": 0.492}, "ja_pre": {"JComQA": 0.8, "JEMHopQA": 0.383, "JHumanEval": 0.308, "JMMLU": 0.438, "JSQuAD": 0.849, "MGSM": 0.292, "NIILC": 0.241, "WMT20enja": 0.132, "WMT20jaen": 0.134, "XLSum": 0.143, "avg": 0.372}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B"}, "Qwen/Qwen2.5-14B": {"active_params": 14, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-14B", "is_post": false, "model_id": "Qwen/Qwen2.5-14B", "name": "Qwen2.5-14B", "params": 14, "results": {"en_pre": {"BBH": 0.751, "GSM8K": 0.793, "HellaSwag": 0.642, "HumanEval": 0.544, "MATH": 0.53, "MMLU": 0.797, "OpenBookQA": 0.412, "SQuAD2": 0.63, "TriviaQA": 0.666, "XWINO": 0.899, "avg": 0.666}, "ja_pre": {"JComQA": 0.958, "JEMHopQA": 0.567, "JHumanEval": 0.55, "JMMLU": 0.69, "JSQuAD": 0.923, "MGSM": 0.74, "NIILC": 0.537, "WMT20enja": 0.26, "WMT20jaen": 0.23, "XLSum": 0.225, "avg": 0.568}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-14B"}, "Qwen/Qwen2.5-32B": {"active_params": 33, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-32B", "is_post": false, "model_id": "Qwen/Qwen2.5-32B", "name": "Qwen2.5-32B", "params": 33, "results": {"en_pre": {"BBH": 0.819, "GSM8K": 0.718, "HellaSwag": 0.656, "HumanEval": 0.523, "MATH": 0.6, "MMLU": 0.832, "OpenBookQA": 0.406, "SQuAD2": 0.668, "TriviaQA": 0.664, "XWINO": 0.913, "avg": 0.68}, "ja_pre": {"JComQA": 0.961, "JEMHopQA": 0.561, "JHumanEval": 0.637, "JMMLU": 0.751, "JSQuAD": 0.925, "MGSM": 0.808, "NIILC": 0.538, "WMT20enja": 0.271, "WMT20jaen": 0.233, "XLSum": 0.228, "avg": 0.591}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-32B"}, "Qwen/Qwen2.5-3B": {"active_params": 3.1, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-3B", "is_post": false, "model_id": "Qwen/Qwen2.5-3B", "name": "Qwen2.5-3B", "params": 3.1, "results": {"en_pre": {"BBH": 0.554, "GSM8K": 0.58, "HellaSwag": 0.553, "HumanEval": 0.387, "MATH": 0.44, "MMLU": 0.657, "OpenBookQA": 0.36, "SQuAD2": 0.541, "TriviaQA": 0.504, "XWINO": 0.872, "avg": 0.545}, "ja_pre": {"JComQA": 0.847, "JEMHopQA": 0.475, "JHumanEval": 0.404, "JMMLU": 0.529, "JSQuAD": 0.878, "MGSM": 0.46, "NIILC": 0.306, "WMT20enja": 0.18, "WMT20jaen": 0.167, "XLSum": 0.176, "avg": 0.442}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-3B"}, "Qwen/Qwen2.5-72B": {"active_params": 72, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-72B", "is_post": false, "model_id": "Qwen/Qwen2.5-72B", "name": "Qwen2.5-72B", "params": 72, "results": {"en_pre": {"BBH": 0.847, "GSM8K": 0.87, "HellaSwag": 0.685, "HumanEval": 0.554, "MATH": 0.626, "MMLU": 0.861, "OpenBookQA": 0.416, "SQuAD2": 0.693, "TriviaQA": 0.76, "XWINO": 0.901, "avg": 0.721}, "ja_pre": {"JComQA": 0.972, "JEMHopQA": 0.611, "JHumanEval": 0.648, "JMMLU": 0.804, "JSQuAD": 0.93, "MGSM": 0.828, "NIILC": 0.619, "WMT20enja": 0.287, "WMT20jaen": 0.252, "XLSum": 0.279, "avg": 0.623}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-72B"}, "Qwen/Qwen2.5-7B": {"active_params": 7.6, "base": "\uff0d", "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-7B", "is_post": false, "model_id": "Qwen/Qwen2.5-7B", "name": "Qwen2.5-7B", "params": 7.6, "results": {"en_pre": {"BBH": 0.69, "GSM8K": 0.832, "HellaSwag": 0.6, "HumanEval": 0.554, "MATH": 0.51, "MMLU": 0.742, "OpenBookQA": 0.392, "SQuAD2": 0.618, "TriviaQA": 0.601, "XWINO": 0.888, "avg": 0.643}, "ja_pre": {"JComQA": 0.924, "JEMHopQA": 0.459, "JHumanEval": 0.507, "JMMLU": 0.634, "JSQuAD": 0.907, "MGSM": 0.616, "NIILC": 0.426, "WMT20enja": 0.229, "WMT20jaen": 0.199, "XLSum": 0.216, "avg": 0.512}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-7B"}, "Qwen/Qwen3-0.6B-Base": {"active_params": 0.6, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-0.6B-Base", "is_post": false, "model_id": "Qwen/Qwen3-0.6B-Base", "name": "Qwen3-0.6B-Base", "params": 0.6, "results": {"en_pre": {"BBH": 0.407, "GSM8K": 0.483, "HellaSwag": 0.41, "HumanEval": 0.295, "MATH": 0.334, "MMLU": 0.523, "OpenBookQA": 0.268, "SQuAD2": 0.501, "TriviaQA": 0.2, "XWINO": 0.782, "avg": 0.42}, "ja_pre": {"JComQA": 0.705, "JEMHopQA": 0.344, "JHumanEval": 0.216, "JMMLU": 0.373, "JSQuAD": 0.807, "MGSM": 0.3, "NIILC": 0.175, "WMT20enja": 0.096, "WMT20jaen": 0.097, "XLSum": 0.111, "avg": 0.322}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-0.6B-Base"}, "Qwen/Qwen3-1.7B-Base": {"active_params": 1.7, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-1.7B-Base", "is_post": false, "model_id": "Qwen/Qwen3-1.7B-Base", "name": "Qwen3-1.7B-Base", "params": 1.7, "results": {"en_pre": {"BBH": 0.535, "GSM8K": 0.629, "HellaSwag": 0.493, "HumanEval": 0.462, "MATH": 0.456, "MMLU": 0.626, "OpenBookQA": 0.348, "SQuAD2": 0.504, "TriviaQA": 0.362, "XWINO": 0.849, "avg": 0.526}, "ja_pre": {"JComQA": 0.855, "JEMHopQA": 0.435, "JHumanEval": 0.35, "JMMLU": 0.501, "JSQuAD": 0.871, "MGSM": 0.472, "NIILC": 0.3, "WMT20enja": 0.16, "WMT20jaen": 0.154, "XLSum": 0.133, "avg": 0.423}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-1.7B-Base"}, "Qwen/Qwen3-14B-Base": {"active_params": 15, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-14B-Base", "is_post": false, "model_id": "Qwen/Qwen3-14B-Base", "name": "Qwen3-14B-Base", "params": 15, "results": {"en_pre": {"BBH": 0.787, "GSM8K": 0.799, "HellaSwag": 0.625, "HumanEval": 0.709, "MATH": 0.548, "MMLU": 0.806, "OpenBookQA": 0.416, "SQuAD2": 0.669, "TriviaQA": 0.657, "XWINO": 0.901, "avg": 0.692}, "ja_pre": {"JComQA": 0.956, "JEMHopQA": 0.579, "JHumanEval": 0.709, "JMMLU": 0.729, "JSQuAD": 0.921, "MGSM": 0.768, "NIILC": 0.502, "WMT20enja": 0.26, "WMT20jaen": 0.229, "XLSum": 0.261, "avg": 0.591}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-14B-Base"}, "Qwen/Qwen3-30B-A3B-Base": {"active_params": 3.3, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-30B-A3B-Base", "is_post": false, "model_id": "Qwen/Qwen3-30B-A3B-Base", "name": "Qwen3-30B-A3B-Base", "params": 31, "results": {"en_pre": {"BBH": 0.804, "GSM8K": 0.828, "HellaSwag": 0.631, "HumanEval": 0.694, "MATH": 0.634, "MMLU": 0.812, "OpenBookQA": 0.414, "SQuAD2": 0.619, "TriviaQA": 0.653, "XWINO": 0.901, "avg": 0.699}, "ja_pre": {"JComQA": 0.927, "JEMHopQA": 0.601, "JHumanEval": 0.641, "JMMLU": 0.743, "JSQuAD": 0.918, "MGSM": 0.74, "NIILC": 0.525, "WMT20enja": 0.26, "WMT20jaen": 0.224, "XLSum": 0.223, "avg": 0.58}}, "sortkey": "qwen3 a", "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Base"}, "Qwen/Qwen3-4B-Base": {"active_params": 4.0, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-4B-Base", "is_post": false, "model_id": "Qwen/Qwen3-4B-Base", "name": "Qwen3-4B-Base", "params": 4.0, "results": {"en_pre": {"BBH": 0.713, "GSM8K": 0.719, "HellaSwag": 0.555, "HumanEval": 0.617, "MATH": 0.52, "MMLU": 0.729, "OpenBookQA": 0.382, "SQuAD2": 0.588, "TriviaQA": 0.508, "XWINO": 0.891, "avg": 0.622}, "ja_pre": {"JComQA": 0.91, "JEMHopQA": 0.477, "JHumanEval": 0.537, "JMMLU": 0.649, "JSQuAD": 0.908, "MGSM": 0.644, "NIILC": 0.407, "WMT20enja": 0.214, "WMT20jaen": 0.197, "XLSum": 0.169, "avg": 0.511}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-4B-Base"}, "Qwen/Qwen3-8B-Base": {"active_params": 8.2, "base": "\uff0d", "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-8B-Base", "is_post": false, "model_id": "Qwen/Qwen3-8B-Base", "name": "Qwen3-8B-Base", "params": 8.2, "results": {"en_pre": {"BBH": 0.775, "GSM8K": 0.855, "HellaSwag": 0.594, "HumanEval": 0.669, "MATH": 0.622, "MMLU": 0.765, "OpenBookQA": 0.382, "SQuAD2": 0.602, "TriviaQA": 0.618, "XWINO": 0.903, "avg": 0.679}, "ja_pre": {"JComQA": 0.927, "JEMHopQA": 0.537, "JHumanEval": 0.595, "JMMLU": 0.689, "JSQuAD": 0.912, "MGSM": 0.716, "NIILC": 0.475, "WMT20enja": 0.241, "WMT20jaen": 0.215, "XLSum": 0.207, "avg": 0.551}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-8B-Base"}, "SakanaAI/TinySwallow-1.5B": {"active_params": 1.5, "base": "Qwen2.5-1.5B-Instruct", "date": "2025-01-30", "family": "Qwen2.5", "id": "SakanaAI/TinySwallow-1.5B", "is_post": false, "model_id": "SakanaAI/TinySwallow-1.5B", "name": "TinySwallow-1.5B", "params": 1.5, "results": {"en_pre": {"BBH": 0.376, "GSM8K": 0.379, "HellaSwag": 0.468, "HumanEval": 0.254, "MATH": 0.162, "MMLU": 0.546, "OpenBookQA": 0.308, "SQuAD2": 0.501, "TriviaQA": 0.332, "XWINO": 0.85, "avg": 0.418}, "ja_pre": {"JComQA": 0.84, "JEMHopQA": 0.437, "JHumanEval": 0.231, "JMMLU": 0.446, "JSQuAD": 0.839, "MGSM": 0.256, "NIILC": 0.474, "WMT20enja": 0.201, "WMT20jaen": 0.125, "XLSum": 0.173, "avg": 0.402}}, "sortkey": "tinyswallow", "url": "https://huggingface.co/SakanaAI/TinySwallow-1.5B"}, "google/gemma-2-27b": {"active_params": 27, "base": "\uff0d", "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-27b", "is_post": false, "model_id": "google/gemma-2-27b", "name": "Gemma 2 27B", "params": 27, "results": {"en_pre": {"BBH": 0.762, "GSM8K": 0.757, "HellaSwag": 0.675, "HumanEval": 0.508, "MATH": 0.438, "MMLU": 0.754, "OpenBookQA": 0.412, "SQuAD2": 0.549, "TriviaQA": 0.78, "XWINO": 0.921, "avg": 0.656}, "ja_pre": {"JComQA": 0.936, "JEMHopQA": 0.553, "JHumanEval": 0.49, "JMMLU": 0.659, "JSQuAD": 0.916, "MGSM": 0.596, "NIILC": 0.573, "WMT20enja": 0.295, "WMT20jaen": 0.251, "XLSum": 0.194, "avg": 0.546}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-27b"}, "google/gemma-2-2b": {"active_params": 2.6, "base": "\uff0d", "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-2b", "is_post": false, "model_id": "google/gemma-2-2b", "name": "Gemma 2 2B", "params": 2.6, "results": {"en_pre": {"BBH": 0.424, "GSM8K": 0.249, "HellaSwag": 0.552, "HumanEval": 0.188, "MATH": 0.176, "MMLU": 0.53, "OpenBookQA": 0.342, "SQuAD2": 0.501, "TriviaQA": 0.552, "XWINO": 0.89, "avg": 0.44}, "ja_pre": {"JComQA": 0.721, "JEMHopQA": 0.472, "JHumanEval": 0.177, "JMMLU": 0.388, "JSQuAD": 0.81, "MGSM": 0.124, "NIILC": 0.316, "WMT20enja": 0.203, "WMT20jaen": 0.19, "XLSum": 0.083, "avg": 0.348}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-2b"}, "google/gemma-2-9b": {"active_params": 9.2, "base": "\uff0d", "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-9b", "is_post": false, "model_id": "google/gemma-2-9b", "name": "Gemma 2 9B", "params": 9.2, "results": {"en_pre": {"BBH": 0.708, "GSM8K": 0.688, "HellaSwag": 0.626, "HumanEval": 0.39, "MATH": 0.338, "MMLU": 0.706, "OpenBookQA": 0.382, "SQuAD2": 0.506, "TriviaQA": 0.718, "XWINO": 0.907, "avg": 0.597}, "ja_pre": {"JComQA": 0.904, "JEMHopQA": 0.573, "JHumanEval": 0.345, "JMMLU": 0.623, "JSQuAD": 0.898, "MGSM": 0.456, "NIILC": 0.524, "WMT20enja": 0.269, "WMT20jaen": 0.236, "XLSum": 0.168, "avg": 0.5}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-9b"}, "google/gemma-3-12b-pt": {"active_params": 12, "base": "\uff0d", "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-12b-pt", "is_post": false, "model_id": "google/gemma-3-12b-pt", "name": "Gemma 3 12B", "params": 12, "results": {"en_pre": {"BBH": 0.681, "GSM8K": 0.703, "HellaSwag": 0.637, "HumanEval": 0.445, "MATH": 0.398, "MMLU": 0.737, "OpenBookQA": 0.398, "SQuAD2": 0.524, "TriviaQA": 0.747, "XWINO": 0.917, "avg": 0.619}, "ja_pre": {"JComQA": 0.787, "JEMHopQA": 0.563, "JHumanEval": 0.385, "JMMLU": 0.659, "JSQuAD": 0.911, "MGSM": 0.584, "NIILC": 0.569, "WMT20enja": 0.288, "WMT20jaen": 0.244, "XLSum": 0.194, "avg": 0.518}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-12b-pt"}, "google/gemma-3-1b-pt": {"active_params": 1, "base": "\uff0d", "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-1b-pt", "is_post": false, "model_id": "google/gemma-3-1b-pt", "name": "Gemma 3 1B", "params": 1, "results": {"en_pre": {"BBH": 0.271, "GSM8K": 0.016, "HellaSwag": 0.471, "HumanEval": 0.07, "MATH": 0.008, "MMLU": 0.262, "OpenBookQA": 0.304, "SQuAD2": 0.501, "TriviaQA": 0.358, "XWINO": 0.832, "avg": 0.309}, "ja_pre": {"JComQA": 0.237, "JEMHopQA": 0.41, "JHumanEval": 0.073, "JMMLU": 0.239, "JSQuAD": 0.631, "MGSM": 0.024, "NIILC": 0.252, "WMT20enja": 0.15, "WMT20jaen": 0.136, "XLSum": 0.079, "avg": 0.223}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-1b-pt"}, "google/gemma-3-27b-pt": {"active_params": 27, "base": "\uff0d", "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-27b-pt", "is_post": false, "model_id": "google/gemma-3-27b-pt", "name": "Gemma 3 27B", "params": 27, "results": {"en_pre": {"BBH": 0.711, "GSM8K": 0.801, "HellaSwag": 0.667, "HumanEval": 0.507, "MATH": 0.52, "MMLU": 0.78, "OpenBookQA": 0.414, "SQuAD2": 0.618, "TriviaQA": 0.809, "XWINO": 0.923, "avg": 0.675}, "ja_pre": {"JComQA": 0.944, "JEMHopQA": 0.582, "JHumanEval": 0.473, "JMMLU": 0.724, "JSQuAD": 0.915, "MGSM": 0.704, "NIILC": 0.627, "WMT20enja": 0.301, "WMT20jaen": 0.255, "XLSum": 0.21, "avg": 0.574}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-27b-pt"}, "google/gemma-3-4b-pt": {"active_params": 4.3, "base": "\uff0d", "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-4b-pt", "is_post": false, "model_id": "google/gemma-3-4b-pt", "name": "Gemma 3 4B", "params": 4.3, "results": {"en_pre": {"BBH": 0.495, "GSM8K": 0.376, "HellaSwag": 0.576, "HumanEval": 0.351, "MATH": 0.258, "MMLU": 0.596, "OpenBookQA": 0.36, "SQuAD2": 0.502, "TriviaQA": 0.603, "XWINO": 0.895, "avg": 0.501}, "ja_pre": {"JComQA": 0.851, "JEMHopQA": 0.432, "JHumanEval": 0.273, "JMMLU": 0.499, "JSQuAD": 0.887, "MGSM": 0.248, "NIILC": 0.41, "WMT20enja": 0.23, "WMT20jaen": 0.205, "XLSum": 0.139, "avg": 0.417}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-4b-pt"}, "llm-jp/llm-jp-3-1.8b": {"active_params": 1.8, "base": "\uff0d", "date": "2024-09-25", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3-1.8b", "is_post": false, "model_id": "llm-jp/llm-jp-3-1.8b", "name": "llm-jp-3-1.8b", "params": 1.8, "results": {"en_pre": {"BBH": 0.274, "GSM8K": 0.017, "HellaSwag": 0.462, "HumanEval": 0.008, "MATH": 0.018, "MMLU": 0.248, "OpenBookQA": 0.244, "SQuAD2": 0.501, "TriviaQA": 0.301, "XWINO": 0.851, "avg": 0.292}, "ja_pre": {"JComQA": 0.209, "JEMHopQA": 0.463, "JHumanEval": 0.001, "JMMLU": 0.242, "JSQuAD": 0.703, "MGSM": 0.012, "NIILC": 0.449, "WMT20enja": 0.198, "WMT20jaen": 0.134, "XLSum": 0.1, "avg": 0.251}}, "sortkey": "llm jp 3", "url": "https://huggingface.co/llm-jp/llm-jp-3-1.8b"}, "llm-jp/llm-jp-3-13b": {"active_params": 13, "base": "\uff0d", "date": "2024-09-25", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3-13b", "is_post": false, "model_id": "llm-jp/llm-jp-3-13b", "name": "llm-jp-3-13b", "params": 13, "results": {"en_pre": {"BBH": 0.39, "GSM8K": 0.158, "HellaSwag": 0.57, "HumanEval": 0.032, "MATH": 0.026, "MMLU": 0.462, "OpenBookQA": 0.332, "SQuAD2": 0.501, "TriviaQA": 0.602, "XWINO": 0.902, "avg": 0.397}, "ja_pre": {"JComQA": 0.65, "JEMHopQA": 0.525, "JHumanEval": 0.023, "JMMLU": 0.399, "JSQuAD": 0.882, "MGSM": 0.16, "NIILC": 0.649, "WMT20enja": 0.273, "WMT20jaen": 0.21, "XLSum": 0.164, "avg": 0.393}}, "sortkey": "llm jp 3", "url": "https://huggingface.co/llm-jp/llm-jp-3-13b"}, "llm-jp/llm-jp-3-3.7b": {"active_params": 3.7, "base": "\uff0d", "date": "2024-09-25", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3-3.7b", "is_post": false, "model_id": "llm-jp/llm-jp-3-3.7b", "name": "llm-jp-3-3.7b", "params": 3.7, "results": {"en_pre": {"BBH": 0.309, "GSM8K": 0.055, "HellaSwag": 0.506, "HumanEval": 0.019, "MATH": 0.016, "MMLU": 0.253, "OpenBookQA": 0.28, "SQuAD2": 0.502, "TriviaQA": 0.421, "XWINO": 0.876, "avg": 0.324}, "ja_pre": {"JComQA": 0.203, "JEMHopQA": 0.431, "JHumanEval": 0.0, "JMMLU": 0.249, "JSQuAD": 0.804, "MGSM": 0.06, "NIILC": 0.541, "WMT20enja": 0.223, "WMT20jaen": 0.159, "XLSum": 0.142, "avg": 0.281}}, "sortkey": "llm jp 3", "url": "https://huggingface.co/llm-jp/llm-jp-3-3.7b"}, "llm-jp/llm-jp-3-7.2b": {"active_params": 7.3, "base": "\uff0d", "date": "2025-02-05", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3-7.2b", "is_post": false, "model_id": "llm-jp/llm-jp-3-7.2b", "name": "llm-jp-3-7.2b", "params": 7.3, "results": {"en_pre": {"BBH": 0.361, "GSM8K": 0.086, "HellaSwag": 0.544, "HumanEval": 0.02, "MATH": 0.022, "MMLU": 0.373, "OpenBookQA": 0.312, "SQuAD2": 0.501, "TriviaQA": 0.522, "XWINO": 0.888, "avg": 0.363}, "ja_pre": {"JComQA": 0.509, "JEMHopQA": 0.481, "JHumanEval": 0.021, "JMMLU": 0.344, "JSQuAD": 0.863, "MGSM": 0.088, "NIILC": 0.601, "WMT20enja": 0.249, "WMT20jaen": 0.19, "XLSum": 0.152, "avg": 0.35}}, "sortkey": "llm jp 3", "url": "https://huggingface.co/llm-jp/llm-jp-3-7.2b"}, "meta-llama/Llama-3.2-1B": {"active_params": 1.2, "base": "\uff0d", "date": "2024-09-25", "family": "Llama 3.2", "id": "meta-llama/Llama-3.2-1B", "is_post": false, "model_id": "meta-llama/Llama-3.2-1B", "name": "Llama 3.2 1B", "params": 1.2, "results": {"en_pre": {"BBH": 0.309, "GSM8K": 0.049, "HellaSwag": 0.477, "HumanEval": 0.193, "MATH": 0.02, "MMLU": 0.313, "OpenBookQA": 0.3, "SQuAD2": 0.501, "TriviaQA": 0.388, "XWINO": 0.849, "avg": 0.34}, "ja_pre": {"JComQA": 0.208, "JEMHopQA": 0.404, "JHumanEval": 0.15, "JMMLU": 0.26, "JSQuAD": 0.525, "MGSM": 0.024, "NIILC": 0.188, "WMT20enja": 0.079, "WMT20jaen": 0.092, "XLSum": 0.081, "avg": 0.201}}, "sortkey": "llama 3.2", "url": "https://huggingface.co/meta-llama/Llama-3.2-1B"}, "meta-llama/Llama-3.2-3B": {"active_params": 3.2, "base": "\uff0d", "date": "2024-09-25", "family": "Llama 3.2", "id": "meta-llama/Llama-3.2-3B", "is_post": false, "model_id": "meta-llama/Llama-3.2-3B", "name": "Llama 3.2 3B", "params": 3.2, "results": {"en_pre": {"BBH": 0.467, "GSM8K": 0.262, "HellaSwag": 0.558, "HumanEval": 0.285, "MATH": 0.07, "MMLU": 0.558, "OpenBookQA": 0.326, "SQuAD2": 0.502, "TriviaQA": 0.586, "XWINO": 0.888, "avg": 0.45}, "ja_pre": {"JComQA": 0.605, "JEMHopQA": 0.443, "JHumanEval": 0.235, "JMMLU": 0.352, "JSQuAD": 0.816, "MGSM": 0.136, "NIILC": 0.324, "WMT20enja": 0.161, "WMT20jaen": 0.167, "XLSum": 0.129, "avg": 0.337}}, "sortkey": "llama 3.2", "url": "https://huggingface.co/meta-llama/Llama-3.2-3B"}, "meta-llama/Llama-4-Scout-17B-16E": {"active_params": 17, "base": "\uff0d", "date": "2025-04-04", "family": "Llama 4", "id": "meta-llama/Llama-4-Scout-17B-16E", "is_post": false, "model_id": "meta-llama/Llama-4-Scout-17B-16E", "name": "Llama 4 Scout", "params": 109, "results": {"en_pre": {"BBH": 0.289, "GSM8K": 0.811, "HellaSwag": 0.689, "HumanEval": 0.359, "MATH": 0.522, "MMLU": 0.78, "OpenBookQA": 0.432, "SQuAD2": 0.548, "TriviaQA": 0.75, "XWINO": 0.883, "avg": 0.606}, "ja_pre": {"JComQA": 0.958, "JEMHopQA": 0.595, "JHumanEval": 0.33, "JMMLU": 0.736, "JSQuAD": 0.915, "MGSM": 0.76, "NIILC": 0.616, "WMT20enja": 0.3, "WMT20jaen": 0.258, "XLSum": 0.178, "avg": 0.565}}, "sortkey": "llama 4 scout", "url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E"}, "meta-llama/Meta-Llama-3.1-70B": {"active_params": 70, "base": "\uff0d", "date": "2024-07-23", "family": "Llama 3.1", "id": "meta-llama/Meta-Llama-3.1-70B", "is_post": false, "model_id": "meta-llama/Meta-Llama-3.1-70B", "name": "Llama 3.1 70B", "params": 70, "results": {"en_pre": {"BBH": 0.807, "GSM8K": 0.798, "HellaSwag": 0.69, "HumanEval": 0.546, "MATH": 0.434, "MMLU": 0.786, "OpenBookQA": 0.45, "SQuAD2": 0.605, "TriviaQA": 0.829, "XWINO": 0.92, "avg": 0.687}, "ja_pre": {"JComQA": 0.946, "JEMHopQA": 0.616, "JHumanEval": 0.462, "JMMLU": 0.669, "JSQuAD": 0.925, "MGSM": 0.672, "NIILC": 0.603, "WMT20enja": 0.287, "WMT20jaen": 0.257, "XLSum": 0.228, "avg": 0.566}}, "sortkey": "llama 3.1", "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B"}, "meta-llama/Meta-Llama-3.1-8B": {"active_params": 8.0, "base": "\uff0d", "date": "2024-07-23", "family": "Llama 3.1", "id": "meta-llama/Meta-Llama-3.1-8B", "is_post": false, "model_id": "meta-llama/Meta-Llama-3.1-8B", "name": "Llama 3.1 8B", "params": 8.0, "results": {"en_pre": {"BBH": 0.626, "GSM8K": 0.507, "HellaSwag": 0.609, "HumanEval": 0.364, "MATH": 0.214, "MMLU": 0.651, "OpenBookQA": 0.38, "SQuAD2": 0.503, "TriviaQA": 0.702, "XWINO": 0.907, "avg": 0.546}, "ja_pre": {"JComQA": 0.845, "JEMHopQA": 0.461, "JHumanEval": 0.32, "JMMLU": 0.479, "JSQuAD": 0.895, "MGSM": 0.356, "NIILC": 0.405, "WMT20enja": 0.221, "WMT20jaen": 0.21, "XLSum": 0.179, "avg": 0.437}}, "sortkey": "llama 3.1", "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B"}, "pfnet/plamo-2-1b": {"active_params": 1.3, "base": "\uff0d", "date": "2025-02-21", "family": "PLaMo 2", "id": "pfnet/plamo-2-1b", "is_post": false, "model_id": "pfnet/plamo-2-1b", "name": "PLaMo 2 1B", "params": 1.3, "results": {"en_pre": {"BBH": 0.202, "GSM8K": 0.072, "HellaSwag": 0.425, "HumanEval": 0.08, "MATH": 0.034, "MMLU": 0.294, "OpenBookQA": 0.28, "SQuAD2": 0.501, "TriviaQA": 0.129, "XWINO": 0.807, "avg": 0.282}, "ja_pre": {"JComQA": 0.203, "JEMHopQA": 0.463, "JHumanEval": 0.057, "JMMLU": 0.256, "JSQuAD": 0.626, "MGSM": 0.052, "NIILC": 0.434, "WMT20enja": 0.236, "WMT20jaen": 0.119, "XLSum": 0.055, "avg": 0.25}}, "sortkey": "plamo 2", "url": "https://huggingface.co/pfnet/plamo-2-1b"}, "pfnet/plamo-2-8b": {"active_params": 9.1, "base": "\uff0d", "date": "2025-02-21", "family": "PLaMo 2", "id": "pfnet/plamo-2-8b", "is_post": false, "model_id": "pfnet/plamo-2-8b", "name": "PLaMo 2 8B", "params": 9.1, "results": {"en_pre": {"BBH": 0.387, "GSM8K": 0.55, "HellaSwag": 0.56, "HumanEval": 0.26, "MATH": 0.2, "MMLU": 0.575, "OpenBookQA": 0.346, "SQuAD2": 0.511, "TriviaQA": 0.584, "XWINO": 0.89, "avg": 0.486}, "ja_pre": {"JComQA": 0.909, "JEMHopQA": 0.474, "JHumanEval": 0.213, "JMMLU": 0.536, "JSQuAD": 0.91, "MGSM": 0.508, "NIILC": 0.655, "WMT20enja": 0.28, "WMT20jaen": 0.205, "XLSum": 0.12, "avg": 0.481}}, "sortkey": "plamo 2", "url": "https://huggingface.co/pfnet/plamo-2-8b"}, "pfnet/plamo-3-nict-2b-base": {"active_params": 2.6, "base": "\uff0d", "date": "2025-11-14", "family": "PLaMo 3", "id": "pfnet/plamo-3-nict-2b-base", "is_post": false, "model_id": "pfnet/plamo-3-nict-2b-base", "name": "PLaMo 3 NICT 2B Base", "params": 2.6, "results": {"en_pre": {"BBH": 0.331, "GSM8K": 0.193, "HellaSwag": 0.443, "HumanEval": 0.251, "MATH": 0.14, "MMLU": 0.417, "OpenBookQA": 0.298, "SQuAD2": 0.531, "TriviaQA": 0.296, "XWINO": 0.812, "avg": 0.371}, "ja_pre": {"JComQA": 0.617, "JEMHopQA": 0.447, "JHumanEval": 0.295, "JMMLU": 0.391, "JSQuAD": 0.866, "MGSM": 0.188, "NIILC": 0.548, "WMT20enja": 0.272, "WMT20jaen": 0.166, "XLSum": 0.084, "avg": 0.387}}, "sortkey": "plamo 3 nict", "url": "https://huggingface.co/pfnet/plamo-3-nict-2b-base"}, "pfnet/plamo-3-nict-31b-base": {"active_params": 32, "base": "\uff0d", "date": "2025-11-14", "family": "PLaMo 3", "id": "pfnet/plamo-3-nict-31b-base", "is_post": false, "model_id": "pfnet/plamo-3-nict-31b-base", "name": "PLaMo 3 NICT 31B Base", "params": 32, "results": {"en_pre": {"BBH": 0.769, "GSM8K": 0.782, "HellaSwag": 0.628, "HumanEval": 0.29, "MATH": 0.45, "MMLU": 0.733, "OpenBookQA": 0.382, "SQuAD2": 0.728, "TriviaQA": 0.668, "XWINO": 0.888, "avg": 0.632}, "ja_pre": {"JComQA": 0.955, "JEMHopQA": 0.625, "JHumanEval": 0.507, "JMMLU": 0.679, "JSQuAD": 0.936, "MGSM": 0.68, "NIILC": 0.694, "WMT20enja": 0.313, "WMT20jaen": 0.236, "XLSum": 0.201, "avg": 0.583}}, "sortkey": "plamo 3 nict", "url": "https://huggingface.co/pfnet/plamo-3-nict-31b-base"}, "pfnet/plamo-3-nict-8b-base": {"active_params": 8.1, "base": "\uff0d", "date": "2025-11-14", "family": "PLaMo 3", "id": "pfnet/plamo-3-nict-8b-base", "is_post": false, "model_id": "pfnet/plamo-3-nict-8b-base", "name": "PLaMo 3 NICT 8B Base", "params": 8.1, "results": {"en_pre": {"BBH": 0.568, "GSM8K": 0.534, "HellaSwag": 0.558, "HumanEval": 0.295, "MATH": 0.308, "MMLU": 0.598, "OpenBookQA": 0.344, "SQuAD2": 0.645, "TriviaQA": 0.553, "XWINO": 0.871, "avg": 0.527}, "ja_pre": {"JComQA": 0.825, "JEMHopQA": 0.524, "JHumanEval": 0.384, "JMMLU": 0.573, "JSQuAD": 0.926, "MGSM": 0.436, "NIILC": 0.645, "WMT20enja": 0.301, "WMT20jaen": 0.21, "XLSum": 0.126, "avg": 0.495}}, "sortkey": "plamo 3 nict", "url": "https://huggingface.co/pfnet/plamo-3-nict-8b-base"}, "sbintuitions/sarashina2-13b": {"active_params": 13, "base": "\uff0d", "date": "2024-06-14", "family": "Sarashina2", "id": "sbintuitions/sarashina2-13b", "is_post": false, "model_id": "sbintuitions/sarashina2-13b", "name": "Sarashina2-13B", "params": 13, "results": {"en_pre": {"BBH": 0.422, "GSM8K": 0.158, "HellaSwag": 0.562, "HumanEval": 0.198, "MATH": 0.036, "MMLU": 0.496, "OpenBookQA": 0.34, "SQuAD2": 0.501, "TriviaQA": 0.548, "XWINO": 0.896, "avg": 0.416}, "ja_pre": {"JComQA": 0.85, "JEMHopQA": 0.557, "JHumanEval": 0.161, "JMMLU": 0.473, "JSQuAD": 0.898, "MGSM": 0.188, "NIILC": 0.661, "WMT20enja": 0.284, "WMT20jaen": 0.221, "XLSum": 0.158, "avg": 0.445}}, "sortkey": "sarashina2", "url": "https://huggingface.co/sbintuitions/sarashina2-13b"}, "sbintuitions/sarashina2-70b": {"active_params": 70, "base": "\uff0d", "date": "2024-06-14", "family": "Sarashina2", "id": "sbintuitions/sarashina2-70b", "is_post": false, "model_id": "sbintuitions/sarashina2-70b", "name": "Sarashina2-70B", "params": 70, "results": {"en_pre": {"BBH": 0.643, "GSM8K": 0.011, "HellaSwag": 0.628, "HumanEval": 0.281, "MATH": 0.206, "MMLU": 0.63, "OpenBookQA": 0.388, "SQuAD2": 0.675, "TriviaQA": 0.537, "XWINO": 0.917, "avg": 0.492}, "ja_pre": {"JComQA": 0.929, "JEMHopQA": 0.717, "JHumanEval": 0.235, "JMMLU": 0.592, "JSQuAD": 0.929, "MGSM": 0.488, "NIILC": 0.668, "WMT20enja": 0.313, "WMT20jaen": 0.243, "XLSum": 0.19, "avg": 0.53}}, "sortkey": "sarashina2", "url": "https://huggingface.co/sbintuitions/sarashina2-70b"}, "sbintuitions/sarashina2-7b": {"active_params": 7.3, "base": "\uff0d", "date": "2024-06-14", "family": "Sarashina2", "id": "sbintuitions/sarashina2-7b", "is_post": false, "model_id": "sbintuitions/sarashina2-7b", "name": "Sarashina2-7B", "params": 7.3, "results": {"en_pre": {"BBH": 0.349, "GSM8K": 0.101, "HellaSwag": 0.532, "HumanEval": 0.146, "MATH": 0.034, "MMLU": 0.425, "OpenBookQA": 0.346, "SQuAD2": 0.501, "TriviaQA": 0.479, "XWINO": 0.892, "avg": 0.381}, "ja_pre": {"JComQA": 0.742, "JEMHopQA": 0.509, "JHumanEval": 0.121, "JMMLU": 0.384, "JSQuAD": 0.868, "MGSM": 0.08, "NIILC": 0.634, "WMT20enja": 0.273, "WMT20jaen": 0.201, "XLSum": 0.141, "avg": 0.395}}, "sortkey": "sarashina2", "url": "https://huggingface.co/sbintuitions/sarashina2-7b"}, "sbintuitions/sarashina2.2-0.5b": {"active_params": 0.8, "base": "\uff0d", "date": "2025-03-07", "family": "Sarashina2", "id": "sbintuitions/sarashina2.2-0.5b", "is_post": false, "model_id": "sbintuitions/sarashina2.2-0.5b", "name": "Sarashina2.2 0.5B", "params": 0.8, "results": {"en_pre": {"BBH": 0.305, "GSM8K": 0.246, "HellaSwag": 0.42, "HumanEval": 0.223, "MATH": 0.13, "MMLU": 0.262, "OpenBookQA": 0.302, "SQuAD2": 0.501, "TriviaQA": 0.203, "XWINO": 0.794, "avg": 0.339}, "ja_pre": {"JComQA": 0.211, "JEMHopQA": 0.472, "JHumanEval": 0.148, "JMMLU": 0.253, "JSQuAD": 0.824, "MGSM": 0.196, "NIILC": 0.451, "WMT20enja": 0.201, "WMT20jaen": 0.111, "XLSum": 0.091, "avg": 0.296}}, "sortkey": "sarashina2.2", "url": "https://huggingface.co/sbintuitions/sarashina2.2-0.5b"}, "sbintuitions/sarashina2.2-1b": {"active_params": 1.4, "base": "\uff0d", "date": "2025-03-07", "family": "Sarashina2", "id": "sbintuitions/sarashina2.2-1b", "is_post": false, "model_id": "sbintuitions/sarashina2.2-1b", "name": "Sarashina2.2 1B", "params": 1.4, "results": {"en_pre": {"BBH": 0.376, "GSM8K": 0.403, "HellaSwag": 0.469, "HumanEval": 0.342, "MATH": 0.206, "MMLU": 0.4, "OpenBookQA": 0.324, "SQuAD2": 0.502, "TriviaQA": 0.289, "XWINO": 0.827, "avg": 0.414}, "ja_pre": {"JComQA": 0.649, "JEMHopQA": 0.462, "JHumanEval": 0.215, "JMMLU": 0.371, "JSQuAD": 0.858, "MGSM": 0.388, "NIILC": 0.523, "WMT20enja": 0.219, "WMT20jaen": 0.136, "XLSum": 0.1, "avg": 0.392}}, "sortkey": "sarashina2.2", "url": "https://huggingface.co/sbintuitions/sarashina2.2-1b"}, "sbintuitions/sarashina2.2-3b": {"active_params": 3.4, "base": "\uff0d", "date": "2025-03-07", "family": "Sarashina2", "id": "sbintuitions/sarashina2.2-3b", "is_post": false, "model_id": "sbintuitions/sarashina2.2-3b", "name": "Sarashina2.2 3B", "params": 3.4, "results": {"en_pre": {"BBH": 0.543, "GSM8K": 0.624, "HellaSwag": 0.538, "HumanEval": 0.53, "MATH": 0.31, "MMLU": 0.572, "OpenBookQA": 0.362, "SQuAD2": 0.513, "TriviaQA": 0.447, "XWINO": 0.877, "avg": 0.532}, "ja_pre": {"JComQA": 0.911, "JEMHopQA": 0.563, "JHumanEval": 0.36, "JMMLU": 0.541, "JSQuAD": 0.906, "MGSM": 0.596, "NIILC": 0.642, "WMT20enja": 0.273, "WMT20jaen": 0.202, "XLSum": 0.162, "avg": 0.516}}, "sortkey": "sarashina2.2", "url": "https://huggingface.co/sbintuitions/sarashina2.2-3b"}, "tiiuae/Falcon3-10B-Base": {"active_params": 10, "base": "Falcon3-7B-Base", "date": "2024-12-19", "family": "Falcon3", "id": "tiiuae/Falcon3-10B-Base", "is_post": false, "model_id": "tiiuae/Falcon3-10B-Base", "name": "Falcon3-10B-Base", "params": 10, "results": {"en_pre": {"BBH": 0.762, "GSM8K": 0.802, "HellaSwag": 0.596, "HumanEval": 0.543, "MATH": 0.492, "MMLU": 0.732, "OpenBookQA": 0.368, "SQuAD2": 0.603, "TriviaQA": 0.579, "XWINO": 0.901, "avg": 0.638}, "ja_pre": {"JComQA": 0.68, "JEMHopQA": 0.443, "JHumanEval": 0.426, "JMMLU": 0.435, "JSQuAD": 0.854, "MGSM": 0.376, "NIILC": 0.187, "WMT20enja": 0.103, "WMT20jaen": 0.139, "XLSum": 0.187, "avg": 0.383}}, "sortkey": "falcon3", "url": "https://huggingface.co/tiiuae/Falcon3-10B-Base"}, "tiiuae/Falcon3-1B-Base": {"active_params": 1.7, "base": "Falcon3-3B-Base", "date": "2024-12-19", "family": "Falcon3", "id": "tiiuae/Falcon3-1B-Base", "is_post": false, "model_id": "tiiuae/Falcon3-1B-Base", "name": "Falcon3-1B-Base", "params": 1.7, "results": {"en_pre": {"BBH": 0.311, "GSM8K": 0.337, "HellaSwag": 0.458, "HumanEval": 0.125, "MATH": 0.14, "MMLU": 0.449, "OpenBookQA": 0.316, "SQuAD2": 0.501, "TriviaQA": 0.296, "XWINO": 0.816, "avg": 0.375}, "ja_pre": {"JComQA": 0.216, "JEMHopQA": 0.251, "JHumanEval": 0.088, "JMMLU": 0.264, "JSQuAD": 0.281, "MGSM": 0.008, "NIILC": 0.062, "WMT20enja": 0.012, "WMT20jaen": 0.02, "XLSum": 0.085, "avg": 0.129}}, "sortkey": "falcon3", "url": "https://huggingface.co/tiiuae/Falcon3-1B-Base"}, "tiiuae/Falcon3-3B-Base": {"active_params": 3.2, "base": "Falcon3-7B-Base", "date": "2024-12-19", "family": "Falcon3", "id": "tiiuae/Falcon3-3B-Base", "is_post": false, "model_id": "tiiuae/Falcon3-3B-Base", "name": "Falcon3-3B-Base", "params": 3.2, "results": {"en_pre": {"BBH": 0.547, "GSM8K": 0.634, "HellaSwag": 0.492, "HumanEval": 0.348, "MATH": 0.344, "MMLU": 0.567, "OpenBookQA": 0.312, "SQuAD2": 0.503, "TriviaQA": 0.346, "XWINO": 0.847, "avg": 0.494}, "ja_pre": {"JComQA": 0.281, "JEMHopQA": 0.333, "JHumanEval": 0.229, "JMMLU": 0.319, "JSQuAD": 0.517, "MGSM": 0.096, "NIILC": 0.113, "WMT20enja": 0.031, "WMT20jaen": 0.051, "XLSum": 0.12, "avg": 0.209}}, "sortkey": "falcon3", "url": "https://huggingface.co/tiiuae/Falcon3-3B-Base"}, "tiiuae/Falcon3-7B-Base": {"active_params": 7.5, "base": "\uff0d", "date": "2024-12-19", "family": "Falcon3", "id": "tiiuae/Falcon3-7B-Base", "is_post": false, "model_id": "tiiuae/Falcon3-7B-Base", "name": "Falcon3-7B-Base", "params": 7.5, "results": {"en_pre": {"BBH": 0.673, "GSM8K": 0.766, "HellaSwag": 0.566, "HumanEval": 0.476, "MATH": 0.438, "MMLU": 0.701, "OpenBookQA": 0.354, "SQuAD2": 0.539, "TriviaQA": 0.552, "XWINO": 0.881, "avg": 0.595}, "ja_pre": {"JComQA": 0.634, "JEMHopQA": 0.412, "JHumanEval": 0.361, "JMMLU": 0.385, "JSQuAD": 0.788, "MGSM": 0.244, "NIILC": 0.18, "WMT20enja": 0.078, "WMT20jaen": 0.119, "XLSum": 0.173, "avg": 0.337}}, "sortkey": "falcon3", "url": "https://huggingface.co/tiiuae/Falcon3-7B-Base"}, "tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1": {"active_params": 27, "base": "Gemma 2 27B", "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1", "is_post": false, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1", "name": "Gemma-2-Llama Swallow 27B", "params": 27, "results": {"en_pre": {"BBH": 0.766, "GSM8K": 0.732, "HellaSwag": 0.652, "HumanEval": 0.658, "MATH": 0.416, "MMLU": 0.749, "OpenBookQA": 0.414, "SQuAD2": 0.597, "TriviaQA": 0.756, "XWINO": 0.915, "avg": 0.665}, "ja_pre": {"JComQA": 0.958, "JEMHopQA": 0.66, "JHumanEval": 0.629, "JMMLU": 0.679, "JSQuAD": 0.924, "MGSM": 0.644, "NIILC": 0.671, "WMT20enja": 0.321, "WMT20jaen": 0.255, "XLSum": 0.2, "avg": 0.594}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1"}, "tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1": {"active_params": 2.6, "base": "\uff0d", "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1", "is_post": false, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1", "name": "Gemma-2-Llama Swallow 2B", "params": 2.6, "results": {"en_pre": {"BBH": 0.388, "GSM8K": 0.275, "HellaSwag": 0.516, "HumanEval": 0.286, "MATH": 0.144, "MMLU": 0.538, "OpenBookQA": 0.312, "SQuAD2": 0.501, "TriviaQA": 0.435, "XWINO": 0.871, "avg": 0.427}, "ja_pre": {"JComQA": 0.83, "JEMHopQA": 0.509, "JHumanEval": 0.251, "JMMLU": 0.461, "JSQuAD": 0.863, "MGSM": 0.172, "NIILC": 0.549, "WMT20enja": 0.261, "WMT20jaen": 0.195, "XLSum": 0.119, "avg": 0.421}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1"}, "tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1": {"active_params": 9.2, "base": "Gemma 2 9B", "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1", "is_post": false, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1", "name": "Gemma-2-Llama Swallow 9B", "params": 9.2, "results": {"en_pre": {"BBH": 0.669, "GSM8K": 0.678, "HellaSwag": 0.602, "HumanEval": 0.529, "MATH": 0.33, "MMLU": 0.687, "OpenBookQA": 0.362, "SQuAD2": 0.532, "TriviaQA": 0.659, "XWINO": 0.906, "avg": 0.595}, "ja_pre": {"JComQA": 0.95, "JEMHopQA": 0.643, "JHumanEval": 0.462, "JMMLU": 0.65, "JSQuAD": 0.897, "MGSM": 0.56, "NIILC": 0.677, "WMT20enja": 0.304, "WMT20jaen": 0.247, "XLSum": 0.187, "avg": 0.558}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1"}, "tokyotech-llm/Llama-3.3-Swallow-70B-v0.4": {"active_params": 70, "base": "Llama 3.3 70B Instruct", "date": "2025-03-14", "family": "Llama 3.1", "id": "tokyotech-llm/Llama-3.3-Swallow-70B-v0.4", "is_post": false, "model_id": "tokyotech-llm/Llama-3.3-Swallow-70B-v0.4", "name": "Llama 3.3 Swallow 70B v0.4", "params": 70, "results": {"en_pre": {"BBH": 0.84, "GSM8K": 0.863, "HellaSwag": 0.683, "HumanEval": 0.709, "MATH": 0.496, "MMLU": 0.802, "OpenBookQA": 0.424, "SQuAD2": 0.641, "TriviaQA": 0.817, "XWINO": 0.92, "avg": 0.719}, "ja_pre": {"JComQA": 0.967, "JEMHopQA": 0.671, "JHumanEval": 0.604, "JMMLU": 0.742, "JSQuAD": 0.924, "MGSM": 0.776, "NIILC": 0.732, "WMT20enja": 0.327, "WMT20jaen": 0.26, "XLSum": 0.283, "avg": 0.629}}, "sortkey": "llama 3.3 swallow v0.4", "url": "https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4"}, "tokyotech-llm/Llama-3.3-Swallow-8B-v0.5": {"active_params": 8.0, "base": "Llama 3.1 8B", "date": "2025-06-25", "family": "Llama 3.1", "id": "tokyotech-llm/Llama-3.3-Swallow-8B-v0.5", "is_post": false, "model_id": "tokyotech-llm/Llama-3.3-Swallow-8B-v0.5", "name": "Llama 3.1 Swallow 8B v0.5", "params": 8.0, "results": {"en_pre": {"BBH": 0.666, "GSM8K": 0.699, "HellaSwag": 0.597, "HumanEval": 0.557, "MATH": 0.39, "MMLU": 0.666, "OpenBookQA": 0.372, "SQuAD2": 0.536, "TriviaQA": 0.665, "XWINO": 0.9, "avg": 0.605}, "ja_pre": {"JComQA": 0.952, "JEMHopQA": 0.513, "JHumanEval": 0.491, "JMMLU": 0.59, "JSQuAD": 0.91, "MGSM": 0.572, "NIILC": 0.657, "WMT20enja": 0.294, "WMT20jaen": 0.232, "XLSum": 0.217, "avg": 0.543}}, "sortkey": "llama 3.1 swallow v0.5", "url": "https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-8B-v0.5"}};
const g_tasks = {"en_pre": {"description": "This benchmark evaluates pre-trained LLMs models (without post-training) on English benchmark datasets. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["pre"], "taskdict": {"BBH": {"category": "Collection of hard-to-solve tasks for LLM", "description": "23 tasks that are difficult in BIG-Bench dataset (Srivastava et al., 2023)", "field": "bbh_cot", "input": "3-shot, CoT", "link": {"author": "Suzgun et al.", "href": "https://aclanthology.org/2023.findings-acl.824/", "year": "2023"}, "metric": "Accuracy (exact match)", "name": "BBH", "short": "BBH", "title": "BIG-Bench-Hard (BBH)"}, "GSM8K": {"category": "Mathematics", "description": "Math word problems", "field": "gsm8k", "input": "4-shot", "link": {"author": "Cobbe et al.", "href": "https://arxiv.org/abs/2110.14168", "year": "2021"}, "metric": "Accuracy (exact match)", "name": "GSM8K", "short": "GSM8K", "title": "GSM8K"}, "HellaSwag": {"category": "Commonsense inference", "description": "Four-choice questions to predict the next event", "field": "hellaswag", "input": "4-shot", "link": {"author": "Zellers et al.", "href": "https://aclanthology.org/P19-1472/", "year": "2019"}, "metric": "Accuracy", "name": "HellaSwag", "short": "HellaSwag", "title": "HellaSwag"}, "HumanEval": {"category": "Code generation", "description": "Ability of code generation measured by unit test", "field": "humaneval-unstripped@1", "input": "0-shot, 10 trials", "link": {"author": "Chen et al.", "href": "https://arxiv.org/abs/2107.03374", "year": "2021"}, "metric": "pass@1", "name": "HumanEval", "short": "HumanEval", "title": "HumanEval"}, "MATH": {"category": "Mathematics", "description": "High school math competitions", "field": "minerva_math-cot-maj1@1", "input": "4-shot", "link": {"author": "Hendrycks et al.", "href": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html", "year": "2021"}, "metric": "Accuracy (exact match)", "name": "MATH", "short": "MATH", "title": "MATH"}, "MMLU": {"category": "Multitask natural language understanding", "description": "Four-choice exam questions benchmark MMLU (53 subjects)", "field": "mmlu", "input": "5-shot", "link": {"author": "Hendrycks et al.", "href": "https://openreview.net/forum?id=d7KBjmI3GmQ", "year": "2021"}, "metric": "Accuracy", "name": "MMLU", "short": "MMLU", "title": "MMLU"}, "OpenBookQA": {"category": "Q\u0026A based on facts and common sense", "description": "Four-choice questions based on scientific knowledge and common sense", "field": "OpenBookQA", "input": "4-shot", "link": {"author": "Mihaylov et al.", "href": "https://aclanthology.org/D18-1260/", "year": "2018"}, "metric": "Accuracy", "name": "OpenBookQA", "short": "OpenBookQA", "title": "OpenBookQA"}, "SQuAD2": {"category": "Reading comprehension", "description": "Open-ended Q\u0026A developed for the evidence document", "field": "squad2_best_exact", "input": "4-shot", "link": {"author": "Rajpurkar et al.", "href": "https://aclanthology.org/P18-2124/", "year": "2018"}, "metric": "Accuracy (exact match)", "name": "SQuAD2", "short": "SQuAD2", "title": "SQuAD2"}, "TriviaQA": {"category": "Q\u0026A based on knowledge", "description": "Open-ended Q\u0026A based on trivias", "field": "triviaqa", "input": "4-shot", "link": {"author": "Joshi et al.", "href": "https://aclanthology.org/P17-1147/", "year": "2017"}, "metric": "Accuracy (exact match)", "name": "TriviaQA", "short": "TriviaQA", "title": "TriviaQA"}, "XWINO": {"category": "Commonsense inference", "description": "Two-choice question to predict the antecedent of a pronoun", "field": "xwinograd_en", "input": "4-shot", "link": {"author": "Tikhonov and Ryabinin", "href": "https://aclanthology.org/2021.findings-acl.310/", "year": "2021"}, "metric": "Accuracy", "name": "XWINO", "short": "XWINO", "title": "XWINO"}, "avg": {"category": "Average", "collective": true, "description": "Average score of English datasets for pre-trained models", "field": "en_pre_avg", "metric": "Average", "name": "avg", "short": "Pre (en) avg", "title": "Average"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of English datasets for pre-trained models", "field": "en_pre_avg", "metric": "Average", "name": "avg", "short": "Pre (en) avg", "title": "Average"}, {"category": "Q\u0026A based on facts and common sense", "description": "Four-choice questions based on scientific knowledge and common sense", "field": "OpenBookQA", "input": "4-shot", "link": {"author": "Mihaylov et al.", "href": "https://aclanthology.org/D18-1260/", "year": "2018"}, "metric": "Accuracy", "name": "OpenBookQA", "short": "OpenBookQA", "title": "OpenBookQA"}, {"category": "Q\u0026A based on knowledge", "description": "Open-ended Q\u0026A based on trivias", "field": "triviaqa", "input": "4-shot", "link": {"author": "Joshi et al.", "href": "https://aclanthology.org/P17-1147/", "year": "2017"}, "metric": "Accuracy (exact match)", "name": "TriviaQA", "short": "TriviaQA", "title": "TriviaQA"}, {"category": "Commonsense inference", "description": "Four-choice questions to predict the next event", "field": "hellaswag", "input": "4-shot", "link": {"author": "Zellers et al.", "href": "https://aclanthology.org/P19-1472/", "year": "2019"}, "metric": "Accuracy", "name": "HellaSwag", "short": "HellaSwag", "title": "HellaSwag"}, {"category": "Reading comprehension", "description": "Open-ended Q\u0026A developed for the evidence document", "field": "squad2_best_exact", "input": "4-shot", "link": {"author": "Rajpurkar et al.", "href": "https://aclanthology.org/P18-2124/", "year": "2018"}, "metric": "Accuracy (exact match)", "name": "SQuAD2", "short": "SQuAD2", "title": "SQuAD2"}, {"category": "Commonsense inference", "description": "Two-choice question to predict the antecedent of a pronoun", "field": "xwinograd_en", "input": "4-shot", "link": {"author": "Tikhonov and Ryabinin", "href": "https://aclanthology.org/2021.findings-acl.310/", "year": "2021"}, "metric": "Accuracy", "name": "XWINO", "short": "XWINO", "title": "XWINO"}, {"category": "Multitask natural language understanding", "description": "Four-choice exam questions benchmark MMLU (53 subjects)", "field": "mmlu", "input": "5-shot", "link": {"author": "Hendrycks et al.", "href": "https://openreview.net/forum?id=d7KBjmI3GmQ", "year": "2021"}, "metric": "Accuracy", "name": "MMLU", "short": "MMLU", "title": "MMLU"}, {"category": "Mathematics", "description": "Math word problems", "field": "gsm8k", "input": "4-shot", "link": {"author": "Cobbe et al.", "href": "https://arxiv.org/abs/2110.14168", "year": "2021"}, "metric": "Accuracy (exact match)", "name": "GSM8K", "short": "GSM8K", "title": "GSM8K"}, {"category": "Mathematics", "description": "High school math competitions", "field": "minerva_math-cot-maj1@1", "input": "4-shot", "link": {"author": "Hendrycks et al.", "href": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html", "year": "2021"}, "metric": "Accuracy (exact match)", "name": "MATH", "short": "MATH", "title": "MATH"}, {"category": "Collection of hard-to-solve tasks for LLM", "description": "23 tasks that are difficult in BIG-Bench dataset (Srivastava et al., 2023)", "field": "bbh_cot", "input": "3-shot, CoT", "link": {"author": "Suzgun et al.", "href": "https://aclanthology.org/2023.findings-acl.824/", "year": "2023"}, "metric": "Accuracy (exact match)", "name": "BBH", "short": "BBH", "title": "BIG-Bench-Hard (BBH)"}, {"category": "Code generation", "description": "Ability of code generation measured by unit test", "field": "humaneval-unstripped@1", "input": "0-shot, 10 trials", "link": {"author": "Chen et al.", "href": "https://arxiv.org/abs/2107.03374", "year": "2021"}, "metric": "pass@1", "name": "HumanEval", "short": "HumanEval", "title": "HumanEval"}], "title": "Pre-trained (English)"}, "ja_pre": {"description": "This benchmark evaluates pre-trained LLMs models (without post-training) on Japanese benchmark datasets. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["pre"], "taskdict": {"JComQA": {"category": "Commonsense", "description": "Five-choice questions created with a knowledge base", "field": "jcommonsenseqa", "input": "4-shot", "link": {"author": "Kurihara et al.", "href": "https://aclanthology.org/2022.lrec-1.317/", "year": "2022"}, "metric": "Accuracy", "name": "JComQA", "short": "JComQA", "title": "JCommonsenseQA"}, "JEMHopQA": {"category": "Multi-hop Q\u0026A", "description": "Open-ended Q\u0026A to assess the amount of knowledge and reasoning ability", "field": "jemhopqa", "input": "4-shot", "link": {"author": "Ishii et al.", "href": "https://aclanthology.org/2024.lrec-main.831/", "year": "2024"}, "metric": "Character F1", "name": "JEMHopQA", "short": "JEMHQA", "title": "JEMHopQA"}, "JHumanEval": {"category": "Code generation", "description": "Japanese translation of HumanEval (code genration benchmark)", "field": "jhumaneval-unstripped@1", "input": "0-shot, 10 trials", "link": {"author": "Sato et al.", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P10-9.pdf", "year": "2024"}, "metric": "pass@1", "name": "JHumanEval", "short": "JHumanEval", "title": "JHumanEval"}, "JMMLU": {"category": "Multi-task natural language understanding", "description": "Japanese translation of four-choice exam questions benchmark MMLU (53 subjects)", "field": "jmmlu", "input": "5-shot", "link": {"author": "Yin et al", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A7-5.pdf", "year": "2024"}, "metric": "Accuracy", "name": "JMMLU", "short": "JMMLU", "title": "JMMLU"}, "JSQuAD": {"category": "Reading comprehension", "description": "Open-ended Q\u0026A for Wikipedia article", "field": "jsquad", "input": "4-shot", "link": {"author": "Kurihara et al.", "href": "https://aclanthology.org/2022.lrec-1.317/", "year": "2022"}, "metric": "Character F1", "name": "JSQuAD", "short": "JSQuAD", "title": "JSQuAD"}, "MGSM": {"category": "Mathematics", "description": "Japanese translation of math word problems (GSM8K)", "field": "MATH (mgsm_ja)", "input": "4-shot", "link": {"author": "Shi et al.", "href": "https://openreview.net/forum?id=fR3wGCk-IXp", "year": "2023"}, "metric": "Accuracy (exact match)", "name": "MGSM", "short": "MGSM", "title": "MGSM"}, "NIILC": {"category": "Classical Q\u0026A", "description": "Open-ended Q\u0026A that can be answered by an encyclopedia", "field": "niilc", "input": "4-shot", "link": {"author": "Sekine", "href": "https://www.anlp.jp/proceedings/annual_meeting/2003/pdf_dir/C7-6.pdf", "year": "2003"}, "metric": "Character F1", "name": "NIILC", "short": "NIILC", "title": "NIILC"}, "WMT20enja": {"category": "English-Japanese translation", "description": "Translation of news articles (English to Japanese)", "field": "wmt20_en_ja_bleu", "input": "4-shot", "link": {"author": "Barrault et al.", "href": "https://aclanthology.org/2020.wmt-1.1/", "year": "2020"}, "metric": "BLEU", "name": "WMT20enja", "short": "En-Ja", "title": "WMT20 (en-ja)"}, "WMT20jaen": {"category": "Japanese-English translation", "description": "Translation of news articles (Japanese to English)", "field": "wmt20_ja_en_bleu", "input": "4-shot", "link": {"author": "Barrault et al.", "href": "https://aclanthology.org/2020.wmt-1.1/", "year": "2020"}, "metric": "BLEU", "name": "WMT20jaen", "short": "Ja-En", "title": "WMT20 (ja-en)"}, "XLSum": {"category": "Summarization", "description": "Task to generate a highlight from a news article of BBC", "field": "XLSUM_ja_1shot", "input": "1-shot", "link": {"author": "Hasan et al.", "href": "https://aclanthology.org/2021.findings-acl.413/", "year": "2021"}, "metric": "ROUGE-2", "name": "XLSum", "short": "XL-Sum", "title": "XL-Sum"}, "avg": {"category": "Average", "collective": true, "description": "Average score of Japanese datasets for pre-trained models", "field": "ja_pre_avg", "metric": "Average", "name": "avg", "short": "Pre (ja) avg", "title": "Average"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of Japanese datasets for pre-trained models", "field": "ja_pre_avg", "metric": "Average", "name": "avg", "short": "Pre (ja) avg", "title": "Average"}, {"category": "Commonsense", "description": "Five-choice questions created with a knowledge base", "field": "jcommonsenseqa", "input": "4-shot", "link": {"author": "Kurihara et al.", "href": "https://aclanthology.org/2022.lrec-1.317/", "year": "2022"}, "metric": "Accuracy", "name": "JComQA", "short": "JComQA", "title": "JCommonsenseQA"}, {"category": "Multi-hop Q\u0026A", "description": "Open-ended Q\u0026A to assess the amount of knowledge and reasoning ability", "field": "jemhopqa", "input": "4-shot", "link": {"author": "Ishii et al.", "href": "https://aclanthology.org/2024.lrec-main.831/", "year": "2024"}, "metric": "Character F1", "name": "JEMHopQA", "short": "JEMHQA", "title": "JEMHopQA"}, {"category": "Classical Q\u0026A", "description": "Open-ended Q\u0026A that can be answered by an encyclopedia", "field": "niilc", "input": "4-shot", "link": {"author": "Sekine", "href": "https://www.anlp.jp/proceedings/annual_meeting/2003/pdf_dir/C7-6.pdf", "year": "2003"}, "metric": "Character F1", "name": "NIILC", "short": "NIILC", "title": "NIILC"}, {"category": "Reading comprehension", "description": "Open-ended Q\u0026A for Wikipedia article", "field": "jsquad", "input": "4-shot", "link": {"author": "Kurihara et al.", "href": "https://aclanthology.org/2022.lrec-1.317/", "year": "2022"}, "metric": "Character F1", "name": "JSQuAD", "short": "JSQuAD", "title": "JSQuAD"}, {"category": "Summarization", "description": "Task to generate a highlight from a news article of BBC", "field": "XLSUM_ja_1shot", "input": "1-shot", "link": {"author": "Hasan et al.", "href": "https://aclanthology.org/2021.findings-acl.413/", "year": "2021"}, "metric": "ROUGE-2", "name": "XLSum", "short": "XL-Sum", "title": "XL-Sum"}, {"category": "Mathematics", "description": "Japanese translation of math word problems (GSM8K)", "field": "MATH (mgsm_ja)", "input": "4-shot", "link": {"author": "Shi et al.", "href": "https://openreview.net/forum?id=fR3wGCk-IXp", "year": "2023"}, "metric": "Accuracy (exact match)", "name": "MGSM", "short": "MGSM", "title": "MGSM"}, {"category": "English-Japanese translation", "description": "Translation of news articles (English to Japanese)", "field": "wmt20_en_ja_bleu", "input": "4-shot", "link": {"author": "Barrault et al.", "href": "https://aclanthology.org/2020.wmt-1.1/", "year": "2020"}, "metric": "BLEU", "name": "WMT20enja", "short": "En-Ja", "title": "WMT20 (en-ja)"}, {"category": "Japanese-English translation", "description": "Translation of news articles (Japanese to English)", "field": "wmt20_ja_en_bleu", "input": "4-shot", "link": {"author": "Barrault et al.", "href": "https://aclanthology.org/2020.wmt-1.1/", "year": "2020"}, "metric": "BLEU", "name": "WMT20jaen", "short": "Ja-En", "title": "WMT20 (ja-en)"}, {"category": "Multi-task natural language understanding", "description": "Japanese translation of four-choice exam questions benchmark MMLU (53 subjects)", "field": "jmmlu", "input": "5-shot", "link": {"author": "Yin et al", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A7-5.pdf", "year": "2024"}, "metric": "Accuracy", "name": "JMMLU", "short": "JMMLU", "title": "JMMLU"}, {"category": "Code generation", "description": "Japanese translation of HumanEval (code genration benchmark)", "field": "jhumaneval-unstripped@1", "input": "0-shot, 10 trials", "link": {"author": "Sato et al.", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P10-9.pdf", "year": "2024"}, "metric": "pass@1", "name": "JHumanEval", "short": "JHumanEval", "title": "JHumanEval"}], "title": "Pre-trained (Japanese)"}};
const g_updater = [];

var swallow_leaderboard_models = function(query, sortkey) {
  let models = [];

  if ("name" in query) {
    // Return a single model specified by a name.
    return g_models[query.name];

  } else if ("includes" in query) {
    for (let name of query.includes) {
      if (name in g_models) {
        models.push(g_models[name]);
      }
    }

  } else {
    for (let key in g_models) {
      const model = g_models[key];

      // Filter by the model family.
      if ("family" in query) {
        const family = model.family.toLowerCase();
        if (Array.isArray(query.family)) {
          let m = false;
          for (let f of query.family) {
            if (family.startsWith(f.toLowerCase())) {
              m = true;
              break;
            }
          }
          if (!m) {
            continue;
          }
        } else {
          if (!family.startsWith(query.family.toLowerCase())) {
            continue;
          }
        }
      }

      // Filter by the model size.
      if ("minp" in query && model['params'] < query.minp) {
        continue;
      }
      if ("maxp" in query && query.maxp <= model['params']) {
        continue;
      }

      // Filter by the exclude list.
      if ("excludes" in query && query.excludes.includes(model['id'])) {
        continue;
      }
      
      models.push(model);
    }
  }

  // Sort models.
  if (sortkey !== undefined) {
    models.sort((a, b) => a.results[sortkey[0]][sortkey[1]] - b.results[sortkey[0]][sortkey[1]]);
  }

  return models;
};

var swallow_leaderboard_get_taskdef = function(query) {
  for (let t of g_tasks[query[0]].tasks) {
    if (query[1] == t.name) {
      return t;
    }
  }
  return null;
};

var swallow_leaderboard_tasks = function(queries) {
  let tasks = [];

  for (let query of queries) {
    for (let t of g_tasks[query[0]].tasks) {
      if (query[1] == t.name || query[1] == "__ALL__" || (query[1] == "*" && !t.collective) || (query[1] == "+" && !t.collective && !t.exclude_from_avg)) {
        tasks.push({category: query[0], task: t});
      }
    }
  }

  return tasks;
};

var swallow_leaderboard_chart_bar = function(element, config) {
  var onclick = function(chartContext, options) {
    config.sort = (config.sort + 1) % config.tasks.length;
    option = get_option(element, config);
    ApexCharts.exec(element, 'updateOptions', option, false, true);
  };

  var get_option = function(element, config) {
    const models = "sort" in config ?
      swallow_leaderboard_models(config.models, config.tasks[config.sort]) :
      swallow_leaderboard_models(config.models);

    // Build series.
    let series = [];
    let labels = [];
    let tasks = swallow_leaderboard_tasks(config.tasks);

    if ("groupby" in config && config.groupby == "task") {
      for (var model of models) {
        let data = [];
        for (var q of tasks) {
          data.push(model.results[q.category][q.task.name]);
        }
        series.push({name: model.name, data: data});
      }

      // Build labels.
      for (var q of tasks) {
        labels.push(q.task.title);
      }

    } else {
      for (var query of config.tasks) {
        let data = [];
        for (var model of models) {
          data.push(model.results[query[0]][query[1]]);
        }
        const t = swallow_leaderboard_get_taskdef(query);
        let title = ("show_category" in config && config.show_category) ? g_tasks[query[0]].title : "";
        title += " " + t.title;
        series.push({name: title, data: data});
      }

      // Build labels.
      for (var model of models) {
        labels.push(model.name);
      }
    }

    const pb = "pb" in config ? config.pb : 0;
  
    // Determine the orientation.
    const horizontal = (window.outerWidth < window.outerHeight);
    const height = horizontal ? Math.max(480, labels.length * 16 + 64) : 480;
    const padding = horizontal ? {} : { left: 48, right: 48, top: 0, bottom: pb };
    return {
      chart: {
        type: "bar",
        id: element,
        fontFamily: 'inherit',
        height: height,
        parentHeightOffset: 0,
        animations: {
          enabled: true
        },
        events: {
          xAxisLabelClick: onclick,
        },
      },
      plotOptions: {
        bar: {
          horizontal: horizontal,
        },
      },
      grid: {
        padding: padding,
      },
      series: series,
      xaxis: {
        categories: labels,
        labels: {
          rotateAlways: true,
          hideOverlappingLabels: false,
          style: {
//            fontSize: '9px',
          }
        }
      },
      yaxis: {
        tickAmount: 5,
        min: 0.,
        max: 1.,
        labels: {
          formatter: function(val) {
            return typeof val == "string" ? val : parseFloat(val).toFixed(1);
          }
        },
      },
      dataLabels: {
        enabled: false
      },
      stroke: {
        show: true,
        width: 2,
        colors: ['transparent']
      },
      legend: {
        position: 'top',
      },
      tooltip: {
        shared: false,
        intersect: true,
        marker: {
          show: false,
        },
        y: {
          formatter: function(val) {
            return typeof val == "string" ? val : parseFloat(val).toFixed(4);
          }
        },
      },
      noData: {
        text: " (Select a model)",
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    }
  };

  // Create the bar chart.
  option = get_option(element, config);
  window.ApexCharts && (new ApexCharts(document.getElementById(element), option)).render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      ApexCharts.exec(element, 'updateOptions', option, false, true);
    }
  });
};

var swallow_leaderboard_chart_radar = function(element, config) {
  var get_option = function(element, config) {
    const models = swallow_leaderboard_models(config.models);

    // Build series.
    let series = [];
    var labels = [];
    for (var model of models) {
      let data = [];
      labels = [];

      for (let query of config.tasks) {
        for (let t of g_tasks[query[0]].tasks) {
          if (query[1] == t.name || (query[1] == "*" && !t.collective) || (query[1] == "+" && !t.collective && !t.exclude_from_avg)) {
            data.push(model.results[query[0]][t.name]);
            labels.push(t.short);
          }
        }
      }
      series.push({
        name: model.name,
        data: data,
      });
    }

    return {
      chart: {
        type: "radar",
        id: element,
        fontFamily: 'inherit',
        height: config.height,
        parentHeightOffset: 0,
      },
      series: series,
      xaxis: {
        categories: labels,
      },
      yaxis: {
        show: false,
        tickAmount: 5,
        min: 0.,
        max: 1.,
      },
      legend: {
        position: 'top',
      },
      tooltip: {
        marker: {
          show: false,
        },
      },
      noData: {
        text: " (Select a model)",
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    };
  };
  
  // Create the radar chart.
  option = get_option(element, config);
  window.ApexCharts && (new ApexCharts(document.getElementById(element), option)).render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      ApexCharts.exec(element, 'updateOptions', option, false, true);
    }
  });
};

var swallow_leaderboard_chart_scatter = function(element, config) {
  var get_option = function(element, config) {
    const models = swallow_leaderboard_models(config.models);
    const xcat = config.xaxis[0];
    const xname = config.xaxis[1]
    const ycat = config.yaxis[0];
    const yname = config.yaxis[1];
    // There seems to be no way to obtain a color palette from ApexCharts,
    // so these colors are hard coded.
    const theme_colors = document.documentElement.getAttribute("data-bs-theme") == "dark" ? 
      ['#4ECDC4', '#C7F464', '#81D4FA', '#FD6A6A', '#546E7A']: 
      ['#008FFB', '#00E396', '#FEB019', '#FF4560', '#775DD0'];

    var format_params = function(val) {
      const v = Math.pow(2, parseFloat(val));
      return v < 10 ? v.toFixed(1) : v.toFixed(0);
    };

    var format_value = function(val) {
      return typeof val == "string" ? val : parseFloat(val).toFixed(4);
    };

    var build_axis = function(category, name) {
      if (category == "params" || category == "active_params") {
        
        title = category == "active_params" ? "Active parameters" : "Parameters";
        
        return {
          min: 0, // 2^0 = 1.
          max: 8, // 2^8 = 256.
          title: {text: title + " [B]"},
          tickAmount: 10,
          labels: {
            formatter: function(val) {
              const v = Math.pow(2, parseFloat(val));
              return v < 10 ? v.toFixed(1) : v.toFixed(0);
            }
          }
        }
      } else {
        return {
          min: 0,
          max: 1,
          title: {text: g_tasks[category].taskdict[name].title },
          tickAmount: 10,
          labels: {
            formatter: function(val) {
              return parseFloat(val).toFixed(1)
            }
          }
        }
      }
    };

    // Build series.
    let series = [];
    for (let model of models) {
      const x = (xcat == "params" || xcat == "active_params") ? Math.log2(model[xcat]) : model.results[xcat][xname];
      const y = (ycat == "params" || ycat == "active_params") ? Math.log2(model[ycat]) : model.results[ycat][yname];
      const family = model.family.toLowerCase();
      let color_index = 4;
      if (family.startsWith("gpt") || family.startsWith("o3") || family.startsWith("o4")) {
        color_index = 0;
      } else if (family.startsWith("llama")) {
        color_index = 1;
      } else if (family.startsWith("gemma")) {
        color_index = 2;
      } else if (family.startsWith("qwen")) {
        color_index = 3;
      }
      series.push({name: model.name, data: [[x, y]], color: theme_colors[color_index]});
    }
    const xaxis = build_axis(xcat, xname);
    const yaxis = build_axis(ycat, yname);

    return {
      chart: {
        type: "scatter",
        fontFamily: 'inherit',
        height: 480,
        parentHeightOffset: 0,
        toolbar: {
          tools: {
            download: true,
            selection: false,
            zoom: false,
            zoomin: false,
            zoomout: false,
            pan: false,
            reset: false,
          },
        },
        zoom: {
          enabled: false,
          allowMouseWheelZoom: false,
        },
      },
      series: series,
      xaxis: xaxis,
      yaxis: yaxis,
      legend: {
        show: false,
      },
      markers: {
        size: 4,
      },
      tooltip: {
        marker: {show: false},
        x: {formatter: (xcat == "params" || xcat == "active_params") ? format_params : format_value},
        y: {formatter: (ycat == "params" || ycat == "active_params") ? format_params : format_value},
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    };
  };

  // Create the radar chart.
  option = get_option(element, config);
  let obj_scatter = new ApexCharts(document.getElementById(element), option);
  console.log(obj_scatter);
  obj_scatter.render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      obj_scatter.updateOptions(option, false, true);
    }
  });
};

var swallow_leaderboard_chart = function(element, config) {
  //
  if (config.type == "bar") {
    swallow_leaderboard_chart_bar(element, config);
  } else if (config.type == "radar") {
    swallow_leaderboard_chart_radar(element, config);
  } else if (config.type == "scatter") {
    swallow_leaderboard_chart_scatter(element, config);
  }
};

var swallow_leaderboard_update = function(target, updates) {
  for (let i = 0; i < g_updater.length; ++i) {
    g_updater[i](target, updates);
  }
}
