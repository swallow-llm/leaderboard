# Evaluation results in Swallow LLM Leaderboard
# Copyright (c) 2025 Swallow LLM team
# This file is distributed under Creative Commons Attribution 4.0 (CC-BY 4.0) License

- id: CohereForAI/aya-expanse-8b
  name: Aya Expanse 8B
  date: '2024-10-24'
  params: 8.0
  base_model: (private)
  sortkey: aya expanse
  url: https://huggingface.co/CohereForAI/aya-expanse-8b
  results:
    ja_basic:
      Ja Avg: 0.4449
      JComQA: 0.9223
      JEMHopQA: 0.4665
      NIILC: 0.3847
      JSQuAD: 0.8671
      XL-Sum: 0.2112
      MGSM: 0.608
      WMT20-en-ja: 0.2609
      WMT20-ja-en: 0.2058
      JMMLU: 0.5213
      JHumanEval: 0.0012
    ja_mtb:
      JMT Avg: 0.637
      coding: 0.494
      extraction: 0.718
      humanities: 0.855
      math: 0.398
      reasoning: 0.433
      roleplay: 0.737
      stem: 0.677
      writing: 0.787
    en_basic:
      En Avg: 0.5394
      OpenBookQA: 0.384
      TriviaQA: 0.5908
      HellaSwag: 0.6054
      SQuAD2: 0.6639
      XWINO: 0.8916
      MMLU: 0.6285
      GSM8K: 0.7559
      MATH: 0.284
      BBH: 0.5898
      HumanEval: 0.0
    other:
      GPQA: 0.2589
- id: CohereForAI/aya-expanse-32b
  name: Aya Expanse 32B
  date: '2024-10-24'
  params: 32
  base_model: (private)
  sortkey: aya expanse
  url: https://huggingface.co/CohereForAI/aya-expanse-32b
  results:
    ja_basic:
      Ja Avg: 0.5116
      JComQA: 0.9651
      JEMHopQA: 0.5539
      NIILC: 0.5857
      JSQuAD: 0.8119
      XL-Sum: 0.2951
      MGSM: 0.716
      WMT20-en-ja: 0.2872
      WMT20-ja-en: 0.2448
      JMMLU: 0.6549
      JHumanEval: 0.0012
    ja_mtb:
      JMT Avg: 0.713
      coding: 0.548
      extraction: 0.72
      humanities: 0.846
      math: 0.657
      reasoning: 0.602
      roleplay: 0.824
      stem: 0.712
      writing: 0.794
    en_basic:
      En Avg: 0.6143
      OpenBookQA: 0.42
      TriviaQA: 0.7568
      HellaSwag: 0.6676
      SQuAD2: 0.6795
      XWINO: 0.9118
      MMLU: 0.7435
      GSM8K: 0.8575
      MATH: 0.344
      BBH: 0.7575
      HumanEval: 0.0049
    other:
      GPQA: 0.1518
- id: cyberagent/calm3-22b-chat
  name: CyberAgentLM3-22B-chat
  date: '2024-07-09'
  params: 22
  base_model: (private)
  sortkey: cyberagentlm3
  url: https://huggingface.co/cyberagent/calm3-22b-chat
  results:
    ja_basic:
      Ja Avg: 0.4714
      JComQA: 0.9339
      JEMHopQA: 0.5096
      NIILC: 0.6479
      JSQuAD: 0.9109
      XL-Sum: 0.1044
      MGSM: 0.576
      WMT20-en-ja: 0.2749
      WMT20-ja-en: 0.215
      JMMLU: 0.5411
      JHumanEval: 0.0006
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.519
      extraction: 0.744
      humanities: 0.859
      math: 0.605
      reasoning: 0.548
      roleplay: 0.784
      stem: 0.7
      writing: 0.772
    en_basic:
      En Avg: 0.5269
      OpenBookQA: 0.372
      TriviaQA: 0.6192
      HellaSwag: 0.5975
      SQuAD2: 0.6027
      XWINO: 0.9049
      MMLU: 0.6025
      GSM8K: 0.6975
      MATH: 0.274
      BBH: 0.5988
      HumanEval: 0.0
    other:
      GPQA: 0.2567
- id: tiiuae/Falcon3-1B-Base
  name: Falcon3-1B-Base
  date: '2024-12-19'
  params: 1.7
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-1B-Base
  results:
    ja_basic:
      Ja Avg: 0.129
      JComQA: 0.216
      JEMHopQA: 0.251
      NIILC: 0.062
      JSQuAD: 0.281
      XL-Sum: 0.085
      MGSM: 0.008
      WMT20-en-ja: 0.012
      WMT20-ja-en: 0.02
      JMMLU: 0.264
      JHumanEval: 0.088
    en_basic:
      En Avg: 0.376
      OpenBookQA: 0.316
      TriviaQA: 0.296
      HellaSwag: 0.458
      SQuAD2: 0.501
      XWINO: 0.816
      MMLU: 0.449
      GSM8K: 0.337
      MATH: 0.14
      BBH: 0.323
      HumanEval: 0.125
    other:
      GPQA: 0.025
- id: tiiuae/Falcon3-1B-Instruct
  name: Falcon3-1B-Instruct
  date: '2024-12-19'
  params: 1.7
  base_model: Falcon3-1B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-1B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.1689
      JComQA: 0.2404
      JEMHopQA: 0.3121
      NIILC: 0.1324
      JSQuAD: 0.4538
      XL-Sum: 0.101
      MGSM: 0.02
      WMT20-en-ja: 0.0279
      WMT20-ja-en: 0.032
      JMMLU: 0.2807
      JHumanEval: 0.089
    ja_mtb:
      JMT Avg: 0.161
      coding: 0.176
      extraction: 0.178
      humanities: 0.121
      math: 0.161
      reasoning: 0.224
      roleplay: 0.154
      stem: 0.124
      writing: 0.148
    en_basic:
      En Avg: 0.3811
      OpenBookQA: 0.344
      TriviaQA: 0.2611
      HellaSwag: 0.4801
      SQuAD2: 0.5008
      XWINO: 0.8146
      MMLU: 0.4586
      GSM8K: 0.3912
      MATH: 0.13
      BBH: 0.3301
      HumanEval: 0.1006
    other:
      GPQA: 0.0134
- id: tiiuae/Falcon3-3B-Base
  name: Falcon3-3B-Base
  date: '2024-12-19'
  params: 3.2
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-3B-Base
  results:
    ja_basic:
      Ja Avg: 0.209
      JComQA: 0.281
      JEMHopQA: 0.333
      NIILC: 0.113
      JSQuAD: 0.517
      XL-Sum: 0.12
      MGSM: 0.096
      WMT20-en-ja: 0.031
      WMT20-ja-en: 0.051
      JMMLU: 0.319
      JHumanEval: 0.229
    en_basic:
      En Avg: 0.495
      OpenBookQA: 0.312
      TriviaQA: 0.346
      HellaSwag: 0.492
      SQuAD2: 0.503
      XWINO: 0.847
      MMLU: 0.567
      GSM8K: 0.634
      MATH: 0.344
      BBH: 0.553
      HumanEval: 0.348
    other:
      GPQA: 0.179
- id: tiiuae/Falcon3-3B-Instruct
  name: Falcon3-3B-Instruct
  date: '2024-12-19'
  params: 3.2
  base_model: Falcon3-3B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.2319
      JComQA: 0.4209
      JEMHopQA: 0.1603
      NIILC: 0.1129
      JSQuAD: 0.6322
      XL-Sum: 0.1414
      MGSM: 0.092
      WMT20-en-ja: 0.0615
      WMT20-ja-en: 0.0584
      JMMLU: 0.3313
      JHumanEval: 0.3079
    ja_mtb:
      JMT Avg: 0.26
      coding: 0.329
      extraction: 0.392
      humanities: 0.219
      math: 0.199
      reasoning: 0.267
      roleplay: 0.234
      stem: 0.229
      writing: 0.208
    en_basic:
      En Avg: 0.526
      OpenBookQA: 0.372
      TriviaQA: 0.2856
      HellaSwag: 0.5414
      SQuAD2: 0.5133
      XWINO: 0.8176
      MMLU: 0.5624
      GSM8K: 0.7119
      MATH: 0.44
      BBH: 0.5617
      HumanEval: 0.4537
    other:
      GPQA: 0.2076
- id: tiiuae/Falcon3-7B-Base
  name: Falcon3-7B-Base
  date: '2024-12-19'
  params: 7.5
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-7B-Base
  results:
    ja_basic:
      Ja Avg: 0.337
      JComQA: 0.634
      JEMHopQA: 0.412
      NIILC: 0.18
      JSQuAD: 0.788
      XL-Sum: 0.173
      MGSM: 0.244
      WMT20-en-ja: 0.078
      WMT20-ja-en: 0.119
      JMMLU: 0.385
      JHumanEval: 0.361
    en_basic:
      En Avg: 0.596
      OpenBookQA: 0.354
      TriviaQA: 0.552
      HellaSwag: 0.566
      SQuAD2: 0.539
      XWINO: 0.881
      MMLU: 0.701
      GSM8K: 0.766
      MATH: 0.438
      BBH: 0.692
      HumanEval: 0.476
    other:
      GPQA: 0.252
- id: tiiuae/Falcon3-7B-Instruct
  name: Falcon3-7B-Instruct
  date: '2024-12-19'
  params: 7.5
  base_model: Falcon3-7B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.3635
      JComQA: 0.6836
      JEMHopQA: 0.436
      NIILC: 0.1522
      JSQuAD: 0.8161
      XL-Sum: 0.1767
      MGSM: 0.32
      WMT20-en-ja: 0.0936
      WMT20-ja-en: 0.1256
      JMMLU: 0.4149
      JHumanEval: 0.4165
    ja_mtb:
      JMT Avg: 0.377
      coding: 0.549
      extraction: 0.506
      humanities: 0.34
      math: 0.406
      reasoning: 0.257
      roleplay: 0.299
      stem: 0.34
      writing: 0.317
    en_basic:
      En Avg: 0.6184
      OpenBookQA: 0.394
      TriviaQA: 0.5169
      HellaSwag: 0.6113
      SQuAD2: 0.5247
      XWINO: 0.8551
      MMLU: 0.705
      GSM8K: 0.7733
      MATH: 0.542
      BBH: 0.7108
      HumanEval: 0.5506
    other:
      GPQA: 0.2768
- id: tiiuae/Falcon3-10B-Base
  name: Falcon3-10B-Base
  date: '2024-12-19'
  params: 10
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-10B-Base
  results:
    ja_basic:
      Ja Avg: 0.383
      JComQA: 0.68
      JEMHopQA: 0.443
      NIILC: 0.187
      JSQuAD: 0.854
      XL-Sum: 0.187
      MGSM: 0.376
      WMT20-en-ja: 0.103
      WMT20-ja-en: 0.139
      JMMLU: 0.435
      JHumanEval: 0.426
    en_basic:
      En Avg: 0.639
      OpenBookQA: 0.368
      TriviaQA: 0.579
      HellaSwag: 0.596
      SQuAD2: 0.603
      XWINO: 0.901
      MMLU: 0.732
      GSM8K: 0.802
      MATH: 0.492
      BBH: 0.776
      HumanEval: 0.543
    other:
      GPQA: 0.259
- id: tiiuae/Falcon3-10B-Instruct
  name: Falcon3-10B-Instruct
  date: '2024-12-19'
  params: 10
  base_model: Falcon3-10B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-10B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.3669
      JComQA: 0.6899
      JEMHopQA: 0.2207
      NIILC: 0.1222
      JSQuAD: 0.853
      XL-Sum: 0.1919
      MGSM: 0.392
      WMT20-en-ja: 0.1077
      WMT20-ja-en: 0.1354
      JMMLU: 0.442
      JHumanEval: 0.5146
    ja_mtb:
      JMT Avg: 0.413
      coding: 0.509
      extraction: 0.545
      humanities: 0.382
      math: 0.48
      reasoning: 0.356
      roleplay: 0.335
      stem: 0.373
      writing: 0.324
    en_basic:
      En Avg: 0.6333
      OpenBookQA: 0.424
      TriviaQA: 0.5035
      HellaSwag: 0.6398
      SQuAD2: 0.549
      XWINO: 0.8748
      MMLU: 0.7302
      GSM8K: 0.793
      MATH: 0.462
      BBH: 0.7289
      HumanEval: 0.6274
    other:
      GPQA: 0.3348
- id: google/gemma-2-2b
  name: Gemma 2 2B
  date: '2024-06-27'
  params: 2.6
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-2b
  results:
    ja_basic:
      Ja Avg: 0.348
      JComQA: 0.721
      JEMHopQA: 0.472
      NIILC: 0.316
      JSQuAD: 0.81
      XL-Sum: 0.083
      MGSM: 0.124
      WMT20-en-ja: 0.203
      WMT20-ja-en: 0.19
      JMMLU: 0.388
      JHumanEval: 0.177
    en_basic:
      En Avg: 0.439
      OpenBookQA: 0.342
      TriviaQA: 0.552
      HellaSwag: 0.552
      SQuAD2: 0.501
      XWINO: 0.89
      MMLU: 0.53
      GSM8K: 0.249
      MATH: 0.176
      BBH: 0.415
      HumanEval: 0.188
    other:
      GPQA: 0.049
- id: google/gemma-2-2b-it
  name: Gemma 2 2B IT
  date: '2024-06-27'
  params: 2.6
  base_model: Gemma 2 2B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-2b-it
  results:
    ja_basic:
      Ja Avg: 0.3921
      JComQA: 0.8624
      JEMHopQA: 0.3478
      NIILC: 0.3147
      JSQuAD: 0.879
      XL-Sum: 0.1173
      MGSM: 0.252
      WMT20-en-ja: 0.2068
      WMT20-ja-en: 0.1835
      JMMLU: 0.4366
      JHumanEval: 0.3213
    ja_mtb:
      JMT Avg: 0.569
      coding: 0.454
      extraction: 0.587
      humanities: 0.693
      math: 0.524
      reasoning: 0.445
      roleplay: 0.654
      stem: 0.567
      writing: 0.63
    en_basic:
      En Avg: 0.4887
      OpenBookQA: 0.354
      TriviaQA: 0.5019
      HellaSwag: 0.5202
      SQuAD2: 0.5476
      XWINO: 0.8783
      MMLU: 0.5693
      GSM8K: 0.4397
      MATH: 0.23
      BBH: 0.4637
      HumanEval: 0.3823
    other:
      GPQA: 0.0804
- id: google/gemma-2-9b
  name: Gemma 2 9B
  date: '2024-06-27'
  params: 9.2
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-9b
  results:
    ja_basic:
      Ja Avg: 0.5
      JComQA: 0.904
      JEMHopQA: 0.573
      NIILC: 0.524
      JSQuAD: 0.898
      XL-Sum: 0.168
      MGSM: 0.456
      WMT20-en-ja: 0.269
      WMT20-ja-en: 0.236
      JMMLU: 0.623
      JHumanEval: 0.345
    en_basic:
      En Avg: 0.597
      OpenBookQA: 0.382
      TriviaQA: 0.718
      HellaSwag: 0.626
      SQuAD2: 0.506
      XWINO: 0.907
      MMLU: 0.706
      GSM8K: 0.688
      MATH: 0.338
      BBH: 0.704
      HumanEval: 0.39
    other:
      GPQA: 0.221
- id: google/gemma-2-9b-it
  name: Gemma 2 9B IT
  date: '2024-06-27'
  params: 9.2
  base_model: Gemma 2 9B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-9b-it
  results:
    ja_basic:
      Ja Avg: 0.5345
      JComQA: 0.9312
      JEMHopQA: 0.5324
      NIILC: 0.5265
      JSQuAD: 0.8762
      XL-Sum: 0.1488
      MGSM: 0.636
      WMT20-en-ja: 0.2734
      WMT20-ja-en: 0.239
      JMMLU: 0.6227
      JHumanEval: 0.5591
    ja_mtb:
      JMT Avg: 0.736
      coding: 0.652
      extraction: 0.765
      humanities: 0.857
      math: 0.614
      reasoning: 0.673
      roleplay: 0.811
      stem: 0.713
      writing: 0.8
    en_basic:
      En Avg: 0.6487
      OpenBookQA: 0.432
      TriviaQA: 0.6584
      HellaSwag: 0.6048
      SQuAD2: 0.6592
      XWINO: 0.9041
      MMLU: 0.7234
      GSM8K: 0.7786
      MATH: 0.394
      BBH: 0.7192
      HumanEval: 0.6134
    other:
      GPQA: 0.0647
- id: google/gemma-2-27b
  name: Gemma 2 27B
  date: '2024-06-27'
  params: 27
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-27b
  results:
    ja_basic:
      Ja Avg: 0.546
      JComQA: 0.936
      JEMHopQA: 0.553
      NIILC: 0.573
      JSQuAD: 0.916
      XL-Sum: 0.194
      MGSM: 0.596
      WMT20-en-ja: 0.295
      WMT20-ja-en: 0.251
      JMMLU: 0.659
      JHumanEval: 0.49
    en_basic:
      En Avg: 0.655
      OpenBookQA: 0.412
      TriviaQA: 0.78
      HellaSwag: 0.675
      SQuAD2: 0.549
      XWINO: 0.921
      MMLU: 0.754
      GSM8K: 0.757
      MATH: 0.438
      BBH: 0.76
      HumanEval: 0.508
    other:
      GPQA: 0.125
- id: google/gemma-2-27b-it
  name: Gemma 2 27B IT
  date: '2024-06-27'
  params: 27
  base_model: Gemma 2 27B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-27b-it
  results:
    ja_basic:
      Ja Avg: 0.5673
      JComQA: 0.9562
      JEMHopQA: 0.5413
      NIILC: 0.5755
      JSQuAD: 0.8832
      XL-Sum: 0.1657
      MGSM: 0.704
      WMT20-en-ja: 0.2903
      WMT20-ja-en: 0.2488
      JMMLU: 0.6701
      JHumanEval: 0.6378
    ja_mtb:
      JMT Avg: 0.768
      coding: 0.727
      extraction: 0.809
      humanities: 0.874
      math: 0.719
      reasoning: 0.639
      roleplay: 0.81
      stem: 0.74
      writing: 0.826
    en_basic:
      En Avg: 0.7033
      OpenBookQA: 0.458
      TriviaQA: 0.7658
      HellaSwag: 0.6551
      SQuAD2: 0.6691
      XWINO: 0.9088
      MMLU: 0.7622
      GSM8K: 0.8514
      MATH: 0.466
      BBH: 0.79
      HumanEval: 0.7067
    other:
      GPQA: 0.0871
- id: rinna/gemma-2-baku-2b
  name: Gemma 2 Baku 2B
  date: '2024-10-03'
  params: 2.6
  base_model: ''
  sortkey: gemma 2 baku
  url: https://huggingface.co/rinna/gemma-2-baku-2b
  results:
    ja_basic:
      Ja Avg: 0.372
      JComQA: 0.76
      JEMHopQA: 0.475
      NIILC: 0.443
      JSQuAD: 0.843
      XL-Sum: 0.121
      MGSM: 0.124
      WMT20-en-ja: 0.255
      WMT20-ja-en: 0.187
      JMMLU: 0.376
      JHumanEval: 0.137
    en_basic:
      En Avg: 0.4
      OpenBookQA: 0.314
      TriviaQA: 0.475
      HellaSwag: 0.533
      SQuAD2: 0.501
      XWINO: 0.881
      MMLU: 0.493
      GSM8K: 0.168
      MATH: 0.11
      BBH: 0.376
      HumanEval: 0.15
    other:
      GPQA: 0.029
- id: rinna/gemma-2-baku-2b-it
  name: Gemma 2 Baku 2B IT
  date: '2024-10-03'
  params: 2.6
  base_model: Gemma 2 Baku 2B
  sortkey: gemma 2 baku
  url: https://huggingface.co/rinna/gemma-2-baku-2b-it
  results:
    ja_basic:
      Ja Avg: 0.3662
      JComQA: 0.8552
      JEMHopQA: 0.2278
      NIILC: 0.3899
      JSQuAD: 0.8773
      XL-Sum: 0.1148
      MGSM: 0.172
      WMT20-en-ja: 0.2549
      WMT20-ja-en: 0.1896
      JMMLU: 0.4154
      JHumanEval: 0.1646
    ja_mtb:
      JMT Avg: 0.59
      coding: 0.47
      extraction: 0.625
      humanities: 0.81
      math: 0.414
      reasoning: 0.382
      roleplay: 0.713
      stem: 0.609
      writing: 0.697
    en_basic:
      En Avg: 0.3608
      OpenBookQA: 0.342
      TriviaQA: 0.416
      HellaSwag: 0.5107
      SQuAD2: 0.5219
      XWINO: 0.8705
      MMLU: 0.5259
      GSM8K: 0.0265
      MATH: 0.174
      BBH: 0.0627
      HumanEval: 0.1579
    other:
      GPQA: 0.0513
- id: google/gemma-2-2b-jpn-it
  name: Gemma 2 JPN
  date: '2024-06-27'
  params: 2.6
  base_model: Gemma 2 2B
  sortkey: gemma 2 jpn
  url: https://huggingface.co/google/gemma-2-2b-jpn-it
  results:
    ja_basic:
      Ja Avg: 0.3769
      JComQA: 0.8445
      JEMHopQA: 0.3207
      NIILC: 0.2908
      JSQuAD: 0.8771
      XL-Sum: 0.1315
      MGSM: 0.192
      WMT20-en-ja: 0.2042
      WMT20-ja-en: 0.1795
      JMMLU: 0.418
      JHumanEval: 0.311
    ja_mtb:
      JMT Avg: 0.55
      coding: 0.467
      extraction: 0.488
      humanities: 0.741
      math: 0.379
      reasoning: 0.406
      roleplay: 0.66
      stem: 0.589
      writing: 0.672
    en_basic:
      En Avg: 0.4705
      OpenBookQA: 0.37
      TriviaQA: 0.5026
      HellaSwag: 0.5319
      SQuAD2: 0.5391
      XWINO: 0.8787
      MMLU: 0.557
      GSM8K: 0.351
      MATH: 0.132
      BBH: 0.4506
      HumanEval: 0.3921
    other:
      GPQA: 0.0335
- id: gpt-3.5-turbo-0125
  name: GPT-3.5 (gpt-3.5-turbo-0125)
  date: '2024-01-25'
  params: 0
  base_model: (private)
  sortkey: gpt 3.5 (gpt 3.5 turbo 0125)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.5151
      JComQA: 0.9223
      JEMHopQA: 0.4563
      NIILC: 0.447
      JSQuAD: 0.8933
      XL-Sum: 0.2147
      MGSM: 0.572
      WMT20-en-ja: 0.2867
      WMT20-ja-en: 0.2431
      JMMLU: 0.4993
      JHumanEval: 0.6159
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.693
      extraction: 0.789
      humanities: 0.773
      math: 0.665
      reasoning: 0.462
      roleplay: 0.728
      stem: 0.644
      writing: 0.775
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.2902
- id: gpt-4-turbo-2024-04-09
  name: GPT-4-turbo (gpt-4-turbo-2024-04-09)
  date: '2024-04-09'
  params: 0
  base_model: (private)
  sortkey: gpt 4 turbo (gpt 4 turbo 2024 04 09)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.6264
      JComQA: 0.9714
      JEMHopQA: 0.6897
      NIILC: 0.6151
      JSQuAD: 0.8783
      XL-Sum: 0.2008
      MGSM: 0.848
      WMT20-en-ja: 0.2948
      WMT20-ja-en: 0.2395
      JMMLU: 0.7535
      JHumanEval: 0.7732
    ja_mtb:
      JMT Avg: 0.837
      coding: 0.842
      extraction: 0.891
      humanities: 0.863
      math: 0.865
      reasoning: 0.673
      roleplay: 0.861
      stem: 0.844
      writing: 0.854
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.3036
- id: gpt-4o-2024-05-13
  name: GPT-4o (gpt-4o-2024-05-13)
  date: '2024-05-13'
  params: 0
  base_model: (private)
  sortkey: gpt 4o (gpt 4o 2024 05 13)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.6488
      JComQA: 0.9794
      JEMHopQA: 0.7365
      NIILC: 0.7218
      JSQuAD: 0.8919
      XL-Sum: 0.14
      MGSM: 0.86
      WMT20-en-ja: 0.3142
      WMT20-ja-en: 0.2366
      JMMLU: 0.7941
      JHumanEval: 0.8134
    ja_mtb:
      JMT Avg: 0.848
      coding: 0.859
      extraction: 0.93
      humanities: 0.882
      math: 0.917
      reasoning: 0.631
      roleplay: 0.858
      stem: 0.858
      writing: 0.851
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.3058
- id: gpt-4o-2024-08-06
  name: GPT-4o (gpt-4o-2024-08-06)
  date: '2024-08-06'
  params: 0
  base_model: (private)
  sortkey: gpt 4o (gpt 4o 2024 08 06)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.6462
      JComQA: 0.9821
      JEMHopQA: 0.7309
      NIILC: 0.7091
      JSQuAD: 0.8891
      XL-Sum: 0.17
      MGSM: 0.864
      WMT20-en-ja: 0.3135
      WMT20-ja-en: 0.2541
      JMMLU: 0.797
      JHumanEval: 0.7518
    ja_mtb:
      JMT Avg: 0.848
      coding: 0.855
      extraction: 0.926
      humanities: 0.88
      math: 0.872
      reasoning: 0.706
      roleplay: 0.862
      stem: 0.838
      writing: 0.849
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.317
- id: gpt-4o-mini-2024-07-18
  name: GPT-4o-mini (gpt-4o-mini-2024-07-18)
  date: '2024-08-06'
  params: 0
  base_model: (private)
  sortkey: gpt 4o mini (gpt 4o mini 2024 07 18)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.5805
      JComQA: 0.9607
      JEMHopQA: 0.4642
      NIILC: 0.5913
      JSQuAD: 0.902
      XL-Sum: 0.1601
      MGSM: 0.832
      WMT20-en-ja: 0.2991
      WMT20-ja-en: 0.2411
      JMMLU: 0.6792
      JHumanEval: 0.675
    ja_mtb:
      JMT Avg: 0.824
      coding: 0.825
      extraction: 0.865
      humanities: 0.857
      math: 0.843
      reasoning: 0.665
      roleplay: 0.846
      stem: 0.855
      writing: 0.84
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.2746
- id: meta-llama/Meta-Llama-3-8B
  name: Llama 3 8B
  date: '2024-04-18'
  params: 8.0
  base_model: ''
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-8B
  results:
    ja_basic:
      Ja Avg: 0.429
      JComQA: 0.835
      JEMHopQA: 0.436
      NIILC: 0.41
      JSQuAD: 0.892
      XL-Sum: 0.177
      MGSM: 0.312
      WMT20-en-ja: 0.221
      WMT20-ja-en: 0.206
      JMMLU: 0.455
      JHumanEval: 0.344
    en_basic:
      En Avg: 0.542
      OpenBookQA: 0.38
      TriviaQA: 0.712
      HellaSwag: 0.612
      SQuAD2: 0.502
      XWINO: 0.905
      MMLU: 0.651
      GSM8K: 0.487
      MATH: 0.18
      BBH: 0.62
      HumanEval: 0.376
    other:
      GPQA: 0.136
- id: meta-llama/Meta-Llama-3-8B-Instruct
  name: Llama 3 8B Instruct
  date: '2024-04-18'
  params: 8.0
  base_model: Llama 3 8B
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4303
      JComQA: 0.8803
      JEMHopQA: 0.4173
      NIILC: 0.3853
      JSQuAD: 0.8909
      XL-Sum: 0.1261
      MGSM: 0.424
      WMT20-en-ja: 0.2141
      WMT20-ja-en: 0.202
      JMMLU: 0.4677
      JHumanEval: 0.2957
    ja_mtb:
      JMT Avg: 0.529
      coding: 0.467
      extraction: 0.706
      humanities: 0.692
      math: 0.31
      reasoning: 0.433
      roleplay: 0.542
      stem: 0.532
      writing: 0.546
    en_basic:
      En Avg: 0.6051
      OpenBookQA: 0.388
      TriviaQA: 0.6698
      HellaSwag: 0.5826
      SQuAD2: 0.6109
      XWINO: 0.892
      MMLU: 0.6569
      GSM8K: 0.7453
      MATH: 0.306
      BBH: 0.6457
      HumanEval: 0.5543
    other:
      GPQA: 0.2567
- id: meta-llama/Meta-Llama-3-70B
  name: Llama 3 70B
  date: '2024-04-18'
  params: 70
  base_model: ''
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-70B
  results:
    ja_basic:
      Ja Avg: 0.569
      JComQA: 0.946
      JEMHopQA: 0.606
      NIILC: 0.589
      JSQuAD: 0.922
      XL-Sum: 0.228
      MGSM: 0.664
      WMT20-en-ja: 0.286
      WMT20-ja-en: 0.252
      JMMLU: 0.705
      JHumanEval: 0.491
    en_basic:
      En Avg: 0.689
      OpenBookQA: 0.44
      TriviaQA: 0.826
      HellaSwag: 0.69
      SQuAD2: 0.618
      XWINO: 0.92
      MMLU: 0.787
      GSM8K: 0.801
      MATH: 0.446
      BBH: 0.829
      HumanEval: 0.527
    other:
      GPQA: 0.268
- id: meta-llama/Meta-Llama-3-70B-Instruct
  name: Llama 3 70B Instruct
  date: '2024-04-18'
  params: 70
  base_model: Llama 3 70B
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.5776
      JComQA: 0.9401
      JEMHopQA: 0.6145
      NIILC: 0.5567
      JSQuAD: 0.9131
      XL-Sum: 0.1907
      MGSM: 0.716
      WMT20-en-ja: 0.2695
      WMT20-ja-en: 0.2344
      JMMLU: 0.6798
      JHumanEval: 0.6616
    ja_mtb:
      JMT Avg: 0.64
      coding: 0.588
      extraction: 0.884
      humanities: 0.715
      math: 0.637
      reasoning: 0.487
      roleplay: 0.594
      stem: 0.598
      writing: 0.619
    en_basic:
      En Avg: 0.7293
      OpenBookQA: 0.438
      TriviaQA: 0.7997
      HellaSwag: 0.6549
      SQuAD2: 0.6962
      XWINO: 0.914
      MMLU: 0.8
      GSM8K: 0.909
      MATH: 0.474
      BBH: 0.8331
      HumanEval: 0.7738
    other:
      GPQA: 0.0357
- id: elyza/Llama-3-ELYZA-JP-8B
  name: Llama-3-ELYZA-JP-8B
  date: '2024-06-26'
  params: 8.0
  base_model: (private)
  sortkey: llama 3 elyza jp
  url: https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B
  results:
    ja_basic:
      Ja Avg: 0.4712
      JComQA: 0.8972
      JEMHopQA: 0.4978
      NIILC: 0.4958
      JSQuAD: 0.9057
      XL-Sum: 0.1685
      MGSM: 0.436
      WMT20-en-ja: 0.2504
      WMT20-ja-en: 0.1851
      JMMLU: 0.4866
      JHumanEval: 0.3884
    ja_mtb:
      JMT Avg: 0.587
      coding: 0.389
      extraction: 0.706
      humanities: 0.647
      math: 0.426
      reasoning: 0.613
      roleplay: 0.684
      stem: 0.533
      writing: 0.697
    en_basic:
      En Avg: 0.4953
      OpenBookQA: 0.318
      TriviaQA: 0.5508
      HellaSwag: 0.5229
      SQuAD2: 0.5997
      XWINO: 0.8817
      MMLU: 0.5871
      GSM8K: 0.558
      MATH: 0.164
      BBH: 0.321
      HumanEval: 0.4494
    other:
      GPQA: 0.221
- id: turing-motors/Llama-3-heron-brain-8B-v0.3
  name: Llama 3 heron brain 8B v0.3
  date: '2024-07-01'
  params: 8.0
  base_model: Llama 3 Swallow 8B
  sortkey: llama 3 heron brain v0.3
  url: https://huggingface.co/turing-motors/Llama-3-heron-brain-8B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.4877
      JComQA: 0.9231
      JEMHopQA: 0.4933
      NIILC: 0.5694
      JSQuAD: 0.9056
      XL-Sum: 0.2178
      MGSM: 0.456
      WMT20-en-ja: 0.2771
      WMT20-ja-en: 0.2168
      JMMLU: 0.4993
      JHumanEval: 0.3183
    ja_mtb:
      JMT Avg: 0.497
      coding: 0.362
      extraction: 0.566
      humanities: 0.602
      math: 0.315
      reasoning: 0.426
      roleplay: 0.586
      stem: 0.567
      writing: 0.55
    en_basic:
      En Avg: 0.5511
      OpenBookQA: 0.362
      TriviaQA: 0.6563
      HellaSwag: 0.5688
      SQuAD2: 0.5807
      XWINO: 0.9006
      MMLU: 0.6215
      GSM8K: 0.5777
      MATH: 0.222
      BBH: 0.6409
      HumanEval: 0.3805
    other:
      GPQA: 0.0
- id: turing-motors/Llama-3-heron-brain-70B-v0.3
  name: Llama 3 heron brain 70B v0.3
  date: '2024-07-01'
  params: 70
  base_model: Llama 3 Swallow 70B
  sortkey: llama 3 heron brain v0.3
  url: https://huggingface.co/turing-motors/Llama-3-heron-brain-70B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.6148
      JComQA: 0.9651
      JEMHopQA: 0.652
      NIILC: 0.6792
      JSQuAD: 0.9224
      XL-Sum: 0.2611
      MGSM: 0.772
      WMT20-en-ja: 0.3093
      WMT20-ja-en: 0.2578
      JMMLU: 0.7069
      JHumanEval: 0.6226
    ja_mtb:
      JMT Avg: 0.683
      coding: 0.51
      extraction: 0.87
      humanities: 0.776
      math: 0.68
      reasoning: 0.513
      roleplay: 0.727
      stem: 0.692
      writing: 0.693
    en_basic:
      En Avg: 0.7151
      OpenBookQA: 0.446
      TriviaQA: 0.8107
      HellaSwag: 0.6679
      SQuAD2: 0.7056
      XWINO: 0.9191
      MMLU: 0.7896
      GSM8K: 0.8772
      MATH: 0.508
      BBH: 0.7586
      HumanEval: 0.6683
    other:
      GPQA: 0.346
- id: tokyotech-llm/Llama-3-Swallow-8B-v0.1
  name: Llama 3 Swallow 8B
  date: '2024-07-01'
  params: 8.0
  base_model: ''
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.471
      JComQA: 0.896
      JEMHopQA: 0.478
      NIILC: 0.546
      JSQuAD: 0.9
      XL-Sum: 0.198
      MGSM: 0.44
      WMT20-en-ja: 0.276
      WMT20-ja-en: 0.222
      JMMLU: 0.471
      JHumanEval: 0.282
    en_basic:
      En Avg: 0.523
      OpenBookQA: 0.35
      TriviaQA: 0.656
      HellaSwag: 0.59
      SQuAD2: 0.519
      XWINO: 0.901
      MMLU: 0.615
      GSM8K: 0.483
      MATH: 0.182
      BBH: 0.598
      HumanEval: 0.337
    other:
      GPQA: 0.188
- id: tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1
  name: Llama 3 Swallow 8B Instruct
  date: '2024-07-01'
  params: 8.0
  base_model: Llama 3 Swallow 8B
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.4805
      JComQA: 0.9115
      JEMHopQA: 0.4959
      NIILC: 0.5174
      JSQuAD: 0.9052
      XL-Sum: 0.1283
      MGSM: 0.492
      WMT20-en-ja: 0.2531
      WMT20-ja-en: 0.2274
      JMMLU: 0.4807
      JHumanEval: 0.3939
    ja_mtb:
      JMT Avg: 0.427
      coding: 0.411
      extraction: 0.575
      humanities: 0.476
      math: 0.309
      reasoning: 0.305
      roleplay: 0.499
      stem: 0.438
      writing: 0.406
    en_basic:
      En Avg: 0.5605
      OpenBookQA: 0.37
      TriviaQA: 0.6554
      HellaSwag: 0.5854
      SQuAD2: 0.5672
      XWINO: 0.8994
      MMLU: 0.6331
      GSM8K: 0.5921
      MATH: 0.244
      BBH: 0.6388
      HumanEval: 0.4195
    other:
      GPQA: 0.2567
- id: tokyotech-llm/Llama-3-Swallow-70B-v0.1
  name: Llama 3 Swallow 70B
  date: '2024-07-01'
  params: 70
  base_model: ''
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.594
      JComQA: 0.968
      JEMHopQA: 0.675
      NIILC: 0.684
      JSQuAD: 0.923
      XL-Sum: 0.239
      MGSM: 0.708
      WMT20-en-ja: 0.307
      WMT20-ja-en: 0.255
      JMMLU: 0.706
      JHumanEval: 0.477
    en_basic:
      En Avg: 0.672
      OpenBookQA: 0.43
      TriviaQA: 0.823
      HellaSwag: 0.682
      SQuAD2: 0.628
      XWINO: 0.923
      MMLU: 0.774
      GSM8K: 0.817
      MATH: 0.414
      BBH: 0.734
      HumanEval: 0.499
    other:
      GPQA: 0.319
- id: tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1
  name: Llama 3 Swallow 70B Instruct
  date: '2024-07-01'
  params: 70
  base_model: Llama 3 Swallow 70B
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.5711
      JComQA: 0.9625
      JEMHopQA: 0.6271
      NIILC: 0.5984
      JSQuAD: 0.9206
      XL-Sum: 0.1389
      MGSM: 0.672
      WMT20-en-ja: 0.2715
      WMT20-ja-en: 0.2547
      JMMLU: 0.6572
      JHumanEval: 0.6079
    ja_mtb:
      JMT Avg: 0.618
      coding: 0.633
      extraction: 0.823
      humanities: 0.601
      math: 0.521
      reasoning: 0.482
      roleplay: 0.622
      stem: 0.635
      writing: 0.63
    en_basic:
      En Avg: 0.7158
      OpenBookQA: 0.446
      TriviaQA: 0.8181
      HellaSwag: 0.6757
      SQuAD2: 0.6815
      XWINO: 0.923
      MMLU: 0.7892
      GSM8K: 0.8681
      MATH: 0.46
      BBH: 0.8162
      HumanEval: 0.6799
    other:
      GPQA: 0.3571
- id: rinna/llama-3-youko-8b
  name: Llama 3 Youko 8B
  date: '2024-05-07'
  params: 8.0
  base_model: ''
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-8b
  results:
    ja_basic:
      Ja Avg: 0.442
      JComQA: 0.87
      JEMHopQA: 0.493
      NIILC: 0.513
      JSQuAD: 0.895
      XL-Sum: 0.213
      MGSM: 0.276
      WMT20-en-ja: 0.276
      WMT20-ja-en: 0.219
      JMMLU: 0.449
      JHumanEval: 0.222
    en_basic:
      En Avg: 0.486
      OpenBookQA: 0.348
      TriviaQA: 0.625
      HellaSwag: 0.589
      SQuAD2: 0.502
      XWINO: 0.896
      MMLU: 0.601
      GSM8K: 0.355
      MATH: 0.096
      BBH: 0.571
      HumanEval: 0.281
    other:
      GPQA: 0.063
- id: rinna/llama-3-youko-8b-instruct
  name: Llama 3 Youko 8B Instruct
  date: '2024-05-07'
  params: 8.0
  base_model: Llama 3 Youko 8B
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-8b-instruct
  results:
    ja_basic:
      Ja Avg: 0.4676
      JComQA: 0.9205
      JEMHopQA: 0.4813
      NIILC: 0.517
      JSQuAD: 0.8994
      XL-Sum: 0.2086
      MGSM: 0.472
      WMT20-en-ja: 0.2562
      WMT20-ja-en: 0.1907
      JMMLU: 0.4685
      JHumanEval: 0.2616
    ja_mtb:
      JMT Avg: 0.616
      coding: 0.464
      extraction: 0.757
      humanities: 0.769
      math: 0.414
      reasoning: 0.487
      roleplay: 0.695
      stem: 0.583
      writing: 0.753
    en_basic:
      En Avg: 0.5073
      OpenBookQA: 0.406
      TriviaQA: 0.6125
      HellaSwag: 0.599
      SQuAD2: 0.5589
      XWINO: 0.8972
      MMLU: 0.5965
      GSM8K: 0.5625
      MATH: 0.152
      BBH: 0.4015
      HumanEval: 0.2866
    other:
      GPQA: 0.2902
- id: rinna/llama-3-youko-70b
  name: Llama 3 Youko 70B
  date: '2024-07-25'
  params: 70
  base_model: ''
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-70b
  results:
    ja_basic:
      Ja Avg: 0.571
      JComQA: 0.946
      JEMHopQA: 0.602
      NIILC: 0.61
      JSQuAD: 0.923
      XL-Sum: 0.242
      MGSM: 0.684
      WMT20-en-ja: 0.292
      WMT20-ja-en: 0.25
      JMMLU: 0.704
      JHumanEval: 0.463
    en_basic:
      En Avg: 0.671
      OpenBookQA: 0.436
      TriviaQA: 0.829
      HellaSwag: 0.69
      SQuAD2: 0.61
      XWINO: 0.922
      MMLU: 0.785
      GSM8K: 0.797
      MATH: 0.408
      BBH: 0.826
      HumanEval: 0.412
    other:
      GPQA: 0.25
- id: rinna/llama-3-youko-70b-instruct
  name: Llama 3 Youko 70B Instruct
  date: '2024-07-25'
  params: 70
  base_model: Llama 3 Youko 70B
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-70b-instruct
  results:
    ja_basic:
      Ja Avg: 0.5817
      JComQA: 0.9517
      JEMHopQA: 0.6252
      NIILC: 0.5841
      JSQuAD: 0.9212
      XL-Sum: 0.198
      MGSM: 0.72
      WMT20-en-ja: 0.2629
      WMT20-ja-en: 0.2258
      JMMLU: 0.7184
      JHumanEval: 0.6098
    ja_mtb:
      JMT Avg: 0.75
      coding: 0.607
      extraction: 0.894
      humanities: 0.834
      math: 0.609
      reasoning: 0.673
      roleplay: 0.79
      stem: 0.764
      writing: 0.829
    en_basic:
      En Avg: 0.7085
      OpenBookQA: 0.454
      TriviaQA: 0.7969
      HellaSwag: 0.6861
      SQuAD2: 0.6591
      XWINO: 0.9153
      MMLU: 0.8054
      GSM8K: 0.8923
      MATH: 0.434
      BBH: 0.7801
      HumanEval: 0.6616
    other:
      GPQA: 0.3036
- id: meta-llama/Meta-Llama-3.1-8B
  name: Llama 3.1 8B
  date: '2024-07-23'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
  results:
    ja_basic:
      Ja Avg: 0.437
      JComQA: 0.845
      JEMHopQA: 0.461
      NIILC: 0.405
      JSQuAD: 0.895
      XL-Sum: 0.179
      MGSM: 0.356
      WMT20-en-ja: 0.221
      WMT20-ja-en: 0.21
      JMMLU: 0.479
      JHumanEval: 0.32
    en_basic:
      En Avg: 0.545
      OpenBookQA: 0.38
      TriviaQA: 0.702
      HellaSwag: 0.609
      SQuAD2: 0.503
      XWINO: 0.907
      MMLU: 0.651
      GSM8K: 0.507
      MATH: 0.214
      BBH: 0.616
      HumanEval: 0.364
    other:
      GPQA: 0.17
- id: meta-llama/Meta-Llama-3.1-8B-Instruct
  name: Llama 3.1 8B Instruct
  date: '2024-07-23'
  params: 8.0
  base_model: Llama 3.1 8B
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4698
      JComQA: 0.8803
      JEMHopQA: 0.4469
      NIILC: 0.4072
      JSQuAD: 0.8856
      XL-Sum: 0.1478
      MGSM: 0.516
      WMT20-en-ja: 0.2177
      WMT20-ja-en: 0.1995
      JMMLU: 0.5086
      JHumanEval: 0.4884
    ja_mtb:
      JMT Avg: 0.519
      coding: 0.42
      extraction: 0.83
      humanities: 0.55
      math: 0.514
      reasoning: 0.349
      roleplay: 0.502
      stem: 0.479
      writing: 0.504
    en_basic:
      En Avg: 0.6275
      OpenBookQA: 0.366
      TriviaQA: 0.6993
      HellaSwag: 0.5922
      SQuAD2: 0.6004
      XWINO: 0.9037
      MMLU: 0.6802
      GSM8K: 0.743
      MATH: 0.376
      BBH: 0.6899
      HumanEval: 0.6244
    other:
      GPQA: 0.2835
- id: meta-llama/Meta-Llama-3.1-70B
  name: Llama 3.1 70B
  date: '2024-07-23'
  params: 70
  base_model: ''
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B
  results:
    ja_basic:
      Ja Avg: 0.566
      JComQA: 0.946
      JEMHopQA: 0.616
      NIILC: 0.603
      JSQuAD: 0.925
      XL-Sum: 0.228
      MGSM: 0.672
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.257
      JMMLU: 0.669
      JHumanEval: 0.462
    en_basic:
      En Avg: 0.671
      OpenBookQA: 0.45
      TriviaQA: 0.829
      HellaSwag: 0.69
      SQuAD2: 0.605
      XWINO: 0.92
      MMLU: 0.786
      GSM8K: 0.798
      MATH: 0.434
      BBH: 0.655
      HumanEval: 0.546
    other:
      GPQA: 0.188
- id: meta-llama/Meta-Llama-3.1-70B-Instruct
  name: Llama 3.1 70B Instruct
  date: '2024-07-23'
  params: 70
  base_model: Llama 3.1 70B
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.595
      JComQA: 0.95
      JEMHopQA: 0.6347
      NIILC: 0.5789
      JSQuAD: 0.9208
      XL-Sum: 0.1779
      MGSM: 0.732
      WMT20-en-ja: 0.2792
      WMT20-ja-en: 0.2475
      JMMLU: 0.7331
      JHumanEval: 0.6957
    ja_mtb:
      JMT Avg: 0.706
      coding: 0.691
      extraction: 0.848
      humanities: 0.73
      math: 0.669
      reasoning: 0.618
      roleplay: 0.699
      stem: 0.699
      writing: 0.694
    en_basic:
      En Avg: 0.7381
      OpenBookQA: 0.426
      TriviaQA: 0.821
      HellaSwag: 0.6622
      SQuAD2: 0.6602
      XWINO: 0.917
      MMLU: 0.8222
      GSM8K: 0.8764
      MATH: 0.56
      BBH: 0.8417
      HumanEval: 0.7939
    other:
      GPQA: 0.4353
- id: cyberagent/Llama-3.1-70B-Japanese-Instruct-2407
  name: Llama-3.1-70B-Japanese-Instruct-2407
  date: '2024-07-23'
  params: 70
  base_model: Llama 3.1 70B Instruct
  sortkey: llama 3.1 japanese 2407
  url: https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407
  results:
    ja_basic:
      Ja Avg: 0.5967
      JComQA: 0.9562
      JEMHopQA: 0.6466
      NIILC: 0.6602
      JSQuAD: 0.9187
      XL-Sum: 0.1564
      MGSM: 0.748
      WMT20-en-ja: 0.2901
      WMT20-ja-en: 0.241
      JMMLU: 0.7227
      JHumanEval: 0.6274
    ja_mtb:
      JMT Avg: 0.751
      coding: 0.683
      extraction: 0.827
      humanities: 0.824
      math: 0.749
      reasoning: 0.643
      roleplay: 0.818
      stem: 0.715
      writing: 0.751
    en_basic:
      En Avg: 0.7253
      OpenBookQA: 0.422
      TriviaQA: 0.8104
      HellaSwag: 0.6472
      SQuAD2: 0.6631
      XWINO: 0.917
      MMLU: 0.8074
      GSM8K: 0.8893
      MATH: 0.528
      BBH: 0.8228
      HumanEval: 0.7463
    other:
      GPQA: 0.4442
- id: tokyotech-llm/Llama-3.1-Swallow-8B-v0.1
  name: Llama 3.1 Swallow 8B v0.1
  date: '2024-10-08'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.49
      JComQA: 0.912
      JEMHopQA: 0.509
      NIILC: 0.601
      JSQuAD: 0.899
      XL-Sum: 0.202
      MGSM: 0.46
      WMT20-en-ja: 0.291
      WMT20-ja-en: 0.231
      JMMLU: 0.518
      JHumanEval: 0.276
    en_basic:
      En Avg: 0.538
      OpenBookQA: 0.378
      TriviaQA: 0.671
      HellaSwag: 0.605
      SQuAD2: 0.502
      XWINO: 0.905
      MMLU: 0.624
      GSM8K: 0.511
      MATH: 0.224
      BBH: 0.615
      HumanEval: 0.348
    other:
      GPQA: 0.234
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1
  name: Llama 3.1 Swallow 8B Instruct v0.1
  date: '2024-10-08'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.1
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.5055
      JComQA: 0.924
      JEMHopQA: 0.5874
      NIILC: 0.5736
      JSQuAD: 0.917
      XL-Sum: 0.138
      MGSM: 0.508
      WMT20-en-ja: 0.282
      WMT20-ja-en: 0.2282
      JMMLU: 0.5301
      JHumanEval: 0.3665
    ja_mtb:
      JMT Avg: 0.581
      coding: 0.427
      extraction: 0.738
      humanities: 0.675
      math: 0.527
      reasoning: 0.453
      roleplay: 0.615
      stem: 0.593
      writing: 0.624
    en_basic:
      En Avg: 0.5626
      OpenBookQA: 0.388
      TriviaQA: 0.6488
      HellaSwag: 0.6153
      SQuAD2: 0.5978
      XWINO: 0.8912
      MMLU: 0.6238
      GSM8K: 0.605
      MATH: 0.236
      BBH: 0.6417
      HumanEval: 0.3787
    other:
      GPQA: 0.2813
- id: tokyotech-llm/Llama-3.1-Swallow-70B-v0.1
  name: Llama 3.1 Swallow 70B v0.1
  date: '2024-10-08'
  params: 70
  base_model: ''
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.593
      JComQA: 0.955
      JEMHopQA: 0.645
      NIILC: 0.678
      JSQuAD: 0.923
      XL-Sum: 0.272
      MGSM: 0.684
      WMT20-en-ja: 0.32
      WMT20-ja-en: 0.259
      JMMLU: 0.709
      JHumanEval: 0.487
    en_basic:
      En Avg: 0.679
      OpenBookQA: 0.428
      TriviaQA: 0.826
      HellaSwag: 0.69
      SQuAD2: 0.612
      XWINO: 0.927
      MMLU: 0.772
      GSM8K: 0.809
      MATH: 0.38
      BBH: 0.806
      HumanEval: 0.54
    other:
      GPQA: 0.288
- id: tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1
  name: Llama 3.1 Swallow 70B Instruct v0.1
  date: '2024-10-08'
  params: 70
  base_model: Llama 3.1 Swallow 70B v0.1
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.5884
      JComQA: 0.9616
      JEMHopQA: 0.6206
      NIILC: 0.6601
      JSQuAD: 0.9238
      XL-Sum: 0.1922
      MGSM: 0.776
      WMT20-en-ja: 0.3119
      WMT20-ja-en: 0.259
      JMMLU: 0.7108
      JHumanEval: 0.4677
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.654
      extraction: 0.792
      humanities: 0.768
      math: 0.704
      reasoning: 0.573
      roleplay: 0.682
      stem: 0.653
      writing: 0.704
    en_basic:
      En Avg: 0.7102
      OpenBookQA: 0.446
      TriviaQA: 0.8149
      HellaSwag: 0.6831
      SQuAD2: 0.6807
      XWINO: 0.9166
      MMLU: 0.7868
      GSM8K: 0.884
      MATH: 0.474
      BBH: 0.8481
      HumanEval: 0.5683
    other:
      GPQA: 0.3951
- id: tokyotech-llm/Llama-3.1-Swallow-8B-v0.2
  name: Llama 3.1 Swallow 8B v0.2
  date: '2024-11-11'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1 swallow v0.2
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2
  results:
    ja_basic:
      Ja Avg: 0.499
      JComQA: 0.911
      JEMHopQA: 0.51
      NIILC: 0.627
      JSQuAD: 0.892
      XL-Sum: 0.198
      MGSM: 0.464
      WMT20-en-ja: 0.296
      WMT20-ja-en: 0.233
      JMMLU: 0.525
      JHumanEval: 0.336
    en_basic:
      En Avg: 0.539
      OpenBookQA: 0.382
      TriviaQA: 0.651
      HellaSwag: 0.596
      SQuAD2: 0.513
      XWINO: 0.904
      MMLU: 0.622
      GSM8K: 0.521
      MATH: 0.228
      BBH: 0.605
      HumanEval: 0.366
    other:
      GPQA: 0.225
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2
  name: Llama 3.1 Swallow 8B Instruct v0.2
  date: '2024-11-11'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.2
  sortkey: llama 3.1 swallow v0.2
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2
  results:
    ja_basic:
      Ja Avg: 0.5141
      JComQA: 0.9294
      JEMHopQA: 0.5601
      NIILC: 0.5988
      JSQuAD: 0.9148
      XL-Sum: 0.1372
      MGSM: 0.528
      WMT20-en-ja: 0.2878
      WMT20-ja-en: 0.227
      JMMLU: 0.5504
      JHumanEval: 0.4079
    ja_mtb:
      JMT Avg: 0.612
      coding: 0.534
      extraction: 0.748
      humanities: 0.705
      math: 0.565
      reasoning: 0.475
      roleplay: 0.646
      stem: 0.579
      writing: 0.646
    en_basic:
      En Avg: 0.5743
      OpenBookQA: 0.38
      TriviaQA: 0.6252
      HellaSwag: 0.6034
      SQuAD2: 0.6066
      XWINO: 0.8873
      MMLU: 0.6343
      GSM8K: 0.6202
      MATH: 0.264
      BBH: 0.6487
      HumanEval: 0.4738
    other:
      GPQA: 0.3013
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  name: Llama 3.1 Swallow 8B Instruct v0.3
  date: '2024-12-23'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.2
  sortkey: llama 3.1 swallow v0.3
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.5101
      JComQA: 0.924
      JEMHopQA: 0.5278
      NIILC: 0.5825
      JSQuAD: 0.8956
      XL-Sum: 0.1909
      MGSM: 0.532
      WMT20-en-ja: 0.2809
      WMT20-ja-en: 0.2287
      JMMLU: 0.5445
      JHumanEval: 0.3939
    ja_mtb:
      JMT Avg: 0.705
      coding: 0.562
      extraction: 0.756
      humanities: 0.869
      math: 0.61
      reasoning: 0.512
      roleplay: 0.783
      stem: 0.748
      writing: 0.803
    en_basic:
      En Avg: 0.5662
      OpenBookQA: 0.396
      TriviaQA: 0.6291
      HellaSwag: 0.5933
      SQuAD2: 0.5698
      XWINO: 0.8843
      MMLU: 0.6293
      GSM8K: 0.6224
      MATH: 0.266
      BBH: 0.6263
      HumanEval: 0.4451
    other:
      GPQA: 0.2009
- id: tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3
  name: Llama 3.1 Swallow 70B Instruct v0.3
  date: '2024-12-30'
  params: 70
  base_model: Llama 3.1 Swallow 70B v0.1
  sortkey: llama 3.1 swallow v0.3
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.5977
      JComQA: 0.9643
      JEMHopQA: 0.6322
      NIILC: 0.6539
      JSQuAD: 0.9105
      XL-Sum: 0.1956
      MGSM: 0.772
      WMT20-en-ja: 0.3052
      WMT20-ja-en: 0.2572
      JMMLU: 0.6899
      JHumanEval: 0.5957
    ja_mtb:
      JMT Avg: 0.769
      coding: 0.678
      extraction: 0.82
      humanities: 0.867
      math: 0.776
      reasoning: 0.57
      roleplay: 0.816
      stem: 0.769
      writing: 0.852
    en_basic:
      En Avg: 0.7102
      OpenBookQA: 0.454
      TriviaQA: 0.8246
      HellaSwag: 0.6917
      SQuAD2: 0.6469
      XWINO: 0.9191
      MMLU: 0.7768
      GSM8K: 0.8719
      MATH: 0.458
      BBH: 0.816
      HumanEval: 0.6427
    other:
      GPQA: 0.2589
- id: meta-llama/Llama-3.2-1B
  name: Llama 3.2 1B
  date: '2024-09-25'
  params: 1.2
  base_model: ''
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-1B
  results:
    ja_basic:
      Ja Avg: 0.201
      JComQA: 0.208
      JEMHopQA: 0.404
      NIILC: 0.188
      JSQuAD: 0.525
      XL-Sum: 0.081
      MGSM: 0.024
      WMT20-en-ja: 0.079
      WMT20-ja-en: 0.092
      JMMLU: 0.26
      JHumanEval: 0.15
    en_basic:
      En Avg: 0.339
      OpenBookQA: 0.3
      TriviaQA: 0.388
      HellaSwag: 0.477
      SQuAD2: 0.501
      XWINO: 0.849
      MMLU: 0.313
      GSM8K: 0.049
      MATH: 0.02
      BBH: 0.303
      HumanEval: 0.193
    other:
      GPQA: 0.018
- id: meta-llama/Llama-3.2-1B-Instruct
  name: Llama 3.2 1B Instruct
  date: '2024-09-25'
  params: 1.2
  base_model: Llama 3.2 1B
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.2386
      JComQA: 0.3968
      JEMHopQA: 0.346
      NIILC: 0.1789
      JSQuAD: 0.5704
      XL-Sum: 0.0749
      MGSM: 0.164
      WMT20-en-ja: 0.0698
      WMT20-ja-en: 0.0906
      JMMLU: 0.2875
      JHumanEval: 0.2067
    ja_mtb:
      JMT Avg: 0.273
      coding: 0.254
      extraction: 0.376
      humanities: 0.218
      math: 0.307
      reasoning: 0.267
      roleplay: 0.262
      stem: 0.246
      writing: 0.258
    en_basic:
      En Avg: 0.4079
      OpenBookQA: 0.274
      TriviaQA: 0.3749
      HellaSwag: 0.4396
      SQuAD2: 0.5014
      XWINO: 0.8366
      MMLU: 0.4543
      GSM8K: 0.3177
      MATH: 0.172
      BBH: 0.362
      HumanEval: 0.347
    other:
      GPQA: 0.0513
- id: meta-llama/Llama-3.2-3B
  name: Llama 3.2 3B
  date: '2024-09-25'
  params: 3.2
  base_model: ''
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-3B
  results:
    ja_basic:
      Ja Avg: 0.337
      JComQA: 0.605
      JEMHopQA: 0.443
      NIILC: 0.324
      JSQuAD: 0.816
      XL-Sum: 0.129
      MGSM: 0.136
      WMT20-en-ja: 0.161
      WMT20-ja-en: 0.167
      JMMLU: 0.352
      JHumanEval: 0.235
    en_basic:
      En Avg: 0.45
      OpenBookQA: 0.326
      TriviaQA: 0.586
      HellaSwag: 0.558
      SQuAD2: 0.502
      XWINO: 0.888
      MMLU: 0.558
      GSM8K: 0.262
      MATH: 0.07
      BBH: 0.466
      HumanEval: 0.285
    other:
      GPQA: 0.042
- id: meta-llama/Llama-3.2-3B-Instruct
  name: Llama 3.2 3B Instruct
  date: '2024-09-25'
  params: 3.2
  base_model: Llama 3.2 3B
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.3803
      JComQA: 0.7828
      JEMHopQA: 0.3036
      NIILC: 0.2682
      JSQuAD: 0.8464
      XL-Sum: 0.1115
      MGSM: 0.372
      WMT20-en-ja: 0.1733
      WMT20-ja-en: 0.1551
      JMMLU: 0.4038
      JHumanEval: 0.3866
    ja_mtb:
      JMT Avg: 0.405
      coding: 0.426
      extraction: 0.593
      humanities: 0.431
      math: 0.389
      reasoning: 0.292
      roleplay: 0.35
      stem: 0.38
      writing: 0.38
    en_basic:
      En Avg: 0.5374
      OpenBookQA: 0.306
      TriviaQA: 0.5562
      HellaSwag: 0.5245
      SQuAD2: 0.5397
      XWINO: 0.8735
      MMLU: 0.597
      GSM8K: 0.6293
      MATH: 0.324
      BBH: 0.5124
      HumanEval: 0.511
    other:
      GPQA: 0.2589
- id: meta-llama/Llama-3.3-70B-Instruct
  name: Llama 3.3 70B Instruct
  date: '2024-12-06'
  params: 70
  base_model: Llama 3.1 70B
  sortkey: llama 3.3
  url: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.6005
      JComQA: 0.941
      JEMHopQA: 0.6399
      NIILC: 0.5699
      JSQuAD: 0.8926
      XL-Sum: 0.1787
      MGSM: 0.784
      WMT20-en-ja: 0.2779
      WMT20-ja-en: 0.2429
      JMMLU: 0.7345
      JHumanEval: 0.7439
    ja_mtb:
      JMT Avg: 0.737
      coding: 0.707
      extraction: 0.865
      humanities: 0.757
      math: 0.72
      reasoning: 0.635
      roleplay: 0.773
      stem: 0.706
      writing: 0.733
    en_basic:
      En Avg: 0.7619
      OpenBookQA: 0.426
      TriviaQA: 0.8172
      HellaSwag: 0.6674
      SQuAD2: 0.6837
      XWINO: 0.9174
      MMLU: 0.8241
      GSM8K: 0.8901
      MATH: 0.706
      BBH: 0.8529
      HumanEval: 0.8341
    other:
      GPQA: 0.5915
- id: tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  name: Llama 3.3 Swallow 70B v0.4
  date: '2025-03-14'
  params: 70
  base_model: ''
  sortkey: llama 3.3 swallow v0.4
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  results:
    ja_basic:
      Ja Avg: 0.629
      JComQA: 0.967
      JEMHopQA: 0.671
      NIILC: 0.732
      JSQuAD: 0.924
      XL-Sum: 0.283
      MGSM: 0.776
      WMT20-en-ja: 0.327
      WMT20-ja-en: 0.26
      JMMLU: 0.742
      JHumanEval: 0.604
    en_basic:
      En Avg: 0.711
      OpenBookQA: 0.424
      TriviaQA: 0.817
      HellaSwag: 0.683
      SQuAD2: 0.641
      XWINO: 0.92
      MMLU: 0.802
      GSM8K: 0.863
      MATH: 0.496
      BBH: 0.754
      HumanEval: 0.709
    other:
      GPQA: 0.163
- id: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.1-stage2-iter_0002500
  name: Llama 3.3 Swallow 70B Instruct v0.4
  date: '2025-03-10'
  params: 70
  base_model: Llama 3.3 Swallow 70B v0.4
  sortkey: llama 3.3 swallow v0.4
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.1-stage2-iter_0002500
  results:
    ja_basic:
      Ja Avg: 0.6129
      JComQA: 0.9812
      JEMHopQA: 0.6182
      NIILC: 0.662
      JSQuAD: 0.9067
      XL-Sum: 0.162
      MGSM: 0.812
      WMT20-en-ja: 0.3191
      WMT20-ja-en: 0.2605
      JMMLU: 0.7074
      JHumanEval: 0.7
    ja_mtb:
      JMT Avg: 0.772
      coding: 0.705
      extraction: 0.82
      humanities: 0.87
      math: 0.73
      reasoning: 0.623
      roleplay: 0.811
      stem: 0.781
      writing: 0.832
    en_basic:
      En Avg: 0.7356
      OpenBookQA: 0.448
      TriviaQA: 0.8171
      HellaSwag: 0.6859
      SQuAD2: 0.6539
      XWINO: 0.9123
      MMLU: 0.8028
      GSM8K: 0.9075
      MATH: 0.566
      BBH: 0.8123
      HumanEval: 0.75
    other:
      GPQA: 0.4018
- id: llm-jp/llm-jp-3-1.8b
  name: llm-jp-3-1.8b
  date: '2024-09-25'
  params: 1.8
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-1.8b
  results:
    ja_basic:
      Ja Avg: 0.251
      JComQA: 0.209
      JEMHopQA: 0.463
      NIILC: 0.449
      JSQuAD: 0.703
      XL-Sum: 0.1
      MGSM: 0.012
      WMT20-en-ja: 0.198
      WMT20-ja-en: 0.134
      JMMLU: 0.242
      JHumanEval: 0.001
    en_basic:
      En Avg: 0.293
      OpenBookQA: 0.244
      TriviaQA: 0.301
      HellaSwag: 0.462
      SQuAD2: 0.501
      XWINO: 0.851
      MMLU: 0.248
      GSM8K: 0.017
      MATH: 0.018
      BBH: 0.276
      HumanEval: 0.008
    other:
      GPQA: 0.0
- id: llm-jp/llm-jp-3-1.8b-instruct
  name: llm-jp-3-1.8b-instruct
  date: '2024-09-25'
  params: 1.8
  base_model: llm-jp-3-1.8b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct
  results:
    ja_basic:
      Ja Avg: 0.2927
      JComQA: 0.3244
      JEMHopQA: 0.4131
      NIILC: 0.4663
      JSQuAD: 0.8373
      XL-Sum: 0.1049
      MGSM: 0.08
      WMT20-en-ja: 0.2059
      WMT20-ja-en: 0.1424
      JMMLU: 0.2917
      JHumanEval: 0.061
    ja_mtb:
      JMT Avg: 0.451
      coding: 0.274
      extraction: 0.321
      humanities: 0.68
      math: 0.281
      reasoning: 0.301
      roleplay: 0.628
      stem: 0.504
      writing: 0.617
    en_basic:
      En Avg: 0.3129
      OpenBookQA: 0.286
      TriviaQA: 0.2961
      HellaSwag: 0.4854
      SQuAD2: 0.5018
      XWINO: 0.8469
      MMLU: 0.2772
      GSM8K: 0.0432
      MATH: 0.016
      BBH: 0.2901
      HumanEval: 0.0866
    other:
      GPQA: 0.1228
- id: llm-jp/llm-jp-3-3.7b
  name: llm-jp-3-3.7b
  date: '2024-09-25'
  params: 3.7
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-3.7b
  results:
    ja_basic:
      Ja Avg: 0.281
      JComQA: 0.203
      JEMHopQA: 0.431
      NIILC: 0.541
      JSQuAD: 0.804
      XL-Sum: 0.142
      MGSM: 0.06
      WMT20-en-ja: 0.223
      WMT20-ja-en: 0.159
      JMMLU: 0.249
      JHumanEval: 0.0
    en_basic:
      En Avg: 0.324
      OpenBookQA: 0.28
      TriviaQA: 0.421
      HellaSwag: 0.506
      SQuAD2: 0.502
      XWINO: 0.876
      MMLU: 0.253
      GSM8K: 0.055
      MATH: 0.016
      BBH: 0.309
      HumanEval: 0.019
    other:
      GPQA: 0.007
- id: llm-jp/llm-jp-3-3.7b-instruct
  name: llm-jp-3-3.7b-instruct
  date: '2024-09-25'
  params: 3.7
  base_model: llm-jp-3-3.7b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct
  results:
    ja_basic:
      Ja Avg: 0.3501
      JComQA: 0.5326
      JEMHopQA: 0.4637
      NIILC: 0.5285
      JSQuAD: 0.8469
      XL-Sum: 0.1393
      MGSM: 0.152
      WMT20-en-ja: 0.224
      WMT20-ja-en: 0.1702
      JMMLU: 0.3587
      JHumanEval: 0.0848
    ja_mtb:
      JMT Avg: 0.485
      coding: 0.311
      extraction: 0.418
      humanities: 0.73
      math: 0.311
      reasoning: 0.339
      roleplay: 0.618
      stem: 0.551
      writing: 0.6
    en_basic:
      En Avg: 0.3473
      OpenBookQA: 0.31
      TriviaQA: 0.3981
      HellaSwag: 0.5339
      SQuAD2: 0.5035
      XWINO: 0.8624
      MMLU: 0.349
      GSM8K: 0.0713
      MATH: 0.022
      BBH: 0.3244
      HumanEval: 0.0988
    other:
      GPQA: 0.1116
- id: llm-jp/llm-jp-3-13b
  name: llm-jp-3-13b
  date: '2024-09-25'
  params: 13
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-13b
  results:
    ja_basic:
      Ja Avg: 0.393
      JComQA: 0.65
      JEMHopQA: 0.525
      NIILC: 0.649
      JSQuAD: 0.882
      XL-Sum: 0.164
      MGSM: 0.16
      WMT20-en-ja: 0.273
      WMT20-ja-en: 0.21
      JMMLU: 0.399
      JHumanEval: 0.023
    en_basic:
      En Avg: 0.399
      OpenBookQA: 0.332
      TriviaQA: 0.602
      HellaSwag: 0.57
      SQuAD2: 0.501
      XWINO: 0.902
      MMLU: 0.462
      GSM8K: 0.158
      MATH: 0.026
      BBH: 0.402
      HumanEval: 0.032
    other:
      GPQA: 0.094
- id: llm-jp/llm-jp-3-13b-instruct
  name: llm-jp-3-13b-instruct
  date: '2024-09-25'
  params: 13
  base_model: llm-jp-3-13b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-13b-instruct
  results:
    ja_basic:
      Ja Avg: 0.4358
      JComQA: 0.8937
      JEMHopQA: 0.339
      NIILC: 0.6382
      JSQuAD: 0.9014
      XL-Sum: 0.1507
      MGSM: 0.324
      WMT20-en-ja: 0.2523
      WMT20-ja-en: 0.2034
      JMMLU: 0.4677
      JHumanEval: 0.1878
    ja_mtb:
      JMT Avg: 0.588
      coding: 0.373
      extraction: 0.556
      humanities: 0.816
      math: 0.371
      reasoning: 0.526
      roleplay: 0.73
      stem: 0.614
      writing: 0.715
    en_basic:
      En Avg: 0.4318
      OpenBookQA: 0.342
      TriviaQA: 0.534
      HellaSwag: 0.5944
      SQuAD2: 0.5165
      XWINO: 0.892
      MMLU: 0.5056
      GSM8K: 0.2434
      MATH: 0.046
      BBH: 0.4382
      HumanEval: 0.2055
    other:
      GPQA: 0.2009
- id: mistralai/Mistral-Nemo-Base-2407
  name: Mistral-Nemo-Base-2407 (12B)
  date: '2024-07-18'
  params: 12
  base_model: ''
  sortkey: mistral nemo 2407 ()
  url: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407
  results:
    ja_basic:
      Ja Avg: 0.46
      JComQA: 0.911
      JEMHopQA: 0.516
      NIILC: 0.475
      JSQuAD: 0.904
      XL-Sum: 0.192
      MGSM: 0.416
      WMT20-en-ja: 0.244
      WMT20-ja-en: 0.212
      JMMLU: 0.538
      JHumanEval: 0.194
    en_basic:
      En Avg: 0.559
      OpenBookQA: 0.422
      TriviaQA: 0.741
      HellaSwag: 0.647
      SQuAD2: 0.528
      XWINO: 0.914
      MMLU: 0.69
      GSM8K: 0.55
      MATH: 0.184
      BBH: 0.657
      HumanEval: 0.259
    other:
      GPQA: 0.007
- id: mistralai/Mistral-Nemo-Instruct-2407
  name: Mistral-NeMo-Instruct-2407 (12B)
  date: '2024-07-18'
  params: 12
  base_model: Mistral-Nemo-Base-2407 (12B)
  sortkey: mistral nemo 2407 ()
  url: https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407
  results:
    ja_basic:
      Ja Avg: 0.5004
      JComQA: 0.9267
      JEMHopQA: 0.4967
      NIILC: 0.4841
      JSQuAD: 0.9046
      XL-Sum: 0.1764
      MGSM: 0.552
      WMT20-en-ja: 0.2402
      WMT20-ja-en: 0.2054
      JMMLU: 0.5479
      JHumanEval: 0.4695
    ja_mtb:
      JMT Avg: 0.616
      coding: 0.515
      extraction: 0.698
      humanities: 0.702
      math: 0.512
      reasoning: 0.481
      roleplay: 0.669
      stem: 0.66
      writing: 0.691
    en_basic:
      En Avg: 0.6081
      OpenBookQA: 0.406
      TriviaQA: 0.7259
      HellaSwag: 0.6451
      SQuAD2: 0.6061
      XWINO: 0.9114
      MMLU: 0.6831
      GSM8K: 0.721
      MATH: 0.274
      BBH: 0.5372
      HumanEval: 0.5713
    other:
      GPQA: 0.2076
- id: nvidia/Mistral-NeMo-Minitron-8B-Base
  name: Mistral-NeMo-Minitron 8B
  date: '2024-08-21'
  params: 8.4
  base_model: ''
  sortkey: mistral nemo minitron
  url: https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base
  results:
    ja_basic:
      Ja Avg: 0.444
      JComQA: 0.887
      JEMHopQA: 0.486
      NIILC: 0.374
      JSQuAD: 0.902
      XL-Sum: 0.157
      MGSM: 0.424
      WMT20-en-ja: 0.186
      WMT20-ja-en: 0.193
      JMMLU: 0.494
      JHumanEval: 0.332
    en_basic:
      En Avg: 0.572
      OpenBookQA: 0.406
      TriviaQA: 0.728
      HellaSwag: 0.621
      SQuAD2: 0.525
      XWINO: 0.915
      MMLU: 0.694
      GSM8K: 0.585
      MATH: 0.202
      BBH: 0.658
      HumanEval: 0.382
    other:
      GPQA: 0.179
- id: nvidia/Mistral-NeMo-Minitron-8B-Instruct
  name: Mistral-NeMo-Minitron 8B Instruct
  date: '2024-08-21'
  params: 8.4
  base_model: Mistral-NeMo-Minitron 8B
  sortkey: mistral nemo minitron
  url: https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4781
      JComQA: 0.8919
      JEMHopQA: 0.4979
      NIILC: 0.3801
      JSQuAD: 0.5781
      XL-Sum: 0
      MGSM: 0.556
      WMT20-en-ja: 0.1992
      WMT20-ja-en: 0.1927
      JMMLU: 0.5103
      JHumanEval: 0.4963
    ja_mtb:
      JMT Avg: 0.567
      coding: 0.547
      extraction: 0.684
      humanities: 0.649
      math: 0.545
      reasoning: 0.454
      roleplay: 0.564
      stem: 0.549
      writing: 0.541
    en_basic:
      En Avg: 0.6336
      OpenBookQA: 0.452
      TriviaQA: 0.7192
      HellaSwag: 0.6392
      SQuAD2: 0.6239
      XWINO: 0.9092
      MMLU: 0.7008
      GSM8K: 0.7536
      MATH: 0.274
      BBH: 0.6632
      HumanEval: 0.6006
    other:
      GPQA: 0.2835
- id: mistralai/Mistral-7B-v0.3
  name: Mistral-7B-v0.3
  date: '2024-05-22'
  params: 7.2
  base_model: ''
  sortkey: mistral v0.3
  url: https://huggingface.co/mistralai/Mistral-7B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.361
      JComQA: 0.714
      JEMHopQA: 0.474
      NIILC: 0.245
      JSQuAD: 0.847
      XL-Sum: 0.212
      MGSM: 0.156
      WMT20-en-ja: 0.142
      WMT20-ja-en: 0.171
      JMMLU: 0.404
      JHumanEval: 0.242
    en_basic:
      En Avg: 0.507
      OpenBookQA: 0.374
      TriviaQA: 0.695
      HellaSwag: 0.622
      SQuAD2: 0.511
      XWINO: 0.909
      MMLU: 0.623
      GSM8K: 0.361
      MATH: 0.116
      BBH: 0.585
      HumanEval: 0.273
    other:
      GPQA: 0.129
- id: mistralai/Mistral-7B-Instruct-v0.3
  name: Mistral-7B-Instruct-v0.3
  date: '2024-05-22'
  params: 7.2
  base_model: Mistral-7B-v0.3
  sortkey: mistral v0.3
  url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.3778
      JComQA: 0.7542
      JEMHopQA: 0.4467
      NIILC: 0.2677
      JSQuAD: 0.8695
      XL-Sum: 0.2053
      MGSM: 0.224
      WMT20-en-ja: 0.1634
      WMT20-ja-en: 0.177
      JMMLU: 0.4033
      JHumanEval: 0.2665
    ja_mtb:
      JMT Avg: 0.428
      coding: 0.488
      extraction: 0.54
      humanities: 0.435
      math: 0.354
      reasoning: 0.392
      roleplay: 0.409
      stem: 0.405
      writing: 0.401
    en_basic:
      En Avg: 0.5407
      OpenBookQA: 0.408
      TriviaQA: 0.6767
      HellaSwag: 0.6522
      SQuAD2: 0.5762
      XWINO: 0.9054
      MMLU: 0.6206
      GSM8K: 0.4996
      MATH: 0.16
      BBH: 0.563
      HumanEval: 0.3457
    other:
      GPQA: 0.2768
- id: mistralai/Mixtral-8x22B-v0.1
  name: Mixtral-8x22B-v0.1
  date: '2024-04-17'
  params: 141
  base_model: ''
  sortkey: mixtral v0.1
  url: https://huggingface.co/mistralai/Mixtral-8x22B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.496
      JComQA: 0.895
      JEMHopQA: 0.512
      NIILC: 0.42
      JSQuAD: 0.914
      XL-Sum: 0.241
      MGSM: 0.544
      WMT20-en-ja: 0.229
      WMT20-ja-en: 0.229
      JMMLU: 0.604
      JHumanEval: 0.371
    en_basic:
      En Avg: 0.652
      OpenBookQA: 0.42
      TriviaQA: 0.833
      HellaSwag: 0.696
      SQuAD2: 0.593
      XWINO: 0.919
      MMLU: 0.772
      GSM8K: 0.754
      MATH: 0.414
      BBH: 0.811
      HumanEval: 0.309
    other:
      GPQA: 0.19
- id: mistralai/Mixtral-8x22B-Instruct-v0.1
  name: Mixtral-8x22B-Instruct-v0.1
  date: '2024-04-17'
  params: 141
  base_model: Mixtral-8x22B-v0.1
  sortkey: mixtral v0.1
  url: https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.5323
      JComQA: 0.9026
      JEMHopQA: 0.4978
      NIILC: 0.4464
      JSQuAD: 0.9184
      XL-Sum: 0.2071
      MGSM: 0.696
      WMT20-en-ja: 0.2333
      WMT20-ja-en: 0.2316
      JMMLU: 0.6018
      JHumanEval: 0.5884
    ja_mtb:
      JMT Avg: 0.622
      coding: 0.591
      extraction: 0.797
      humanities: 0.606
      math: 0.585
      reasoning: 0.557
      roleplay: 0.618
      stem: 0.565
      writing: 0.658
    en_basic:
      En Avg: 0.7196
      OpenBookQA: 0.45
      TriviaQA: 0.8274
      HellaSwag: 0.7078
      SQuAD2: 0.6763
      XWINO: 0.9196
      MMLU: 0.7739
      GSM8K: 0.8324
      MATH: 0.456
      BBH: 0.8295
      HumanEval: 0.7232
    other:
      GPQA: 0.3348
- id: microsoft/Phi-3-mini-128k-instruct
  name: Phi-3-Mini-128K-Instruct
  date: '2024-04-23'
  params: 3.8
  base_model: (private)
  sortkey: phi 3 mini 128k
  url: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
  results:
    ja_basic:
      Ja Avg: 0.3819
      JComQA: 0.7203
      JEMHopQA: 0.3943
      NIILC: 0.2082
      JSQuAD: 0.8319
      XL-Sum: 0.1323
      MGSM: 0.408
      WMT20-en-ja: 0.1502
      WMT20-ja-en: 0.1363
      JMMLU: 0.4089
      JHumanEval: 0.428
    ja_mtb:
      JMT Avg: 0.524
      coding: 0.535
      extraction: 0.68
      humanities: 0.553
      math: 0.514
      reasoning: 0.416
      roleplay: 0.505
      stem: 0.465
      writing: 0.525
    en_basic:
      En Avg: 0.6145
      OpenBookQA: 0.422
      TriviaQA: 0.5265
      HellaSwag: 0.6053
      SQuAD2: 0.5594
      XWINO: 0.8714
      MMLU: 0.6953
      GSM8K: 0.7589
      MATH: 0.368
      BBH: 0.7114
      HumanEval: 0.6268
    other:
      GPQA: 0.2746
- id: microsoft/phi-4
  name: Phi-4
  date: '2024-12-13'
  params: 14
  base_model: (private)
  sortkey: phi 4
  url: https://huggingface.co/microsoft/phi-4
  results:
    ja_basic:
      Ja Avg: 0.5797
      JComQA: 0.9446
      JEMHopQA: 0.6075
      NIILC: 0.5068
      JSQuAD: 0.9231
      XL-Sum: 0.2189
      MGSM: 0.796
      WMT20-en-ja: 0.2831
      WMT20-ja-en: 0.2308
      JMMLU: 0.6885
      JHumanEval: 0.5982
    ja_mtb:
      JMT Avg: 0.769
      coding: 0.692
      extraction: 0.929
      humanities: 0.795
      math: 0.914
      reasoning: 0.544
      roleplay: 0.754
      stem: 0.688
      writing: 0.84
    en_basic:
      En Avg: 0.6769
      OpenBookQA: 0.378
      TriviaQA: 0.6823
      HellaSwag: 0.6468
      SQuAD2: 0.6461
      XWINO: 0.9032
      MMLU: 0.802
      GSM8K: 0.8992
      MATH: 0.556
      BBH: 0.6538
      HumanEval: 0.6012
    other:
      GPQA: 0.0
- id: pfnet/plamo-2-1b
  name: PLaMo 2 1B
  date: '2025-02-21'
  params: 1.3
  base_model: ''
  sortkey: plamo 2
  url: https://huggingface.co/pfnet/plamo-2-1b
  results:
    ja_basic:
      Ja Avg: 0.25
      JComQA: 0.203
      JEMHopQA: 0.463
      NIILC: 0.434
      JSQuAD: 0.626
      XL-Sum: 0.055
      MGSM: 0.052
      WMT20-en-ja: 0.236
      WMT20-ja-en: 0.119
      JMMLU: 0.256
      JHumanEval: 0.057
    en_basic:
      En Avg: 0.274
      OpenBookQA: 0.28
      TriviaQA: 0.129
      HellaSwag: 0.425
      SQuAD2: 0.501
      XWINO: 0.807
      MMLU: 0.294
      GSM8K: 0.072
      MATH: 0.034
      BBH: 0.122
      HumanEval: 0.08
    other:
      GPQA: 0.007
- id: pfnet/plamo-2-8b
  name: PLaMo 2 8B
  date: '2025-02-21'
  params: 9.1
  base_model: ''
  sortkey: plamo 2
  url: https://huggingface.co/pfnet/plamo-2-8b
  results:
    ja_basic:
      Ja Avg: 0.481
      JComQA: 0.909
      JEMHopQA: 0.474
      NIILC: 0.655
      JSQuAD: 0.91
      XL-Sum: 0.12
      MGSM: 0.508
      WMT20-en-ja: 0.28
      WMT20-ja-en: 0.205
      JMMLU: 0.536
      JHumanEval: 0.213
    en_basic:
      En Avg: 0.474
      OpenBookQA: 0.346
      TriviaQA: 0.584
      HellaSwag: 0.56
      SQuAD2: 0.511
      XWINO: 0.89
      MMLU: 0.575
      GSM8K: 0.55
      MATH: 0.2
      BBH: 0.26
      HumanEval: 0.26
    other:
      GPQA: 0.022
- id: Qwen/Qwen2-7B
  name: Qwen2-7B
  date: '2024-06-07'
  params: 7.6
  base_model: ''
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-7B
  results:
    ja_basic:
      Ja Avg: 0.472
      JComQA: 0.875
      JEMHopQA: 0.463
      NIILC: 0.372
      JSQuAD: 0.899
      XL-Sum: 0.172
      MGSM: 0.524
      WMT20-en-ja: 0.209
      WMT20-ja-en: 0.195
      JMMLU: 0.587
      JHumanEval: 0.422
    en_basic:
      En Avg: 0.602
      OpenBookQA: 0.374
      TriviaQA: 0.61
      HellaSwag: 0.602
      SQuAD2: 0.574
      XWINO: 0.891
      MMLU: 0.705
      GSM8K: 0.781
      MATH: 0.492
      BBH: 0.53
      HumanEval: 0.46
    other:
      GPQA: 0.246
- id: Qwen/Qwen2-7B-Instruct
  name: Qwen2-7B-Instruct
  date: '2024-06-07'
  params: 7.6
  base_model: Qwen2-7B
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4779
      JComQA: 0.8883
      JEMHopQA: 0.3902
      NIILC: 0.3788
      JSQuAD: 0.8967
      XL-Sum: 0.1265
      MGSM: 0.576
      WMT20-en-ja: 0.2061
      WMT20-ja-en: 0.1902
      JMMLU: 0.571
      JHumanEval: 0.5549
    ja_mtb:
      JMT Avg: 0.646
      coding: 0.512
      extraction: 0.771
      humanities: 0.719
      math: 0.687
      reasoning: 0.514
      roleplay: 0.683
      stem: 0.563
      writing: 0.717
    en_basic:
      En Avg: 0.582
      OpenBookQA: 0.396
      TriviaQA: 0.5466
      HellaSwag: 0.6146
      SQuAD2: 0.5927
      XWINO: 0.886
      MMLU: 0.7071
      GSM8K: 0.6262
      MATH: 0.504
      BBH: 0.3044
      HumanEval: 0.6427
    other:
      GPQA: 0.1049
- id: Qwen/Qwen2-72B
  name: Qwen2-72B
  date: '2024-06-07'
  params: 72
  base_model: ''
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-72B
  results:
    ja_basic:
      Ja Avg: 0.593
      JComQA: 0.96
      JEMHopQA: 0.62
      NIILC: 0.561
      JSQuAD: 0.926
      XL-Sum: 0.238
      MGSM: 0.768
      WMT20-en-ja: 0.275
      WMT20-ja-en: 0.241
      JMMLU: 0.782
      JHumanEval: 0.561
    en_basic:
      En Avg: 0.702
      OpenBookQA: 0.418
      TriviaQA: 0.79
      HellaSwag: 0.677
      SQuAD2: 0.673
      XWINO: 0.915
      MMLU: 0.842
      GSM8K: 0.893
      MATH: 0.56
      BBH: 0.643
      HumanEval: 0.608
    other:
      GPQA: 0.299
- id: Qwen/Qwen2-72B-Instruct
  name: Qwen2-72B-Instruct
  date: '2024-06-07'
  params: 72
  base_model: Qwen2-72B
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-72B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.5978
      JComQA: 0.9634
      JEMHopQA: 0.6284
      NIILC: 0.5566
      JSQuAD: 0.9198
      XL-Sum: 0.1658
      MGSM: 0.78
      WMT20-en-ja: 0.26
      WMT20-ja-en: 0.2317
      JMMLU: 0.7707
      JHumanEval: 0.7012
    ja_mtb:
      JMT Avg: 0.756
      coding: 0.632
      extraction: 0.8
      humanities: 0.842
      math: 0.688
      reasoning: 0.616
      roleplay: 0.824
      stem: 0.797
      writing: 0.846
    en_basic:
      En Avg: 0.6687
      OpenBookQA: 0.444
      TriviaQA: 0.7588
      HellaSwag: 0.6849
      SQuAD2: 0.6849
      XWINO: 0.911
      MMLU: 0.8395
      GSM8K: 0.8484
      MATH: 0.634
      BBH: 0.1931
      HumanEval: 0.6884
    other:
      GPQA: 0.4018
- id: Qwen/Qwen2.5-0.5B
  name: Qwen2.5-0.5B
  date: '2024-09-19'
  params: 0.5
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-0.5B
  results:
    ja_basic:
      Ja Avg: 0.234
      JComQA: 0.369
      JEMHopQA: 0.389
      NIILC: 0.139
      JSQuAD: 0.635
      XL-Sum: 0.101
      MGSM: 0.076
      WMT20-en-ja: 0.058
      WMT20-ja-en: 0.064
      JMMLU: 0.304
      JHumanEval: 0.203
    en_basic:
      En Avg: 0.365
      OpenBookQA: 0.266
      TriviaQA: 0.19
      HellaSwag: 0.399
      SQuAD2: 0.501
      XWINO: 0.768
      MMLU: 0.479
      GSM8K: 0.341
      MATH: 0.148
      BBH: 0.277
      HumanEval: 0.277
    other:
      GPQA: 0.094
- id: Qwen/Qwen2.5-0.5B-Instruct
  name: Qwen2.5-0.5B-Instruct
  date: '2024-09-19'
  params: 0.5
  base_model: Qwen2.5-0.5B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.2432
      JComQA: 0.3816
      JEMHopQA: 0.4007
      NIILC: 0.1572
      JSQuAD: 0.6866
      XL-Sum: 0.1116
      MGSM: 0.08
      WMT20-en-ja: 0.0947
      WMT20-ja-en: 0.0666
      JMMLU: 0.3183
      JHumanEval: 0.1348
    ja_mtb:
      JMT Avg: 0.294
      coding: 0.335
      extraction: 0.284
      humanities: 0.285
      math: 0.317
      reasoning: 0.248
      roleplay: 0.294
      stem: 0.279
      writing: 0.313
    en_basic:
      En Avg: 0.3363
      OpenBookQA: 0.272
      TriviaQA: 0.1838
      HellaSwag: 0.3977
      SQuAD2: 0.5011
      XWINO: 0.7669
      MMLU: 0.4709
      GSM8K: 0.1895
      MATH: 0.236
      BBH: 0.1055
      HumanEval: 0.2396
    other:
      GPQA: 0.0446
- id: Qwen/Qwen2.5-1.5B
  name: Qwen2.5-1.5B
  date: '2024-09-19'
  params: 1.5
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-1.5B
  results:
    ja_basic:
      Ja Avg: 0.372
      JComQA: 0.8
      JEMHopQA: 0.383
      NIILC: 0.241
      JSQuAD: 0.849
      XL-Sum: 0.143
      MGSM: 0.292
      WMT20-en-ja: 0.132
      WMT20-ja-en: 0.134
      JMMLU: 0.438
      JHumanEval: 0.308
    en_basic:
      En Avg: 0.49
      OpenBookQA: 0.342
      TriviaQA: 0.397
      HellaSwag: 0.499
      SQuAD2: 0.506
      XWINO: 0.851
      MMLU: 0.61
      GSM8K: 0.611
      MATH: 0.314
      BBH: 0.413
      HumanEval: 0.356
    other:
      GPQA: 0.045
- id: Qwen/Qwen2.5-1.5B-Instruct
  name: Qwen2.5-1.5B-Instruct
  date: '2024-09-19'
  params: 1.5
  base_model: Qwen2.5-1.5B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.3552
      JComQA: 0.8123
      JEMHopQA: 0.2759
      NIILC: 0.2405
      JSQuAD: 0.8474
      XL-Sum: 0.1276
      MGSM: 0.292
      WMT20-en-ja: 0.1473
      WMT20-ja-en: 0.1192
      JMMLU: 0.4473
      JHumanEval: 0.2421
    ja_mtb:
      JMT Avg: 0.45
      coding: 0.408
      extraction: 0.513
      humanities: 0.456
      math: 0.527
      reasoning: 0.352
      roleplay: 0.473
      stem: 0.406
      writing: 0.469
    en_basic:
      En Avg: 0.4241
      OpenBookQA: 0.334
      TriviaQA: 0.3781
      HellaSwag: 0.5029
      SQuAD2: 0.501
      XWINO: 0.8439
      MMLU: 0.6037
      GSM8K: 0.257
      MATH: 0.272
      BBH: 0.2715
      HumanEval: 0.2768
    other:
      GPQA: 0.1406
- id: Qwen/Qwen2.5-3B
  name: Qwen2.5-3B
  date: '2024-09-19'
  params: 3.1
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-3B
  results:
    ja_basic:
      Ja Avg: 0.442
      JComQA: 0.847
      JEMHopQA: 0.475
      NIILC: 0.306
      JSQuAD: 0.878
      XL-Sum: 0.176
      MGSM: 0.46
      WMT20-en-ja: 0.18
      WMT20-ja-en: 0.167
      JMMLU: 0.529
      JHumanEval: 0.404
    en_basic:
      En Avg: 0.534
      OpenBookQA: 0.36
      TriviaQA: 0.504
      HellaSwag: 0.553
      SQuAD2: 0.541
      XWINO: 0.872
      MMLU: 0.657
      GSM8K: 0.58
      MATH: 0.44
      BBH: 0.442
      HumanEval: 0.387
    other:
      GPQA: 0.0
- id: Qwen/Qwen2.5-3B-Instruct
  name: Qwen2.5-3B-Instruct
  date: '2024-09-19'
  params: 3.1
  base_model: Qwen2.5-3B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4088
      JComQA: 0.8758
      JEMHopQA: 0.3043
      NIILC: 0.2934
      JSQuAD: 0.8664
      XL-Sum: 0.1442
      MGSM: 0.228
      WMT20-en-ja: 0.1984
      WMT20-ja-en: 0.1679
      JMMLU: 0.5363
      JHumanEval: 0.4738
    ja_mtb:
      JMT Avg: 0.593
      coding: 0.567
      extraction: 0.647
      humanities: 0.597
      math: 0.665
      reasoning: 0.457
      roleplay: 0.649
      stem: 0.526
      writing: 0.637
    en_basic:
      En Avg: 0.4717
      OpenBookQA: 0.364
      TriviaQA: 0.4462
      HellaSwag: 0.562
      SQuAD2: 0.5041
      XWINO: 0.8692
      MMLU: 0.6642
      GSM8K: 0.0955
      MATH: 0.612
      BBH: 0.1284
      HumanEval: 0.4713
    other:
      GPQA: 0.3036
- id: Qwen/Qwen2.5-7B
  name: Qwen2.5-7B
  date: '2024-09-19'
  params: 7.6
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-7B
  results:
    ja_basic:
      Ja Avg: 0.512
      JComQA: 0.924
      JEMHopQA: 0.459
      NIILC: 0.426
      JSQuAD: 0.907
      XL-Sum: 0.216
      MGSM: 0.616
      WMT20-en-ja: 0.229
      WMT20-ja-en: 0.199
      JMMLU: 0.634
      JHumanEval: 0.507
    en_basic:
      En Avg: 0.63
      OpenBookQA: 0.392
      TriviaQA: 0.601
      HellaSwag: 0.6
      SQuAD2: 0.618
      XWINO: 0.888
      MMLU: 0.742
      GSM8K: 0.832
      MATH: 0.51
      BBH: 0.562
      HumanEval: 0.554
    other:
      GPQA: 0.295
- id: Qwen/Qwen2.5-7B-Instruct
  name: Qwen2.5-7B-Instruct
  date: '2024-09-19'
  params: 7.6
  base_model: Qwen2.5-7B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.4983
      JComQA: 0.9151
      JEMHopQA: 0.4293
      NIILC: 0.391
      JSQuAD: 0.891
      XL-Sum: 0.1675
      MGSM: 0.632
      WMT20-en-ja: 0.2105
      WMT20-ja-en: 0.1916
      JMMLU: 0.623
      JHumanEval: 0.5323
    ja_mtb:
      JMT Avg: 0.665
      coding: 0.599
      extraction: 0.741
      humanities: 0.719
      math: 0.637
      reasoning: 0.541
      roleplay: 0.744
      stem: 0.624
      writing: 0.713
    en_basic:
      En Avg: 0.6039
      OpenBookQA: 0.428
      TriviaQA: 0.5187
      HellaSwag: 0.6241
      SQuAD2: 0.5694
      XWINO: 0.877
      MMLU: 0.7416
      GSM8K: 0.7392
      MATH: 0.688
      BBH: 0.2166
      HumanEval: 0.636
    other:
      GPQA: 0.3103
- id: Qwen/Qwen2.5-14B-Instruct
  name: Qwen2.5-14B-Instruct
  date: '2024-09-25'
  params: 14
  base_model: Qwen2.5-14B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.5534
      JComQA: 0.9526
      JEMHopQA: 0.588
      NIILC: 0.519
      JSQuAD: 0.9023
      XL-Sum: 0.14
      MGSM: 0.68
      WMT20-en-ja: 0.1925
      WMT20-ja-en: 0.1603
      JMMLU: 0.708
      JHumanEval: 0.6909
    ja_mtb:
      JMT Avg: 0.762
      coding: 0.673
      extraction: 0.829
      humanities: 0.798
      math: 0.828
      reasoning: 0.571
      roleplay: 0.815
      stem: 0.743
      writing: 0.841
    en_basic:
      En Avg: 0.6144
      OpenBookQA: 0.438
      TriviaQA: 0.5919
      HellaSwag: 0.6558
      SQuAD2: 0.6797
      XWINO: 0.8903
      MMLU: 0.8005
      GSM8K: 0.7612
      MATH: 0.666
      BBH: 0.0287
      HumanEval: 0.6323
    other:
      GPQA: 0.3795
- id: Qwen/Qwen2.5-32B-Instruct
  name: Qwen2.5-32B-Instruct
  date: '2024-09-25'
  params: 32
  base_model: Qwen2.5-32B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.5706
      JComQA: 0.9589
      JEMHopQA: 0.5672
      NIILC: 0.4966
      JSQuAD: 0.9033
      XL-Sum: 0.1688
      MGSM: 0.78
      WMT20-en-ja: 0.2279
      WMT20-ja-en: 0.1953
      JMMLU: 0.7574
      JHumanEval: 0.6506
    ja_mtb:
      JMT Avg: 0.809
      coding: 0.724
      extraction: 0.885
      humanities: 0.816
      math: 0.918
      reasoning: 0.726
      roleplay: 0.834
      stem: 0.763
      writing: 0.808
    en_basic:
      En Avg: 0.5881
      OpenBookQA: 0.424
      TriviaQA: 0.5341
      HellaSwag: 0.6709
      SQuAD2: 0.5362
      XWINO: 0.8933
      MMLU: 0.8336
      GSM8K: 0.5807
      MATH: 0.802
      BBH: 0.0172
      HumanEval: 0.589
    other:
      GPQA: 0.4018
- id: Qwen/Qwen2.5-72B
  name: Qwen2.5-72B
  date: '2024-09-19'
  params: 72
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-72B
  results:
    ja_basic:
      Ja Avg: 0.623
      JComQA: 0.972
      JEMHopQA: 0.611
      NIILC: 0.619
      JSQuAD: 0.93
      XL-Sum: 0.279
      MGSM: 0.828
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.252
      JMMLU: 0.804
      JHumanEval: 0.648
    en_basic:
      En Avg: 0.709
      OpenBookQA: 0.416
      TriviaQA: 0.76
      HellaSwag: 0.685
      SQuAD2: 0.693
      XWINO: 0.901
      MMLU: 0.861
      GSM8K: 0.87
      MATH: 0.626
      BBH: 0.727
      HumanEval: 0.554
    other:
      GPQA: 0.248
- id: Qwen/Qwen2.5-72B-Instruct
  name: Qwen2.5-72B-Instruct
  date: '2024-09-19'
  params: 72
  base_model: Qwen2.5-72B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.5738
      JComQA: 0.9696
      JEMHopQA: 0.5693
      NIILC: 0.5823
      JSQuAD: 0.7382
      XL-Sum: 0.17
      MGSM: 0.84
      WMT20-en-ja: 0.2266
      WMT20-ja-en: 0.2184
      JMMLU: 0.7893
      JHumanEval: 0.6341
    ja_mtb:
      JMT Avg: 0.835
      coding: 0.795
      extraction: 0.86
      humanities: 0.865
      math: 0.857
      reasoning: 0.784
      roleplay: 0.863
      stem: 0.804
      writing: 0.854
    en_basic:
      En Avg: 0.6913
      OpenBookQA: 0.454
      TriviaQA: 0.676
      HellaSwag: 0.7061
      SQuAD2: 0.6767
      XWINO: 0.8895
      MMLU: 0.8475
      GSM8K: 0.9045
      MATH: 0.77
      BBH: 0.3749
      HumanEval: 0.614
    other:
      GPQA: 0.4643
- id: sbintuitions/sarashina2-7b
  name: Sarashina2-7B
  date: '2024-06-14'
  params: 7.3
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-7b
  results:
    ja_basic:
      Ja Avg: 0.395
      JComQA: 0.742
      JEMHopQA: 0.509
      NIILC: 0.634
      JSQuAD: 0.868
      XL-Sum: 0.141
      MGSM: 0.08
      WMT20-en-ja: 0.273
      WMT20-ja-en: 0.201
      JMMLU: 0.384
      JHumanEval: 0.121
    en_basic:
      En Avg: 0.383
      OpenBookQA: 0.346
      TriviaQA: 0.479
      HellaSwag: 0.532
      SQuAD2: 0.501
      XWINO: 0.892
      MMLU: 0.425
      GSM8K: 0.101
      MATH: 0.034
      BBH: 0.373
      HumanEval: 0.146
    other:
      GPQA: 0.078
- id: sbintuitions/sarashina2-13b
  name: Sarashina2-13B
  date: '2024-06-14'
  params: 13
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-13b
  results:
    ja_basic:
      Ja Avg: 0.445
      JComQA: 0.85
      JEMHopQA: 0.557
      NIILC: 0.661
      JSQuAD: 0.898
      XL-Sum: 0.158
      MGSM: 0.188
      WMT20-en-ja: 0.284
      WMT20-ja-en: 0.221
      JMMLU: 0.473
      JHumanEval: 0.161
    en_basic:
      En Avg: 0.418
      OpenBookQA: 0.34
      TriviaQA: 0.548
      HellaSwag: 0.562
      SQuAD2: 0.501
      XWINO: 0.896
      MMLU: 0.496
      GSM8K: 0.158
      MATH: 0.036
      BBH: 0.442
      HumanEval: 0.198
    other:
      GPQA: 0.02
- id: sbintuitions/sarashina2-70b
  name: Sarashina2-70B
  date: '2024-06-14'
  params: 70
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-70b
  results:
    ja_basic:
      Ja Avg: 0.53
      JComQA: 0.929
      JEMHopQA: 0.717
      NIILC: 0.668
      JSQuAD: 0.929
      XL-Sum: 0.19
      MGSM: 0.488
      WMT20-en-ja: 0.313
      WMT20-ja-en: 0.243
      JMMLU: 0.592
      JHumanEval: 0.235
    en_basic:
      En Avg: 0.491
      OpenBookQA: 0.388
      TriviaQA: 0.537
      HellaSwag: 0.628
      SQuAD2: 0.675
      XWINO: 0.917
      MMLU: 0.63
      GSM8K: 0.011
      MATH: 0.206
      BBH: 0.639
      HumanEval: 0.281
    other:
      GPQA: 0.163
- id: stockmark/stockmark-100b
  name: Stockmark-100b
  date: '2024-05-16'
  params: 100
  base_model: ''
  sortkey: stockmark
  url: https://huggingface.co/stockmark/stockmark-100b
  results:
    ja_basic:
      Ja Avg: 0.238
      JComQA: 0.205
      JEMHopQA: 0.408
      NIILC: 0.557
      JSQuAD: 0.558
      XL-Sum: 0.062
      MGSM: 0.008
      WMT20-en-ja: 0.203
      WMT20-ja-en: 0.118
      JMMLU: 0.235
      JHumanEval: 0.032
    en_basic:
      En Avg: 0.302
      OpenBookQA: 0.278
      TriviaQA: 0.366
      HellaSwag: 0.458
      SQuAD2: 0.501
      XWINO: 0.82
      MMLU: 0.258
      GSM8K: 0.017
      MATH: 0.014
      BBH: 0.259
      HumanEval: 0.046
    other:
      GPQA: 0.0
- id: tokyotech-llm/Swallow-7b-hf
  name: Swallow 7B
  date: '2023-12-19'
  params: 6.7
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-7b-hf
  results:
    ja_basic:
      Ja Avg: 0.346
      JComQA: 0.483
      JEMHopQA: 0.511
      NIILC: 0.585
      JSQuAD: 0.847
      XL-Sum: 0.182
      MGSM: 0.108
      WMT20-en-ja: 0.25
      WMT20-ja-en: 0.149
      JMMLU: 0.324
      JHumanEval: 0.018
    en_basic:
      En Avg: 0.363
      OpenBookQA: 0.312
      TriviaQA: 0.491
      HellaSwag: 0.527
      SQuAD2: 0.501
      XWINO: 0.885
      MMLU: 0.391
      GSM8K: 0.103
      MATH: 0.02
      BBH: 0.354
      HumanEval: 0.041
    other:
      GPQA: 0.013
- id: tokyotech-llm/Swallow-13b-hf
  name: Swallow 13B
  date: '2023-12-19'
  params: 13
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-13b-hf
  results:
    ja_basic:
      Ja Avg: 0.415
      JComQA: 0.764
      JEMHopQA: 0.507
      NIILC: 0.643
      JSQuAD: 0.893
      XL-Sum: 0.215
      MGSM: 0.208
      WMT20-en-ja: 0.272
      WMT20-ja-en: 0.178
      JMMLU: 0.439
      JHumanEval: 0.027
    en_basic:
      En Avg: 0.412
      OpenBookQA: 0.344
      TriviaQA: 0.58
      HellaSwag: 0.56
      SQuAD2: 0.502
      XWINO: 0.902
      MMLU: 0.501
      GSM8K: 0.197
      MATH: 0.024
      BBH: 0.43
      HumanEval: 0.08
    other:
      GPQA: 0.116
- id: tokyotech-llm/Swallow-70b-hf
  name: Swallow 70B
  date: '2023-12-19'
  params: 70
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-70b-hf
  results:
    ja_basic:
      Ja Avg: 0.519
      JComQA: 0.92
      JEMHopQA: 0.626
      NIILC: 0.689
      JSQuAD: 0.92
      XL-Sum: 0.225
      MGSM: 0.48
      WMT20-en-ja: 0.304
      WMT20-ja-en: 0.231
      JMMLU: 0.579
      JHumanEval: 0.22
    en_basic:
      En Avg: 0.543
      OpenBookQA: 0.416
      TriviaQA: 0.761
      HellaSwag: 0.643
      SQuAD2: 0.522
      XWINO: 0.92
      MMLU: 0.659
      GSM8K: 0.503
      MATH: 0.108
      BBH: 0.655
      HumanEval: 0.24
    other:
      GPQA: 0.098
- id: tokyotech-llm/Swallow-MS-7b-v0.1
  name: Swallow-MS 7B v0.1
  date: '2024-03-11'
  params: 7.2
  base_model: ''
  sortkey: swallow ms v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MS-7b-v0.1
  results:
    ja_basic:
      Ja Avg: 0.439
      JComQA: 0.873
      JEMHopQA: 0.517
      NIILC: 0.572
      JSQuAD: 0.879
      XL-Sum: 0.197
      MGSM: 0.244
      WMT20-en-ja: 0.251
      WMT20-ja-en: 0.167
      JMMLU: 0.459
      JHumanEval: 0.232
    en_basic:
      En Avg: 0.461
      OpenBookQA: 0.352
      TriviaQA: 0.599
      HellaSwag: 0.579
      SQuAD2: 0.501
      XWINO: 0.901
      MMLU: 0.548
      GSM8K: 0.268
      MATH: 0.096
      BBH: 0.491
      HumanEval: 0.27
    other:
      GPQA: 0.08
- id: tokyotech-llm/Swallow-MS-7b-instruct-v0.1
  name: Swallow-MS-7b-instruct-v0.1
  date: '2024-03-11'
  params: 7.2
  base_model: Swallow-MS 7B v0.1
  sortkey: swallow ms v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MS-7b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.3936
      JComQA: 0.7578
      JEMHopQA: 0.4902
      NIILC: 0.4464
      JSQuAD: 0.8635
      XL-Sum: 0.1578
      MGSM: 0.172
      WMT20-en-ja: 0.2272
      WMT20-ja-en: 0.1872
      JMMLU: 0.4194
      JHumanEval: 0.2146
    ja_mtb:
      JMT Avg: 0.4
      coding: 0.358
      extraction: 0.421
      humanities: 0.501
      math: 0.222
      reasoning: 0.349
      roleplay: 0.458
      stem: 0.444
      writing: 0.449
    en_basic:
      En Avg: 0.4362
      OpenBookQA: 0.36
      TriviaQA: 0.5003
      HellaSwag: 0.5865
      SQuAD2: 0.51
      XWINO: 0.886
      MMLU: 0.5259
      GSM8K: 0.2146
      MATH: 0.082
      BBH: 0.4411
      HumanEval: 0.2555
    other:
      GPQA: 0.2009
- id: tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1
  name: Swallow-MX 8x7B v0.1
  date: '2024-03-11'
  params: 47
  base_model: ''
  sortkey: swallow mx v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1
  results:
    ja_basic:
      Ja Avg: 0.506
      JComQA: 0.922
      JEMHopQA: 0.533
      NIILC: 0.577
      JSQuAD: 0.917
      XL-Sum: 0.263
      MGSM: 0.444
      WMT20-en-ja: 0.272
      WMT20-ja-en: 0.209
      JMMLU: 0.565
      JHumanEval: 0.358
    en_basic:
      En Avg: 0.589
      OpenBookQA: 0.348
      TriviaQA: 0.773
      HellaSwag: 0.651
      SQuAD2: 0.538
      XWINO: 0.919
      MMLU: 0.692
      GSM8K: 0.574
      MATH: 0.298
      BBH: 0.686
      HumanEval: 0.41
    other:
      GPQA: 0.103
- id: tokyotech-llm/Swallow-7b-instruct-v0.1
  name: Swallow-7b-instruct-v0.1
  date: '2023-12-19'
  params: 6.7
  base_model: Swallow 7B
  sortkey: swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.3527
      JComQA: 0.5987
      JEMHopQA: 0.4913
      NIILC: 0.5314
      JSQuAD: 0.8374
      XL-Sum: 0.1532
      MGSM: 0.128
      WMT20-en-ja: 0.2283
      WMT20-ja-en: 0.1789
      JMMLU: 0.3522
      JHumanEval: 0.0274
    ja_mtb:
      JMT Avg: 0.419
      coding: 0.324
      extraction: 0.401
      humanities: 0.519
      math: 0.275
      reasoning: 0.344
      roleplay: 0.535
      stem: 0.494
      writing: 0.462
    en_basic:
      En Avg: 0.3759
      OpenBookQA: 0.33
      TriviaQA: 0.4806
      HellaSwag: 0.5502
      SQuAD2: 0.5009
      XWINO: 0.8796
      MMLU: 0.4074
      GSM8K: 0.1236
      MATH: 0.034
      BBH: 0.3594
      HumanEval: 0.0939
    other:
      GPQA: 0.1183
- id: tokyotech-llm/Swallow-70b-instruct-v0.1
  name: Swallow-70b-instruct-v0.1
  date: '2023-12-19'
  params: 70
  base_model: Swallow 70B
  sortkey: swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.4923
      JComQA: 0.9231
      JEMHopQA: 0.566
      NIILC: 0.5654
      JSQuAD: 0.9034
      XL-Sum: 0.1865
      MGSM: 0.42
      WMT20-en-ja: 0.2629
      WMT20-ja-en: 0.232
      JMMLU: 0.571
      JHumanEval: 0.2927
    ja_mtb:
      JMT Avg: 0.509
      coding: 0.381
      extraction: 0.604
      humanities: 0.568
      math: 0.464
      reasoning: 0.402
      roleplay: 0.583
      stem: 0.557
      writing: 0.51
    en_basic:
      En Avg: 0.5561
      OpenBookQA: 0.446
      TriviaQA: 0.7422
      HellaSwag: 0.6557
      SQuAD2: 0.5705
      XWINO: 0.9166
      MMLU: 0.6679
      GSM8K: 0.5095
      MATH: 0.108
      BBH: 0.6643
      HumanEval: 0.2805
    other:
      GPQA: 0.2679
- id: weblab-GENIAC/Tanuki-8B-dpo-v1.0
  name: Tanuki-8B-dpo-v1.0
  date: '2024-08-30'
  params: 7.5
  base_model: (private)
  sortkey: tanuki dpo v1.0
  url: https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0
  results:
    ja_basic:
      Ja Avg: 0.311
      JComQA: 0.2779
      JEMHopQA: 0.2843
      NIILC: 0.3699
      JSQuAD: 0.6702
      XL-Sum: 0.1018
      MGSM: 0.428
      WMT20-en-ja: 0.2385
      WMT20-ja-en: 0.1829
      JMMLU: 0.3056
      JHumanEval: 0.2506
    ja_mtb:
      JMT Avg: 0.529
      coding: 0.461
      extraction: 0.597
      humanities: 0.562
      math: 0.495
      reasoning: 0.377
      roleplay: 0.589
      stem: 0.509
      writing: 0.643
    en_basic:
      En Avg: 0.4065
      OpenBookQA: 0.334
      TriviaQA: 0.2835
      HellaSwag: 0.4685
      SQuAD2: 0.5009
      XWINO: 0.8159
      MMLU: 0.3768
      GSM8K: 0.4867
      MATH: 0.178
      BBH: 0.3325
      HumanEval: 0.2878
    other:
      GPQA: 0.1362
- id: weblab-GENIAC/Tanuki-8x8B-dpo-v1.0
  name: Tanuki-8x8B-dpo-v1.0
  date: '2024-08-30'
  params: 47
  base_model: (private)
  sortkey: tanuki dpo v1.0
  url: https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0
  results:
    ja_basic:
      Ja Avg: 0.4537
      JComQA: 0.7078
      JEMHopQA: 0.5509
      NIILC: 0.612
      JSQuAD: 0.8673
      XL-Sum: 0.1421
      MGSM: 0.456
      WMT20-en-ja: 0.2692
      WMT20-ja-en: 0.2082
      JMMLU: 0.4394
      JHumanEval: 0.2841
    ja_mtb:
      JMT Avg: 0.546
      coding: 0.513
      extraction: 0.489
      humanities: 0.624
      math: 0.557
      reasoning: 0.445
      roleplay: 0.604
      stem: 0.547
      writing: 0.594
    en_basic:
      En Avg: 0.4641
      OpenBookQA: 0.348
      TriviaQA: 0.4811
      HellaSwag: 0.5548
      SQuAD2: 0.5214
      XWINO: 0.8499
      MMLU: 0.4934
      GSM8K: 0.5444
      MATH: 0.236
      BBH: 0.4191
      HumanEval: 0.1927
    other:
      GPQA: 0.1585
- id: SakanaAI/TinySwallow-1.5B
  name: TinySwallow-1.5B
  date: '2025-01-30'
  params: 1.5
  base_model: ''
  sortkey: tinyswallow
  url: https://huggingface.co/SakanaAI/TinySwallow-1.5B
  results:
    ja_basic:
      Ja Avg: 0.402
      JComQA: 0.84
      JEMHopQA: 0.437
      NIILC: 0.474
      JSQuAD: 0.839
      XL-Sum: 0.173
      MGSM: 0.256
      WMT20-en-ja: 0.201
      WMT20-ja-en: 0.125
      JMMLU: 0.446
      JHumanEval: 0.231
    en_basic:
      En Avg: 0.413
      OpenBookQA: 0.308
      TriviaQA: 0.332
      HellaSwag: 0.468
      SQuAD2: 0.501
      XWINO: 0.85
      MMLU: 0.546
      GSM8K: 0.379
      MATH: 0.162
      BBH: 0.328
      HumanEval: 0.254
    other:
      GPQA: 0.011
- id: SakanaAI/TinySwallow-1.5B-Instruct
  name: TinySwallow-1.5B-Instruct
  date: '2025-01-30'
  params: 1.5
  base_model: TinySwallow-1.5B
  sortkey: tinyswallow
  url: https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.3975
      JComQA: 0.8025
      JEMHopQA: 0.3448
      NIILC: 0.4471
      JSQuAD: 0.8561
      XL-Sum: 0.1593
      MGSM: 0.308
      WMT20-en-ja: 0.2035
      WMT20-ja-en: 0.143
      JMMLU: 0.4606
      JHumanEval: 0.2506
    ja_mtb:
      JMT Avg: 0.565
      coding: 0.434
      extraction: 0.572
      humanities: 0.772
      math: 0.453
      reasoning: 0.392
      roleplay: 0.645
      stem: 0.61
      writing: 0.643
    en_basic:
      En Avg: 0.4114
      OpenBookQA: 0.31
      TriviaQA: 0.309
      HellaSwag: 0.4867
      SQuAD2: 0.5007
      XWINO: 0.843
      MMLU: 0.5598
      GSM8K: 0.398
      MATH: 0.162
      BBH: 0.2505
      HumanEval: 0.2939
    other:
      GPQA: 0.0134
- id: 01-ai/Yi-1.5-6B
  name: Yi-1.5 6B
  date: '2024-05-13'
  params: 6.1
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-6B
  results:
    ja_basic:
      Ja Avg: 0.354
      JComQA: 0.658
      JEMHopQA: 0.38
      NIILC: 0.226
      JSQuAD: 0.829
      XL-Sum: 0.198
      MGSM: 0.24
      WMT20-en-ja: 0.13
      WMT20-ja-en: 0.147
      JMMLU: 0.423
      JHumanEval: 0.313
    en_basic:
      En Avg: 0.54
      OpenBookQA: 0.344
      TriviaQA: 0.593
      HellaSwag: 0.575
      SQuAD2: 0.651
      XWINO: 0.898
      MMLU: 0.636
      GSM8K: 0.522
      MATH: 0.244
      BBH: 0.583
      HumanEval: 0.352
    other:
      GPQA: 0.266
- id: 01-ai/Yi-1.5-9B
  name: Yi-1.5 9B
  date: '2024-05-13'
  params: 8.8
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-9B
  results:
    ja_basic:
      Ja Avg: 0.432
      JComQA: 0.834
      JEMHopQA: 0.417
      NIILC: 0.265
      JSQuAD: 0.894
      XL-Sum: 0.224
      MGSM: 0.42
      WMT20-en-ja: 0.174
      WMT20-ja-en: 0.187
      JMMLU: 0.516
      JHumanEval: 0.391
    en_basic:
      En Avg: 0.592
      OpenBookQA: 0.39
      TriviaQA: 0.619
      HellaSwag: 0.601
      SQuAD2: 0.693
      XWINO: 0.902
      MMLU: 0.696
      GSM8K: 0.62
      MATH: 0.3
      BBH: 0.71
      HumanEval: 0.384
    other:
      GPQA: 0.275
- id: 01-ai/Yi-1.5-34B
  name: Yi-1.5 34B
  date: '2024-05-13'
  params: 34
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-34B
  results:
    ja_basic:
      Ja Avg: 0.468
      JComQA: 0.869
      JEMHopQA: 0.461
      NIILC: 0.332
      JSQuAD: 0.899
      XL-Sum: 0.238
      MGSM: 0.52
      WMT20-en-ja: 0.219
      WMT20-ja-en: 0.208
      JMMLU: 0.591
      JHumanEval: 0.346
    en_basic:
      En Avg: 0.65
      OpenBookQA: 0.402
      TriviaQA: 0.708
      HellaSwag: 0.662
      SQuAD2: 0.754
      XWINO: 0.91
      MMLU: 0.774
      GSM8K: 0.743
      MATH: 0.394
      BBH: 0.763
      HumanEval: 0.385
    other:
      GPQA: 0.342

