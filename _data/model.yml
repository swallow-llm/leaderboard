# Evaluation results in Swallow LLM Leaderboard
# Copyright (c) 2025 Swallow LLM team
# This file is distributed under Creative Commons Attribution 4.0 (CC-BY 4.0) License

- id: abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0
  model_id: abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0
  name: ABEJA-QwQ32b-Reasoning-Japanese-v1.0
  date: '2025-04-25'
  params: 33
  active_params: 33
  family: Qwen2.5
  pre_training: ABEJA-Qwen2.5-32b-Japanese-v0.1
  reasoning: 'on'
  sortkey: abeja qwq reasoning japanese v1.0
  is_post: true
  url: https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0
  results:
    ja_post:
      avg: 0.73
      JEMHopQA: 0.644
      MMLU-ProX: 0.712
      GPQA: 0.527
      MATH100: 0.899
      JHumanEval: 0.866
      __MIFEvalJa: 0.619
    en_post:
      avg: 0.739
      HellaSwag: 0.906
      MMLU-Pro: 0.78
      GPQA: 0.606
      MATH500: 0.964
      AIME: 0.617
      LCB: 0.563
    ja_mtb:
      avg: 0.843
      coding: 0.868
      extraction: 0.893
      humanities: 0.885
      math: 0.889
      reasoning: 0.694
      roleplay: 0.848
      stem: 0.85
      writing: 0.821
    en_mtb:
      avg: 0.866
      coding: 0.808
      extraction: 0.878
      humanities: 0.899
      math: 0.951
      reasoning: 0.757
      roleplay: 0.872
      stem: 0.882
      writing: 0.881
- id: cyberagent/calm3-22b-chat
  model_id: cyberagent/calm3-22b-chat
  name: CyberAgentLM3-22B-chat
  date: '2024-07-09'
  params: 22
  active_params: 22
  family: CyberAgentLM3
  pre_training: (private)
  reasoning: N/A
  sortkey: cyberagentlm3
  is_post: true
  url: https://huggingface.co/cyberagent/calm3-22b-chat
  results:
    ja_post:
      avg: 0.397
      JEMHopQA: 0.612
      MMLU-ProX: 0.31
      GPQA: 0.266
      MATH100: 0.354
      JHumanEval: 0.443
      __MIFEvalJa: 0.429
    en_post:
      avg: 0.28
      HellaSwag: 0.77
      MMLU-Pro: 0.26
      GPQA: 0.288
      MATH500: 0.298
      AIME: 0.017
      LCB: 0.045
    ja_mtb:
      avg: 0.697
      coding: 0.5
      extraction: 0.733
      humanities: 0.859
      math: 0.591
      reasoning: 0.611
      roleplay: 0.791
      stem: 0.721
      writing: 0.769
    en_mtb:
      avg: 0.621
      coding: 0.467
      extraction: 0.695
      humanities: 0.828
      math: 0.479
      reasoning: 0.429
      roleplay: 0.678
      stem: 0.647
      writing: 0.747
- id: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  name: DeepSeek-R1-Distill-Llama-8B
  date: '2025-01-20'
  params: 8.0
  active_params: 8.0
  family: Llama 3
  pre_training: Llama 3.1 8B
  reasoning: 'on'
  sortkey: deepseek r1 distill llama
  is_post: true
  url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  results:
    ja_post:
      avg: 0.451
      JEMHopQA: 0.348
      MMLU-ProX: 0.319
      GPQA: 0.31
      MATH100: 0.556
      JHumanEval: 0.721
      __MIFEvalJa: 0.319
    en_post:
      avg: 0.549
      HellaSwag: 0.688
      MMLU-Pro: 0.549
      GPQA: 0.46
      MATH500: 0.866
      AIME: 0.367
      LCB: 0.364
    ja_mtb:
      avg: 0.526
      coding: 0.376
      extraction: 0.625
      humanities: 0.681
      math: 0.595
      reasoning: 0.496
      roleplay: 0.483
      stem: 0.51
      writing: 0.442
    en_mtb:
      avg: 0.704
      coding: 0.398
      extraction: 0.745
      humanities: 0.825
      math: 0.827
      reasoning: 0.562
      roleplay: 0.768
      stem: 0.731
      writing: 0.775
- id: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  name: DeepSeek-R1-Distill-Llama-70B
  date: '2025-01-20'
  params: 70
  active_params: 70
  family: Llama 3
  pre_training: Llama 3.3 70B Instruct
  reasoning: N/A
  sortkey: deepseek r1 distill llama
  is_post: true
  url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  results:
    ja_post:
      avg: 0.683
      JEMHopQA: 0.567
      MMLU-ProX: 0.642
      GPQA: 0.538
      MATH100: 0.859
      JHumanEval: 0.812
      __MIFEvalJa: 0.558
    en_post:
      avg: 0.73
      HellaSwag: 0.891
      MMLU-Pro: 0.776
      GPQA: 0.626
      MATH500: 0.936
      AIME: 0.617
      LCB: 0.534
    ja_mtb:
      avg: 0.707
      coding: 0.551
      extraction: 0.778
      humanities: 0.838
      math: 0.78
      reasoning: 0.525
      roleplay: 0.768
      stem: 0.733
      writing: 0.681
    en_mtb:
      avg: 0.842
      coding: 0.787
      extraction: 0.931
      humanities: 0.862
      math: 0.919
      reasoning: 0.723
      roleplay: 0.85
      stem: 0.806
      writing: 0.854
- id: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
  model_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
  name: DeepSeek-R1-Distill-Qwen-7B
  date: '2025-01-20'
  params: 7.6
  active_params: 7.6
  family: Qwen2
  pre_training: Qwen2.5-Math-7B
  reasoning: 'on'
  sortkey: deepseek r1 distill qwen
  is_post: true
  url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
  results:
    ja_post:
      avg: 0.514
      JEMHopQA: 0.279
      MMLU-ProX: 0.438
      GPQA: 0.4
      MATH100: 0.778
      JHumanEval: 0.674
      __MIFEvalJa: 0.341
    en_post:
      avg: 0.546
      HellaSwag: 0.564
      MMLU-Pro: 0.547
      GPQA: 0.495
      MATH500: 0.902
      AIME: 0.417
      LCB: 0.351
    ja_mtb:
      avg: 0.411
      coding: 0.371
      extraction: 0.572
      humanities: 0.347
      math: 0.804
      reasoning: 0.346
      roleplay: 0.275
      stem: 0.341
      writing: 0.228
    en_mtb:
      avg: 0.649
      coding: 0.481
      extraction: 0.656
      humanities: 0.708
      math: 0.762
      reasoning: 0.52
      roleplay: 0.686
      stem: 0.7
      writing: 0.677
- id: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
  model_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
  name: DeepSeek-R1-Distill-Qwen-14B
  date: '2025-01-20'
  params: 15
  active_params: 15
  family: Qwen2.5
  pre_training: Qwen2.5-14B
  reasoning: 'on'
  sortkey: deepseek r1 distill qwen
  is_post: true
  url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
  results:
    ja_post:
      avg: 0.638
      JEMHopQA: 0.508
      MMLU-ProX: 0.591
      GPQA: 0.496
      MATH100: 0.737
      JHumanEval: 0.859
      __MIFEvalJa: 0.496
    en_post:
      avg: 0.672
      HellaSwag: 0.841
      MMLU-Pro: 0.707
      GPQA: 0.525
      MATH500: 0.908
      AIME: 0.567
      LCB: 0.486
    ja_mtb:
      avg: 0.7
      coding: 0.632
      extraction: 0.803
      humanities: 0.739
      math: 0.857
      reasoning: 0.563
      roleplay: 0.72
      stem: 0.631
      writing: 0.658
    en_mtb:
      avg: 0.775
      coding: 0.512
      extraction: 0.851
      humanities: 0.815
      math: 0.886
      reasoning: 0.745
      roleplay: 0.841
      stem: 0.75
      writing: 0.803
- id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  model_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  name: DeepSeek-R1-Distill-Qwen-32B
  date: '2025-01-20'
  params: 33
  active_params: 33
  family: Qwen2.5
  pre_training: Qwen2.5-32B
  reasoning: 'on'
  sortkey: deepseek r1 distill qwen
  is_post: true
  url: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  results:
    ja_post:
      avg: 0.692
      JEMHopQA: 0.572
      MMLU-ProX: 0.66
      GPQA: 0.536
      MATH100: 0.838
      JHumanEval: 0.855
      __MIFEvalJa: 0.509
    en_post:
      avg: 0.701
      HellaSwag: 0.885
      MMLU-Pro: 0.737
      GPQA: 0.571
      MATH500: 0.926
      AIME: 0.567
      LCB: 0.523
    ja_mtb:
      avg: 0.753
      coding: 0.669
      extraction: 0.874
      humanities: 0.764
      math: 0.867
      reasoning: 0.606
      roleplay: 0.79
      stem: 0.738
      writing: 0.716
    en_mtb:
      avg: 0.822
      coding: 0.619
      extraction: 0.901
      humanities: 0.869
      math: 0.918
      reasoning: 0.793
      roleplay: 0.861
      stem: 0.768
      writing: 0.85
- id: cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese
  model_id: cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese
  name: DeepSeek-R1-Distill-Qwen-14B-Japanese
  date: '2025-01-27'
  params: 15
  active_params: 15
  family: Qwen2.5
  pre_training: DeepSeek-R1-Distill-Qwen-14B
  reasoning: 'on'
  sortkey: deepseek r1 distill qwen japanese
  is_post: true
  url: https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese
  results:
    ja_post:
      avg: 0.575
      JEMHopQA: 0.545
      MMLU-ProX: 0.525
      GPQA: 0.4
      MATH100: 0.788
      JHumanEval: 0.62
      __MIFEvalJa: 0.513
    en_post:
      avg: 0.629
      HellaSwag: 0.823
      MMLU-Pro: 0.679
      GPQA: 0.47
      MATH500: 0.916
      AIME: 0.433
      LCB: 0.451
    ja_mtb:
      avg: 0.771
      coding: 0.557
      extraction: 0.777
      humanities: 0.88
      math: 0.871
      reasoning: 0.664
      roleplay: 0.801
      stem: 0.859
      writing: 0.758
    en_mtb:
      avg: 0.835
      coding: 0.724
      extraction: 0.884
      humanities: 0.87
      math: 0.907
      reasoning: 0.771
      roleplay: 0.867
      stem: 0.817
      writing: 0.838
- id: cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese
  model_id: cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese
  name: DeepSeek-R1-Distill-Qwen-32B-Japanese
  date: '2025-01-27'
  params: 33
  active_params: 33
  family: Qwen2.5
  pre_training: DeepSeek-R1-Distill-Qwen-32B
  reasoning: 'on'
  sortkey: deepseek r1 distill qwen japanese
  is_post: true
  url: https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese
  results:
    ja_post:
      avg: 0.649
      JEMHopQA: 0.654
      MMLU-ProX: 0.606
      GPQA: 0.464
      MATH100: 0.838
      JHumanEval: 0.68
      __MIFEvalJa: 0.544
    en_post:
      avg: 0.697
      HellaSwag: 0.872
      MMLU-Pro: 0.737
      GPQA: 0.576
      MATH500: 0.94
      AIME: 0.55
      LCB: 0.508
    ja_mtb:
      avg: 0.808
      coding: 0.639
      extraction: 0.813
      humanities: 0.917
      math: 0.924
      reasoning: 0.652
      roleplay: 0.842
      stem: 0.872
      writing: 0.802
    en_mtb:
      avg: 0.857
      coding: 0.73
      extraction: 0.893
      humanities: 0.894
      math: 0.964
      reasoning: 0.77
      roleplay: 0.871
      stem: 0.872
      writing: 0.861
- id: elyza/ELYZA-Thinking-1.0-Qwen-32B
  model_id: elyza/ELYZA-Thinking-1.0-Qwen-32B
  name: ELYZA-Thinking-1.0-Qwen-32B
  date: '2025-05-01'
  params: 33
  active_params: 33
  family: Qwen2.5
  pre_training: Qwen2.5-32B-Instruct
  reasoning: 'on'
  sortkey: elyza thinking 1.0 qwen
  is_post: true
  url: https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B
  results:
    ja_post:
      avg: 0.526
      JEMHopQA: 0.601
      MMLU-ProX: 0.623
      GPQA: 0.455
      MATH100: 0.788
      JHumanEval: 0.162
      __MIFEvalJa: 0.566
    en_post:
      avg: 0.571
      HellaSwag: 0.888
      MMLU-Pro: 0.708
      GPQA: 0.576
      MATH500: 0.86
      AIME: 0.3
      LCB: 0.093
    ja_mtb:
      avg: 0.694
      coding: 0.687
      extraction: 0.824
      humanities: 0.688
      math: 0.927
      reasoning: 0.641
      roleplay: 0.583
      stem: 0.656
      writing: 0.542
    en_mtb:
      avg: 0.748
      coding: 0.77
      extraction: 0.913
      humanities: 0.754
      math: 0.912
      reasoning: 0.775
      roleplay: 0.617
      stem: 0.639
      writing: 0.606
- id: tiiuae/Falcon3-1B-Base
  model_id: tiiuae/Falcon3-1B-Base
  name: Falcon3-1B-Base
  date: '2024-12-19'
  params: 1.7
  active_params: 1.7
  family: Falcon3
  base: Falcon3-3B-Base
  sortkey: falcon3
  is_post: false
  url: https://huggingface.co/tiiuae/Falcon3-1B-Base
  results:
    ja_pre:
      avg: 0.129
      JComQA: 0.216
      JEMHopQA: 0.251
      NIILC: 0.062
      JSQuAD: 0.281
      XLSum: 0.085
      MGSM: 0.008
      WMT20enja: 0.012
      WMT20jaen: 0.02
      JMMLU: 0.264
      JHumanEval: 0.088
    en_pre:
      avg: 0.376
      OpenBookQA: 0.316
      TriviaQA: 0.296
      HellaSwag: 0.458
      SQuAD2: 0.501
      XWINO: 0.816
      MMLU: 0.449
      GSM8K: 0.337
      MATH: 0.14
      BBH: 0.323
      HumanEval: 0.125
- id: tiiuae/Falcon3-3B-Base
  model_id: tiiuae/Falcon3-3B-Base
  name: Falcon3-3B-Base
  date: '2024-12-19'
  params: 3.2
  active_params: 3.2
  family: Falcon3
  base: Falcon3-7B-Base
  sortkey: falcon3
  is_post: false
  url: https://huggingface.co/tiiuae/Falcon3-3B-Base
  results:
    ja_pre:
      avg: 0.209
      JComQA: 0.281
      JEMHopQA: 0.333
      NIILC: 0.113
      JSQuAD: 0.517
      XLSum: 0.12
      MGSM: 0.096
      WMT20enja: 0.031
      WMT20jaen: 0.051
      JMMLU: 0.319
      JHumanEval: 0.229
    en_pre:
      avg: 0.495
      OpenBookQA: 0.312
      TriviaQA: 0.346
      HellaSwag: 0.492
      SQuAD2: 0.503
      XWINO: 0.847
      MMLU: 0.567
      GSM8K: 0.634
      MATH: 0.344
      BBH: 0.553
      HumanEval: 0.348
- id: tiiuae/Falcon3-7B-Base
  model_id: tiiuae/Falcon3-7B-Base
  name: Falcon3-7B-Base
  date: '2024-12-19'
  params: 7.5
  active_params: 7.5
  family: Falcon3
  base: －
  sortkey: falcon3
  is_post: false
  url: https://huggingface.co/tiiuae/Falcon3-7B-Base
  results:
    ja_pre:
      avg: 0.337
      JComQA: 0.634
      JEMHopQA: 0.412
      NIILC: 0.18
      JSQuAD: 0.788
      XLSum: 0.173
      MGSM: 0.244
      WMT20enja: 0.078
      WMT20jaen: 0.119
      JMMLU: 0.385
      JHumanEval: 0.361
    en_pre:
      avg: 0.596
      OpenBookQA: 0.354
      TriviaQA: 0.552
      HellaSwag: 0.566
      SQuAD2: 0.539
      XWINO: 0.881
      MMLU: 0.701
      GSM8K: 0.766
      MATH: 0.438
      BBH: 0.692
      HumanEval: 0.476
- id: tiiuae/Falcon3-10B-Base
  model_id: tiiuae/Falcon3-10B-Base
  name: Falcon3-10B-Base
  date: '2024-12-19'
  params: 10
  active_params: 10
  family: Falcon3
  base: Falcon3-7B-Base
  sortkey: falcon3
  is_post: false
  url: https://huggingface.co/tiiuae/Falcon3-10B-Base
  results:
    ja_pre:
      avg: 0.383
      JComQA: 0.68
      JEMHopQA: 0.443
      NIILC: 0.187
      JSQuAD: 0.854
      XLSum: 0.187
      MGSM: 0.376
      WMT20enja: 0.103
      WMT20jaen: 0.139
      JMMLU: 0.435
      JHumanEval: 0.426
    en_pre:
      avg: 0.639
      OpenBookQA: 0.368
      TriviaQA: 0.579
      HellaSwag: 0.596
      SQuAD2: 0.603
      XWINO: 0.901
      MMLU: 0.732
      GSM8K: 0.802
      MATH: 0.492
      BBH: 0.776
      HumanEval: 0.543
- id: google/gemma-2-2b
  model_id: google/gemma-2-2b
  name: Gemma 2 2B
  date: '2024-06-27'
  params: 2.6
  active_params: 2.6
  family: Gemma 2
  base: －
  sortkey: gemma 2
  is_post: false
  url: https://huggingface.co/google/gemma-2-2b
  results:
    ja_pre:
      avg: 0.348
      JComQA: 0.721
      JEMHopQA: 0.472
      NIILC: 0.316
      JSQuAD: 0.81
      XLSum: 0.083
      MGSM: 0.124
      WMT20enja: 0.203
      WMT20jaen: 0.19
      JMMLU: 0.388
      JHumanEval: 0.177
    en_pre:
      avg: 0.439
      OpenBookQA: 0.342
      TriviaQA: 0.552
      HellaSwag: 0.552
      SQuAD2: 0.501
      XWINO: 0.89
      MMLU: 0.53
      GSM8K: 0.249
      MATH: 0.176
      BBH: 0.415
      HumanEval: 0.188
- id: google/gemma-2-2b-it
  model_id: google/gemma-2-2b-it
  name: Gemma 2 2B IT
  date: '2024-06-27'
  params: 2.6
  active_params: 2.6
  family: Gemma 2
  pre_training: Gemma 2 2B
  reasoning: N/A
  sortkey: gemma 2
  is_post: true
  url: https://huggingface.co/google/gemma-2-2b-it
  results:
    ja_post:
      avg: 0.269
      JEMHopQA: 0.321
      MMLU-ProX: 0.214
      GPQA: 0.248
      MATH100: 0.202
      JHumanEval: 0.359
      __MIFEvalJa: 0.416
    en_post:
      avg: 0.256
      HellaSwag: 0.596
      MMLU-Pro: 0.287
      GPQA: 0.359
      MATH500: 0.262
      AIME: 0.0
      LCB: 0.034
    ja_mtb:
      avg: 0.555
      coding: 0.46
      extraction: 0.585
      humanities: 0.673
      math: 0.448
      reasoning: 0.422
      roleplay: 0.641
      stem: 0.571
      writing: 0.639
    en_mtb:
      avg: 0.718
      coding: 0.543
      extraction: 0.687
      humanities: 0.868
      math: 0.659
      reasoning: 0.609
      roleplay: 0.78
      stem: 0.78
      writing: 0.816
- id: google/gemma-2-9b
  model_id: google/gemma-2-9b
  name: Gemma 2 9B
  date: '2024-06-27'
  params: 9.2
  active_params: 9.2
  family: Gemma 2
  base: －
  sortkey: gemma 2
  is_post: false
  url: https://huggingface.co/google/gemma-2-9b
  results:
    ja_pre:
      avg: 0.5
      JComQA: 0.904
      JEMHopQA: 0.573
      NIILC: 0.524
      JSQuAD: 0.898
      XLSum: 0.168
      MGSM: 0.456
      WMT20enja: 0.269
      WMT20jaen: 0.236
      JMMLU: 0.623
      JHumanEval: 0.345
    en_pre:
      avg: 0.597
      OpenBookQA: 0.382
      TriviaQA: 0.718
      HellaSwag: 0.626
      SQuAD2: 0.506
      XWINO: 0.907
      MMLU: 0.706
      GSM8K: 0.688
      MATH: 0.338
      BBH: 0.704
      HumanEval: 0.39
- id: google/gemma-2-9b-it
  model_id: google/gemma-2-9b-it
  name: Gemma 2 9B IT
  date: '2024-06-27'
  params: 9.2
  active_params: 9.2
  family: Gemma 2
  pre_training: Gemma 2 9B
  reasoning: N/A
  sortkey: gemma 2
  is_post: true
  url: https://huggingface.co/google/gemma-2-9b-it
  results:
    ja_post:
      avg: 0.447
      JEMHopQA: 0.506
      MMLU-ProX: 0.423
      GPQA: 0.277
      MATH100: 0.444
      JHumanEval: 0.583
      __MIFEvalJa: 0.558
    en_post:
      avg: 0.392
      HellaSwag: 0.829
      MMLU-Pro: 0.503
      GPQA: 0.369
      MATH500: 0.488
      AIME: 0.017
      LCB: 0.146
    ja_mtb:
      avg: 0.743
      coding: 0.635
      extraction: 0.816
      humanities: 0.865
      math: 0.686
      reasoning: 0.649
      roleplay: 0.784
      stem: 0.734
      writing: 0.773
    en_mtb:
      avg: 0.761
      coding: 0.624
      extraction: 0.799
      humanities: 0.893
      math: 0.682
      reasoning: 0.61
      roleplay: 0.832
      stem: 0.808
      writing: 0.841
- id: google/gemma-2-27b
  model_id: google/gemma-2-27b
  name: Gemma 2 27B
  date: '2024-06-27'
  params: 27
  active_params: 27
  family: Gemma 2
  base: －
  sortkey: gemma 2
  is_post: false
  url: https://huggingface.co/google/gemma-2-27b
  results:
    ja_pre:
      avg: 0.546
      JComQA: 0.936
      JEMHopQA: 0.553
      NIILC: 0.573
      JSQuAD: 0.916
      XLSum: 0.194
      MGSM: 0.596
      WMT20enja: 0.295
      WMT20jaen: 0.251
      JMMLU: 0.659
      JHumanEval: 0.49
    en_pre:
      avg: 0.655
      OpenBookQA: 0.412
      TriviaQA: 0.78
      HellaSwag: 0.675
      SQuAD2: 0.549
      XWINO: 0.921
      MMLU: 0.754
      GSM8K: 0.757
      MATH: 0.438
      BBH: 0.76
      HumanEval: 0.508
- id: google/gemma-2-27b-it
  model_id: google/gemma-2-27b-it
  name: Gemma 2 27B IT
  date: '2024-06-27'
  params: 27
  active_params: 27
  family: Gemma 2
  pre_training: Gemma 2 27B
  reasoning: N/A
  sortkey: gemma 2
  is_post: true
  url: https://huggingface.co/google/gemma-2-27b-it
  results:
    ja_post:
      avg: 0.506
      JEMHopQA: 0.561
      MMLU-ProX: 0.462
      GPQA: 0.304
      MATH100: 0.505
      JHumanEval: 0.7
      __MIFEvalJa: 0.588
    en_post:
      avg: 0.441
      HellaSwag: 0.846
      MMLU-Pro: 0.572
      GPQA: 0.404
      MATH500: 0.56
      AIME: 0.033
      LCB: 0.23
    ja_mtb:
      avg: 0.762
      coding: 0.76
      extraction: 0.825
      humanities: 0.874
      math: 0.697
      reasoning: 0.578
      roleplay: 0.818
      stem: 0.745
      writing: 0.796
    en_mtb:
      avg: 0.8
      coding: 0.701
      extraction: 0.855
      humanities: 0.891
      math: 0.724
      reasoning: 0.702
      roleplay: 0.843
      stem: 0.827
      writing: 0.858
- id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1
  name: Gemma-2-Llama Swallow 2B
  date: '2025-05-19'
  params: 2.6
  active_params: 2.6
  family: Gemma 2
  base: －
  sortkey: gemma 2 llama swallow
  is_post: false
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1
  results:
    ja_pre:
      avg: 0.421
      JComQA: 0.83
      JEMHopQA: 0.509
      NIILC: 0.549
      JSQuAD: 0.863
      XLSum: 0.119
      MGSM: 0.172
      WMT20enja: 0.261
      WMT20jaen: 0.195
      JMMLU: 0.461
      JHumanEval: 0.251
    en_pre:
      avg: 0.426
      OpenBookQA: 0.312
      TriviaQA: 0.435
      HellaSwag: 0.516
      SQuAD2: 0.501
      XWINO: 0.871
      MMLU: 0.538
      GSM8K: 0.275
      MATH: 0.144
      BBH: 0.384
      HumanEval: 0.286
- id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1
  name: Gemma-2-Llama Swallow 2B IT
  date: '2025-05-19'
  params: 2.6
  active_params: 2.6
  family: Gemma 2
  pre_training: Gemma2-Llama Swallow 2B
  reasoning: N/A
  sortkey: gemma 2 llama swallow
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1
  results:
    ja_post:
      avg: 0.245
      JEMHopQA: 0.274
      MMLU-ProX: 0.19
      GPQA: 0.259
      MATH100: 0.263
      JHumanEval: 0.241
      __MIFEvalJa: 0.363
    en_post:
      avg: 0.184
      HellaSwag: 0.495
      MMLU-Pro: 0.169
      GPQA: 0.268
      MATH500: 0.138
      AIME: 0.0
      LCB: 0.036
    ja_mtb:
      avg: 0.583
      coding: 0.408
      extraction: 0.551
      humanities: 0.774
      math: 0.42
      reasoning: 0.418
      roleplay: 0.725
      stem: 0.655
      writing: 0.709
    en_mtb:
      avg: 0.584
      coding: 0.461
      extraction: 0.534
      humanities: 0.758
      math: 0.376
      reasoning: 0.452
      roleplay: 0.728
      stem: 0.646
      writing: 0.715
- id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1
  name: Gemma-2-Llama Swallow 9B
  date: '2025-05-19'
  params: 9.2
  active_params: 9.2
  family: Gemma 2
  base: Gemma 2 9B
  sortkey: gemma 2 llama swallow
  is_post: false
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1
  results:
    ja_pre:
      avg: 0.558
      JComQA: 0.95
      JEMHopQA: 0.643
      NIILC: 0.677
      JSQuAD: 0.897
      XLSum: 0.187
      MGSM: 0.56
      WMT20enja: 0.304
      WMT20jaen: 0.247
      JMMLU: 0.65
      JHumanEval: 0.462
    en_pre:
      avg: 0.595
      OpenBookQA: 0.362
      TriviaQA: 0.659
      HellaSwag: 0.602
      SQuAD2: 0.532
      XWINO: 0.906
      MMLU: 0.687
      GSM8K: 0.678
      MATH: 0.33
      BBH: 0.664
      HumanEval: 0.529
- id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1
  name: Gemma-2-Llama Swallow 9B IT
  date: '2025-05-19'
  params: 9.2
  active_params: 9.2
  family: Gemma 2
  pre_training: Gemma 2 9B
  reasoning: N/A
  sortkey: gemma 2 llama swallow
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1
  results:
    ja_post:
      avg: 0.402
      JEMHopQA: 0.465
      MMLU-ProX: 0.372
      GPQA: 0.283
      MATH100: 0.374
      JHumanEval: 0.518
      __MIFEvalJa: 0.54
    en_post:
      avg: 0.323
      HellaSwag: 0.801
      MMLU-Pro: 0.296
      GPQA: 0.283
      MATH500: 0.438
      AIME: 0.017
      LCB: 0.106
    ja_mtb:
      avg: 0.729
      coding: 0.579
      extraction: 0.787
      humanities: 0.88
      math: 0.661
      reasoning: 0.616
      roleplay: 0.788
      stem: 0.735
      writing: 0.783
    en_mtb:
      avg: 0.734
      coding: 0.523
      extraction: 0.831
      humanities: 0.886
      math: 0.72
      reasoning: 0.518
      roleplay: 0.789
      stem: 0.786
      writing: 0.819
- id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1
  name: Gemma-2-Llama Swallow 27B
  date: '2025-05-19'
  params: 27
  active_params: 27
  family: Gemma 2
  base: Gemma 2 27B
  sortkey: gemma 2 llama swallow
  is_post: false
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1
  results:
    ja_pre:
      avg: 0.594
      JComQA: 0.958
      JEMHopQA: 0.66
      NIILC: 0.671
      JSQuAD: 0.924
      XLSum: 0.2
      MGSM: 0.644
      WMT20enja: 0.321
      WMT20jaen: 0.255
      JMMLU: 0.679
      JHumanEval: 0.629
    en_pre:
      avg: 0.665
      OpenBookQA: 0.414
      TriviaQA: 0.756
      HellaSwag: 0.652
      SQuAD2: 0.597
      XWINO: 0.915
      MMLU: 0.749
      GSM8K: 0.732
      MATH: 0.416
      BBH: 0.765
      HumanEval: 0.658
- id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1
  model_id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1
  name: Gemma-2-Llama Swallow 27B IT
  date: '2025-05-19'
  params: 27
  active_params: 27
  family: Gemma 2
  pre_training: Gemma 2 27B
  reasoning: N/A
  sortkey: gemma 2 llama swallow
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1
  results:
    ja_post:
      avg: 0.517
      JEMHopQA: 0.681
      MMLU-ProX: 0.452
      GPQA: 0.333
      MATH100: 0.465
      JHumanEval: 0.656
      __MIFEvalJa: 0.54
    en_post:
      avg: 0.393
      HellaSwag: 0.786
      MMLU-Pro: 0.436
      GPQA: 0.343
      MATH500: 0.544
      AIME: 0.033
      LCB: 0.218
    ja_mtb:
      avg: 0.759
      coding: 0.627
      extraction: 0.846
      humanities: 0.868
      math: 0.767
      reasoning: 0.548
      roleplay: 0.796
      stem: 0.785
      writing: 0.833
    en_mtb:
      avg: 0.771
      coding: 0.626
      extraction: 0.789
      humanities: 0.883
      math: 0.751
      reasoning: 0.622
      roleplay: 0.824
      stem: 0.821
      writing: 0.853
- id: google/gemma-3-1b-pt
  model_id: google/gemma-3-1b-pt
  name: Gemma 3 1B
  date: '2025-03-12'
  params: 1
  active_params: 1
  family: Gemma 3
  base: －
  sortkey: gemma 3
  is_post: false
  url: https://huggingface.co/google/gemma-3-1b-pt
  results:
    ja_pre:
      avg: 0.223
      JComQA: 0.237
      JEMHopQA: 0.41
      NIILC: 0.252
      JSQuAD: 0.631
      XLSum: 0.079
      MGSM: 0.024
      WMT20enja: 0.15
      WMT20jaen: 0.136
      JMMLU: 0.239
      JHumanEval: 0.073
    en_pre:
      avg: 0.31
      OpenBookQA: 0.304
      TriviaQA: 0.358
      HellaSwag: 0.471
      SQuAD2: 0.501
      XWINO: 0.832
      MMLU: 0.262
      GSM8K: 0.016
      MATH: 0.008
      BBH: 0.276
      HumanEval: 0.07
- id: google/gemma-3-1b-it
  model_id: google/gemma-3-1b-it
  name: Gemma 3 1B IT
  date: '2025-03-12'
  params: 1.0
  active_params: 1.0
  family: Gemma 3
  pre_training: Gemma 3 1B
  reasoning: N/A
  sortkey: gemma 3
  is_post: true
  url: https://huggingface.co/google/gemma-3-1b-it
  results:
    ja_post:
      avg: 0.17
      JEMHopQA: 0.168
      MMLU-ProX: 0.148
      GPQA: 0.248
      MATH100: 0.172
      JHumanEval: 0.112
      __MIFEvalJa: 0.323
    en_post:
      avg: 0.201
      HellaSwag: 0.357
      MMLU-Pro: 0.171
      GPQA: 0.237
      MATH500: 0.438
      AIME: 0.0
      LCB: 0.002
    ja_mtb:
      avg: 0.434
      coding: 0.396
      extraction: 0.484
      humanities: 0.519
      math: 0.343
      reasoning: 0.337
      roleplay: 0.519
      stem: 0.434
      writing: 0.436
    en_mtb:
      avg: 0.578
      coding: 0.503
      extraction: 0.453
      humanities: 0.719
      math: 0.709
      reasoning: 0.366
      roleplay: 0.634
      stem: 0.686
      writing: 0.553
- id: google/gemma-3-4b-pt
  model_id: google/gemma-3-4b-pt
  name: Gemma 3 4B
  date: '2025-03-12'
  params: 4.3
  active_params: 4.3
  family: Gemma 3
  base: －
  sortkey: gemma 3
  is_post: false
  url: https://huggingface.co/google/gemma-3-4b-pt
  results:
    ja_pre:
      avg: 0.417
      JComQA: 0.851
      JEMHopQA: 0.432
      NIILC: 0.41
      JSQuAD: 0.887
      XLSum: 0.139
      MGSM: 0.248
      WMT20enja: 0.23
      WMT20jaen: 0.205
      JMMLU: 0.499
      JHumanEval: 0.273
    en_pre:
      avg: 0.501
      OpenBookQA: 0.36
      TriviaQA: 0.603
      HellaSwag: 0.576
      SQuAD2: 0.502
      XWINO: 0.895
      MMLU: 0.596
      GSM8K: 0.376
      MATH: 0.258
      BBH: 0.495
      HumanEval: 0.351
- id: google/gemma-3-4b-it
  model_id: google/gemma-3-4b-it
  name: Gemma 3 4B IT
  date: '2025-03-12'
  params: 4.3
  active_params: 4.3
  family: Gemma 3
  pre_training: Gemma 3 4B
  reasoning: N/A
  sortkey: gemma 3
  is_post: true
  url: https://huggingface.co/google/gemma-3-4b-it
  results:
    ja_post:
      avg: 0.448
      JEMHopQA: 0.45
      MMLU-ProX: 0.335
      GPQA: 0.246
      MATH100: 0.606
      JHumanEval: 0.604
      __MIFEvalJa: 0.473
    en_post:
      avg: 0.405
      HellaSwag: 0.62
      MMLU-Pro: 0.44
      GPQA: 0.354
      MATH500: 0.748
      AIME: 0.117
      LCB: 0.151
    ja_mtb:
      avg: 0.735
      coding: 0.727
      extraction: 0.65
      humanities: 0.814
      math: 0.826
      reasoning: 0.482
      roleplay: 0.787
      stem: 0.796
      writing: 0.802
    en_mtb:
      avg: 0.793
      coding: 0.704
      extraction: 0.762
      humanities: 0.901
      math: 0.855
      reasoning: 0.553
      roleplay: 0.869
      stem: 0.845
      writing: 0.857
- id: google/gemma-3-12b-pt
  model_id: google/gemma-3-12b-pt
  name: Gemma 3 12B
  date: '2025-03-12'
  params: 12
  active_params: 12
  family: Gemma 3
  base: －
  sortkey: gemma 3
  is_post: false
  url: https://huggingface.co/google/gemma-3-12b-pt
  results:
    ja_pre:
      avg: 0.518
      JComQA: 0.787
      JEMHopQA: 0.563
      NIILC: 0.569
      JSQuAD: 0.911
      XLSum: 0.194
      MGSM: 0.584
      WMT20enja: 0.288
      WMT20jaen: 0.244
      JMMLU: 0.659
      JHumanEval: 0.385
    en_pre:
      avg: 0.619
      OpenBookQA: 0.398
      TriviaQA: 0.747
      HellaSwag: 0.637
      SQuAD2: 0.524
      XWINO: 0.917
      MMLU: 0.737
      GSM8K: 0.703
      MATH: 0.398
      BBH: 0.683
      HumanEval: 0.445
- id: google/gemma-3-12b-it
  model_id: google/gemma-3-12b-it
  name: Gemma 3 12B IT
  date: '2025-03-12'
  params: 12
  active_params: 12
  family: Gemma 3
  pre_training: Gemma 3 12B
  reasoning: N/A
  sortkey: gemma 3
  is_post: true
  url: https://huggingface.co/google/gemma-3-12b-it
  results:
    ja_post:
      avg: 0.597
      JEMHopQA: 0.525
      MMLU-ProX: 0.527
      GPQA: 0.373
      MATH100: 0.798
      JHumanEval: 0.763
      __MIFEvalJa: 0.619
    en_post:
      avg: 0.524
      HellaSwag: 0.816
      MMLU-Pro: 0.617
      GPQA: 0.389
      MATH500: 0.862
      AIME: 0.217
      LCB: 0.247
    ja_mtb:
      avg: 0.811
      coding: 0.784
      extraction: 0.807
      humanities: 0.88
      math: 0.858
      reasoning: 0.582
      roleplay: 0.856
      stem: 0.878
      writing: 0.844
    en_mtb:
      avg: 0.86
      coding: 0.741
      extraction: 0.862
      humanities: 0.917
      math: 0.932
      reasoning: 0.758
      roleplay: 0.891
      stem: 0.899
      writing: 0.879
- id: google/gemma-3-27b-pt
  model_id: google/gemma-3-27b-pt
  name: Gemma 3 27B
  date: '2025-03-12'
  params: 27
  active_params: 27
  family: Gemma 3
  base: －
  sortkey: gemma 3
  is_post: false
  url: https://huggingface.co/google/gemma-3-27b-pt
  results:
    ja_pre:
      avg: 0.574
      JComQA: 0.944
      JEMHopQA: 0.582
      NIILC: 0.627
      JSQuAD: 0.915
      XLSum: 0.21
      MGSM: 0.704
      WMT20enja: 0.301
      WMT20jaen: 0.255
      JMMLU: 0.724
      JHumanEval: 0.473
    en_pre:
      avg: 0.677
      OpenBookQA: 0.414
      TriviaQA: 0.809
      HellaSwag: 0.667
      SQuAD2: 0.618
      XWINO: 0.923
      MMLU: 0.78
      GSM8K: 0.801
      MATH: 0.52
      BBH: 0.732
      HumanEval: 0.507
- id: google/gemma-3-27b-it
  model_id: google/gemma-3-27b-it
  name: Gemma 3 27B IT
  date: '2025-03-12'
  params: 27
  active_params: 27
  family: Gemma 3
  pre_training: Gemma 3 27B
  reasoning: N/A
  sortkey: gemma 3
  is_post: true
  url: https://huggingface.co/google/gemma-3-27b-it
  results:
    ja_post:
      avg: 0.658
      JEMHopQA: 0.607
      MMLU-ProX: 0.609
      GPQA: 0.417
      MATH100: 0.859
      JHumanEval: 0.796
      __MIFEvalJa: 0.597
    en_post:
      avg: 0.571
      HellaSwag: 0.861
      MMLU-Pro: 0.681
      GPQA: 0.475
      MATH500: 0.88
      AIME: 0.233
      LCB: 0.298
    ja_mtb:
      avg: 0.83
      coding: 0.747
      extraction: 0.942
      humanities: 0.878
      math: 0.808
      reasoning: 0.733
      roleplay: 0.849
      stem: 0.853
      writing: 0.831
    en_mtb:
      avg: 0.88
      coding: 0.767
      extraction: 0.919
      humanities: 0.92
      math: 0.924
      reasoning: 0.797
      roleplay: 0.908
      stem: 0.917
      writing: 0.888
- id: gpt-4.1-2025-04-14
  model_id: gpt-4.1-2025-04-14
  name: GPT-4.1 (gpt-4.1-2025-04-14)
  date: '2025-04-14'
  params: 0
  active_params: 0
  family: GPT-4.1
  pre_training: (private)
  reasoning: N/A
  sortkey: gpt 4.1 (gpt 4.1 2025 04 14)
  is_post: true
  url: https://platform.openai.com/docs/models
  results:
    ja_post:
      avg: 0.808
      JEMHopQA: 0.856
      MMLU-ProX: 0.772
      GPQA: 0.603
      MATH100: 0.899
      JHumanEval: 0.911
      __MIFEvalJa: 0.81
    en_post:
      avg: 0.685
      HellaSwag: 0.94
      MMLU-Pro: 0.813
      GPQA: 0.667
      MATH500: 0.906
      AIME: 0.4
      LCB: 0.387
    ja_mtb:
      avg: 0.892
      coding: 0.917
      extraction: 0.911
      humanities: 0.885
      math: 0.98
      reasoning: 0.819
      roleplay: 0.879
      stem: 0.887
      writing: 0.858
    en_mtb:
      avg: 0.908
      coding: 0.898
      extraction: 0.936
      humanities: 0.903
      math: 0.942
      reasoning: 0.863
      roleplay: 0.901
      stem: 0.925
      writing: 0.898
- id: gpt-4o-2024-08-06
  model_id: gpt-4o-2024-08-06
  name: GPT-4o (gpt-4o-2024-08-06)
  date: '2024-08-06'
  params: 0
  active_params: 0
  family: GPT-4o
  pre_training: (private)
  reasoning: N/A
  sortkey: gpt 4o (gpt 4o 2024 08 06)
  is_post: true
  url: https://platform.openai.com/docs/models
  results:
    ja_post:
      avg: 0.71
      JEMHopQA: 0.813
      MMLU-ProX: 0.685
      GPQA: 0.453
      MATH100: 0.758
      JHumanEval: 0.844
      __MIFEvalJa: 0.704
    en_post:
      avg: 0.56
      HellaSwag: 0.93
      MMLU-Pro: 0.749
      GPQA: 0.556
      MATH500: 0.792
      AIME: 0.083
      LCB: 0.25
    ja_mtb:
      avg: 0.865
      coding: 0.896
      extraction: 0.929
      humanities: 0.874
      math: 0.895
      reasoning: 0.755
      roleplay: 0.869
      stem: 0.847
      writing: 0.855
    en_mtb:
      avg: 0.922
      coding: 0.943
      extraction: 0.927
      humanities: 0.896
      math: 0.993
      reasoning: 0.976
      roleplay: 0.874
      stem: 0.905
      writing: 0.865
- id: gpt-5-2025-08-07
  model_id: gpt-5-2025-08-07
  name: GPT-5 (gpt-5-2025-08-07)
  date: '2025-08-07'
  params: 0
  active_params: 0
  family: GPT-5
  pre_training: (private)
  reasoning: on (middle)
  sortkey: gpt 5 (gpt 5 2025 08 07)
  is_post: true
  url: https://platform.openai.com/docs/models
  results:
    ja_post:
      avg: 0.891
      JEMHopQA: 0.9
      MMLU-ProX: 0.849
      GPQA: 0.786
      MATH100: 0.98
      JHumanEval: 0.943
      __MIFEvalJa: 0.907
    en_post:
      avg: 0.875
      HellaSwag: 0.959
      MMLU-Pro: 0.865
      GPQA: 0.828
      MATH500: 0.99
      AIME: 0.933
      LCB: 0.677
    ja_mtb:
      avg: 0.882
      coding: 0.893
      extraction: 0.883
      humanities: 0.928
      math: 0.882
      reasoning: 0.758
      roleplay: 0.896
      stem: 0.933
      writing: 0.885
    en_mtb:
      avg: 0.888
      coding: 0.876
      extraction: 0.912
      humanities: 0.923
      math: 0.843
      reasoning: 0.811
      roleplay: 0.906
      stem: 0.927
      writing: 0.904
- id: openai/gpt-oss-20b
  model_id: openai/gpt-oss-20b
  name: gpt-oss-20b
  date: '2025-08-05'
  params: 22
  active_params: 3.6
  family: gpt-oss
  pre_training: (private)
  reasoning: on (middle)
  sortkey: gpt oss
  is_post: true
  url: https://huggingface.co/openai/gpt-oss-20b
  results:
    ja_post:
      avg: 0.727
      JEMHopQA: 0.506
      MMLU-ProX: 0.702
      GPQA: 0.571
      MATH100: 0.929
      JHumanEval: 0.927
      __MIFEvalJa: 0.549
    en_post:
      avg: 0.737
      HellaSwag: 0.847
      MMLU-Pro: 0.741
      GPQA: 0.636
      MATH500: 0.944
      AIME: 0.617
      LCB: 0.635
    ja_mtb:
      avg: 0.869
      coding: 0.914
      extraction: 0.917
      humanities: 0.853
      math: 0.994
      reasoning: 0.772
      roleplay: 0.772
      stem: 0.909
      writing: 0.824
    en_mtb:
      avg: 0.889
      coding: 0.913
      extraction: 0.881
      humanities: 0.935
      math: 0.913
      reasoning: 0.779
      roleplay: 0.88
      stem: 0.939
      writing: 0.869
- id: openai/gpt-oss-120b
  model_id: openai/gpt-oss-120b
  name: gpt-oss-120b
  date: '2025-08-05'
  params: 120
  active_params: 5.1
  family: gpt-oss
  pre_training: (private)
  reasoning: on (middle)
  sortkey: gpt oss
  is_post: true
  url: https://huggingface.co/openai/gpt-oss-120b
  results:
    ja_post:
      avg: 0.79
      JEMHopQA: 0.635
      MMLU-ProX: 0.756
      GPQA: 0.663
      MATH100: 0.97
      JHumanEval: 0.925
      __MIFEvalJa: 0.735
    en_post:
      avg: 0.794
      HellaSwag: 0.878
      MMLU-Pro: 0.79
      GPQA: 0.727
      MATH500: 0.966
      AIME: 0.733
      LCB: 0.67
    ja_mtb:
      avg: 0.907
      coding: 0.898
      extraction: 0.924
      humanities: 0.915
      math: 0.999
      reasoning: 0.862
      roleplay: 0.855
      stem: 0.948
      writing: 0.852
    en_mtb:
      avg: 0.918
      coding: 0.947
      extraction: 0.892
      humanities: 0.915
      math: 0.989
      reasoning: 0.871
      roleplay: 0.886
      stem: 0.96
      writing: 0.886
- id: meta-llama/Meta-Llama-3.1-8B
  model_id: meta-llama/Meta-Llama-3.1-8B
  name: Llama 3.1 8B
  date: '2024-07-23'
  params: 8.0
  active_params: 8.0
  family: Llama 3.1
  base: －
  sortkey: llama 3.1
  is_post: false
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
  results:
    ja_pre:
      avg: 0.437
      JComQA: 0.845
      JEMHopQA: 0.461
      NIILC: 0.405
      JSQuAD: 0.895
      XLSum: 0.179
      MGSM: 0.356
      WMT20enja: 0.221
      WMT20jaen: 0.21
      JMMLU: 0.479
      JHumanEval: 0.32
    en_pre:
      avg: 0.545
      OpenBookQA: 0.38
      TriviaQA: 0.702
      HellaSwag: 0.609
      SQuAD2: 0.503
      XWINO: 0.907
      MMLU: 0.651
      GSM8K: 0.507
      MATH: 0.214
      BBH: 0.616
      HumanEval: 0.364
- id: meta-llama/Meta-Llama-3.1-8B-Instruct
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  name: Llama 3.1 8B Instruct
  date: '2024-07-23'
  params: 8.0
  active_params: 8.0
  family: Llama 3
  pre_training: Llama 3.1 8B
  reasoning: N/A
  sortkey: llama 3.1
  is_post: true
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
  results:
    ja_post:
      avg: 0.403
      JEMHopQA: 0.482
      MMLU-ProX: 0.306
      GPQA: 0.261
      MATH100: 0.384
      JHumanEval: 0.58
      __MIFEvalJa: 0.381
    en_post:
      avg: 0.387
      HellaSwag: 0.769
      MMLU-Pro: 0.489
      GPQA: 0.374
      MATH500: 0.526
      AIME: 0.033
      LCB: 0.131
    ja_mtb:
      avg: 0.592
      coding: 0.528
      extraction: 0.848
      humanities: 0.585
      math: 0.6
      reasoning: 0.465
      roleplay: 0.569
      stem: 0.562
      writing: 0.577
    en_mtb:
      avg: 0.737
      coding: 0.556
      extraction: 0.816
      humanities: 0.871
      math: 0.697
      reasoning: 0.522
      roleplay: 0.821
      stem: 0.765
      writing: 0.85
- id: meta-llama/Meta-Llama-3.1-70B
  model_id: meta-llama/Meta-Llama-3.1-70B
  name: Llama 3.1 70B
  date: '2024-07-23'
  params: 70
  active_params: 70
  family: Llama 3.1
  base: －
  sortkey: llama 3.1
  is_post: false
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B
  results:
    ja_pre:
      avg: 0.566
      JComQA: 0.946
      JEMHopQA: 0.616
      NIILC: 0.603
      JSQuAD: 0.925
      XLSum: 0.228
      MGSM: 0.672
      WMT20enja: 0.287
      WMT20jaen: 0.257
      JMMLU: 0.669
      JHumanEval: 0.462
    en_pre:
      avg: 0.671
      OpenBookQA: 0.45
      TriviaQA: 0.829
      HellaSwag: 0.69
      SQuAD2: 0.605
      XWINO: 0.92
      MMLU: 0.786
      GSM8K: 0.798
      MATH: 0.434
      BBH: 0.655
      HumanEval: 0.546
- id: nvidia/Llama-3.1-Nemotron-Nano-8B-v1
  model_id: nvidia/Llama-3.1-Nemotron-Nano-8B-v1
  name: Llama-3.1-Nemotron-Nano-8B-v1
  date: '2025-03-18'
  params: 8.0
  active_params: 8.0
  family: Llama 3
  pre_training: Llama 3.1 8B
  reasoning: 'on'
  sortkey: llama 3.1 nemotron nano v1
  is_post: true
  url: https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1
  results:
    ja_post:
      avg: 0.55
      JEMHopQA: 0.202
      MMLU-ProX: 0.489
      GPQA: 0.339
      MATH100: 0.919
      JHumanEval: 0.802
      __MIFEvalJa: 0.186
    en_post:
      avg: 0.588
      HellaSwag: 0.518
      MMLU-Pro: 0.566
      GPQA: 0.47
      MATH500: 0.948
      AIME: 0.55
      LCB: 0.478
    ja_mtb:
      avg: 0.363
      coding: 0.374
      extraction: 0.503
      humanities: 0.311
      math: 0.564
      reasoning: 0.27
      roleplay: 0.289
      stem: 0.301
      writing: 0.293
    en_mtb:
      avg: 0.701
      coding: 0.658
      extraction: 0.654
      humanities: 0.696
      math: 0.906
      reasoning: 0.526
      roleplay: 0.712
      stem: 0.738
      writing: 0.72
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  model_id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  name: Llama 3.1 Swallow 8B Instruct v0.3
  date: '2024-12-23'
  params: 8.0
  active_params: 8.0
  family: Llama 3
  pre_training: Llama 3.1 Swallow 8B v0.2
  reasoning: N/A
  sortkey: llama 3.1 swallow v0.3
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  results:
    ja_post:
      avg: 0.389
      JEMHopQA: 0.549
      MMLU-ProX: 0.306
      GPQA: 0.239
      MATH100: 0.364
      JHumanEval: 0.488
      __MIFEvalJa: 0.491
    en_post:
      avg: 0.291
      HellaSwag: 0.725
      MMLU-Pro: 0.287
      GPQA: 0.293
      MATH500: 0.338
      AIME: 0.0
      LCB: 0.102
    ja_mtb:
      avg: 0.709
      coding: 0.57
      extraction: 0.783
      humanities: 0.869
      math: 0.631
      reasoning: 0.506
      roleplay: 0.782
      stem: 0.716
      writing: 0.813
    en_mtb:
      avg: 0.691
      coding: 0.528
      extraction: 0.714
      humanities: 0.886
      math: 0.562
      reasoning: 0.458
      roleplay: 0.773
      stem: 0.768
      writing: 0.838
- id: tokyotech-llm/Llama-3.3-Swallow-8B-v0.5
  model_id: tokyotech-llm/Llama-3.3-Swallow-8B-v0.5
  name: Llama 3.1 Swallow 8B v0.5
  date: '2025-06-25'
  params: 8.0
  active_params: 8.0
  family: Llama 3.1
  base: Llama 3.1 8B
  sortkey: llama 3.1 swallow v0.5
  is_post: false
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-8B-v0.5
  results:
    ja_pre:
      avg: 0.543
      JComQA: 0.952
      JEMHopQA: 0.513
      NIILC: 0.657
      JSQuAD: 0.91
      XLSum: 0.217
      MGSM: 0.572
      WMT20enja: 0.294
      WMT20jaen: 0.232
      JMMLU: 0.59
      JHumanEval: 0.491
    en_pre:
      avg: 0.597
      OpenBookQA: 0.372
      TriviaQA: 0.665
      HellaSwag: 0.597
      SQuAD2: 0.536
      XWINO: 0.9
      MMLU: 0.666
      GSM8K: 0.699
      MATH: 0.39
      BBH: 0.589
      HumanEval: 0.557
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5
  model_id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5
  name: Llama 3.1 Swallow 8B Instruct v0.5
  date: '2025-06-25'
  params: 8.0
  active_params: 8.0
  family: Llama 3
  pre_training: Llama 3.1 Swallow 8B v0.2
  reasoning: N/A
  sortkey: llama 3.1 swallow v0.5
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5
  results:
    ja_post:
      avg: 0.451
      JEMHopQA: 0.602
      MMLU-ProX: 0.369
      GPQA: 0.295
      MATH100: 0.404
      JHumanEval: 0.584
      __MIFEvalJa: 0.496
    en_post:
      avg: 0.315
      HellaSwag: 0.648
      MMLU-Pro: 0.399
      GPQA: 0.318
      MATH500: 0.452
      AIME: 0.0
      LCB: 0.072
    ja_mtb:
      avg: 0.726
      coding: 0.59
      extraction: 0.843
      humanities: 0.884
      math: 0.47
      reasoning: 0.618
      roleplay: 0.78
      stem: 0.799
      writing: 0.822
    en_mtb:
      avg: 0.753
      coding: 0.576
      extraction: 0.801
      humanities: 0.9
      math: 0.769
      reasoning: 0.499
      roleplay: 0.848
      stem: 0.796
      writing: 0.833
- id: meta-llama/Llama-3.2-1B
  model_id: meta-llama/Llama-3.2-1B
  name: Llama 3.2 1B
  date: '2024-09-25'
  params: 1.2
  active_params: 1.2
  family: Llama 3.2
  base: －
  sortkey: llama 3.2
  is_post: false
  url: https://huggingface.co/meta-llama/Llama-3.2-1B
  results:
    ja_pre:
      avg: 0.201
      JComQA: 0.208
      JEMHopQA: 0.404
      NIILC: 0.188
      JSQuAD: 0.525
      XLSum: 0.081
      MGSM: 0.024
      WMT20enja: 0.079
      WMT20jaen: 0.092
      JMMLU: 0.26
      JHumanEval: 0.15
    en_pre:
      avg: 0.339
      OpenBookQA: 0.3
      TriviaQA: 0.388
      HellaSwag: 0.477
      SQuAD2: 0.501
      XWINO: 0.849
      MMLU: 0.313
      GSM8K: 0.049
      MATH: 0.02
      BBH: 0.303
      HumanEval: 0.193
- id: meta-llama/Llama-3.2-3B
  model_id: meta-llama/Llama-3.2-3B
  name: Llama 3.2 3B
  date: '2024-09-25'
  params: 3.2
  active_params: 3.2
  family: Llama 3.2
  base: －
  sortkey: llama 3.2
  is_post: false
  url: https://huggingface.co/meta-llama/Llama-3.2-3B
  results:
    ja_pre:
      avg: 0.337
      JComQA: 0.605
      JEMHopQA: 0.443
      NIILC: 0.324
      JSQuAD: 0.816
      XLSum: 0.129
      MGSM: 0.136
      WMT20enja: 0.161
      WMT20jaen: 0.167
      JMMLU: 0.352
      JHumanEval: 0.235
    en_pre:
      avg: 0.45
      OpenBookQA: 0.326
      TriviaQA: 0.586
      HellaSwag: 0.558
      SQuAD2: 0.502
      XWINO: 0.888
      MMLU: 0.558
      GSM8K: 0.262
      MATH: 0.07
      BBH: 0.466
      HumanEval: 0.285
- id: meta-llama/Llama-3.3-70B-Instruct
  model_id: meta-llama/Llama-3.3-70B-Instruct
  name: Llama 3.3 70B Instruct
  date: '2024-12-06'
  params: 70
  active_params: 70
  family: Llama 3
  pre_training: Llama 3.1 70B
  reasoning: N/A
  sortkey: llama 3.3
  is_post: true
  url: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
  results:
    ja_post:
      avg: 0.603
      JEMHopQA: 0.557
      MMLU-ProX: 0.607
      GPQA: 0.453
      MATH100: 0.646
      JHumanEval: 0.752
      __MIFEvalJa: 0.65
    en_post:
      avg: 0.545
      HellaSwag: 0.911
      MMLU-Pro: 0.717
      GPQA: 0.48
      MATH500: 0.746
      AIME: 0.117
      LCB: 0.303
    ja_mtb:
      avg: 0.735
      coding: 0.672
      extraction: 0.878
      humanities: 0.751
      math: 0.742
      reasoning: 0.638
      roleplay: 0.762
      stem: 0.735
      writing: 0.7
    en_mtb:
      avg: 0.863
      coding: 0.795
      extraction: 0.935
      humanities: 0.891
      math: 0.895
      reasoning: 0.861
      roleplay: 0.858
      stem: 0.822
      writing: 0.847
- id: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  model_id: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  name: Llama-3.3-Nemotron-Super-49B-v1
  date: '2025-03-18'
  params: 50
  active_params: 50
  family: Llama 3
  pre_training: Llama 3.3 70B Instruct
  reasoning: N/A
  sortkey: llama 3.3 nemotron super v1
  is_post: true
  url: https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1
  results:
    ja_post:
      avg: 0.716
      JEMHopQA: 0.541
      MMLU-ProX: 0.687
      GPQA: 0.531
      MATH100: 0.919
      JHumanEval: 0.9
      __MIFEvalJa: 0.558
    en_post:
      avg: 0.711
      HellaSwag: 0.885
      MMLU-Pro: 0.783
      GPQA: 0.667
      MATH500: 0.96
      AIME: 0.567
      LCB: 0.408
    ja_mtb:
      avg: 0.806
      coding: 0.731
      extraction: 0.898
      humanities: 0.821
      math: 0.801
      reasoning: 0.755
      roleplay: 0.804
      stem: 0.809
      writing: 0.828
    en_mtb:
      avg: 0.881
      coding: 0.782
      extraction: 0.915
      humanities: 0.91
      math: 0.963
      reasoning: 0.8
      roleplay: 0.878
      stem: 0.908
      writing: 0.893
- id: tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  model_id: tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  name: Llama 3.3 Swallow 70B v0.4
  date: '2025-03-14'
  params: 70
  active_params: 70
  family: Llama 3.1
  base: Llama 3.3 70B Instruct
  sortkey: llama 3.3 swallow v0.4
  is_post: false
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  results:
    ja_pre:
      avg: 0.629
      JComQA: 0.967
      JEMHopQA: 0.671
      NIILC: 0.732
      JSQuAD: 0.924
      XLSum: 0.283
      MGSM: 0.776
      WMT20enja: 0.327
      WMT20jaen: 0.26
      JMMLU: 0.742
      JHumanEval: 0.604
    en_pre:
      avg: 0.711
      OpenBookQA: 0.424
      TriviaQA: 0.817
      HellaSwag: 0.683
      SQuAD2: 0.641
      XWINO: 0.92
      MMLU: 0.802
      GSM8K: 0.863
      MATH: 0.496
      BBH: 0.754
      HumanEval: 0.709
- id: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4
  model_id: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4
  name: Llama 3.3 Swallow 70B Instruct v0.4
  date: '2025-03-10'
  params: 70
  active_params: 70
  family: Llama 3
  pre_training: Llama 3.3 Swallow 70B v0.4
  reasoning: N/A
  sortkey: llama 3.3 swallow v0.4
  is_post: true
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4
  results:
    ja_post:
      avg: 0.594
      JEMHopQA: 0.658
      MMLU-ProX: 0.533
      GPQA: 0.355
      MATH100: 0.697
      JHumanEval: 0.727
      __MIFEvalJa: 0.593
    en_post:
      avg: 0.47
      HellaSwag: 0.884
      MMLU-Pro: 0.57
      GPQA: 0.409
      MATH500: 0.642
      AIME: 0.083
      LCB: 0.232
    ja_mtb:
      avg: 0.791
      coding: 0.696
      extraction: 0.856
      humanities: 0.881
      math: 0.807
      reasoning: 0.664
      roleplay: 0.827
      stem: 0.772
      writing: 0.822
    en_mtb:
      avg: 0.816
      coding: 0.672
      extraction: 0.902
      humanities: 0.888
      math: 0.839
      reasoning: 0.706
      roleplay: 0.828
      stem: 0.838
      writing: 0.855
- id: meta-llama/Llama-4-Scout-17B-16E
  model_id: meta-llama/Llama-4-Scout-17B-16E
  name: Llama 4 Scout
  date: '2025-04-04'
  params: 109
  active_params: 17
  family: Llama 4
  base: －
  sortkey: llama 4 scout
  is_post: false
  url: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E
  results:
    ja_pre:
      avg: 0.565
      JComQA: 0.958
      JEMHopQA: 0.595
      NIILC: 0.616
      JSQuAD: 0.915
      XLSum: 0.178
      MGSM: 0.76
      WMT20enja: 0.3
      WMT20jaen: 0.258
      JMMLU: 0.736
      JHumanEval: 0.33
    en_pre:
      avg: 0.639
      OpenBookQA: 0.432
      TriviaQA: 0.75
      HellaSwag: 0.689
      SQuAD2: 0.548
      XWINO: 0.883
      MMLU: 0.78
      GSM8K: 0.811
      MATH: 0.522
      BBH: 0.618
      HumanEval: 0.359
- id: meta-llama/Llama-4-Scout-17B-16E-Instruct
  model_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
  name: Llama 4 Scout Instruct
  date: '2025-04-04'
  params: 109
  active_params: 17
  family: Llama 4
  pre_training: Llama 4 Scout
  reasoning: N/A
  sortkey: llama 4 scout
  is_post: true
  url: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct
  results:
    ja_post:
      avg: 0.663
      JEMHopQA: 0.512
      MMLU-ProX: 0.687
      GPQA: 0.54
      MATH100: 0.758
      JHumanEval: 0.82
      __MIFEvalJa: 0.611
    en_post:
      avg: 0.594
      HellaSwag: 0.891
      MMLU-Pro: 0.744
      GPQA: 0.606
      MATH500: 0.834
      AIME: 0.183
      LCB: 0.309
    ja_mtb:
      avg: 0.789
      coding: 0.763
      extraction: 0.923
      humanities: 0.816
      math: 0.879
      reasoning: 0.615
      roleplay: 0.787
      stem: 0.752
      writing: 0.778
    en_mtb:
      avg: 0.857
      coding: 0.722
      extraction: 0.911
      humanities: 0.86
      math: 0.92
      reasoning: 0.904
      roleplay: 0.836
      stem: 0.84
      writing: 0.862
- id: llm-jp/llm-jp-3-1.8b
  model_id: llm-jp/llm-jp-3-1.8b
  name: llm-jp-3-1.8b
  date: '2024-09-25'
  params: 1.8
  active_params: 1.8
  family: llm-jp-3
  base: －
  sortkey: llm jp 3
  is_post: false
  url: https://huggingface.co/llm-jp/llm-jp-3-1.8b
  results:
    ja_pre:
      avg: 0.251
      JComQA: 0.209
      JEMHopQA: 0.463
      NIILC: 0.449
      JSQuAD: 0.703
      XLSum: 0.1
      MGSM: 0.012
      WMT20enja: 0.198
      WMT20jaen: 0.134
      JMMLU: 0.242
      JHumanEval: 0.001
    en_pre:
      avg: 0.293
      OpenBookQA: 0.244
      TriviaQA: 0.301
      HellaSwag: 0.462
      SQuAD2: 0.501
      XWINO: 0.851
      MMLU: 0.248
      GSM8K: 0.017
      MATH: 0.018
      BBH: 0.276
      HumanEval: 0.008
- id: llm-jp/llm-jp-3-3.7b
  model_id: llm-jp/llm-jp-3-3.7b
  name: llm-jp-3-3.7b
  date: '2024-09-25'
  params: 3.7
  active_params: 3.7
  family: llm-jp-3
  base: －
  sortkey: llm jp 3
  is_post: false
  url: https://huggingface.co/llm-jp/llm-jp-3-3.7b
  results:
    ja_pre:
      avg: 0.281
      JComQA: 0.203
      JEMHopQA: 0.431
      NIILC: 0.541
      JSQuAD: 0.804
      XLSum: 0.142
      MGSM: 0.06
      WMT20enja: 0.223
      WMT20jaen: 0.159
      JMMLU: 0.249
      JHumanEval: 0.0
    en_pre:
      avg: 0.324
      OpenBookQA: 0.28
      TriviaQA: 0.421
      HellaSwag: 0.506
      SQuAD2: 0.502
      XWINO: 0.876
      MMLU: 0.253
      GSM8K: 0.055
      MATH: 0.016
      BBH: 0.309
      HumanEval: 0.019
- id: llm-jp/llm-jp-3-7.2b
  model_id: llm-jp/llm-jp-3-7.2b
  name: llm-jp-3-7.2b
  date: '2025-02-05'
  params: 7.3
  active_params: 7.3
  family: llm-jp-3
  base: －
  sortkey: llm jp 3
  is_post: false
  url: https://huggingface.co/llm-jp/llm-jp-3-7.2b
  results:
    ja_pre:
      avg: 0.35
      JComQA: 0.509
      JEMHopQA: 0.481
      NIILC: 0.601
      JSQuAD: 0.863
      XLSum: 0.152
      MGSM: 0.088
      WMT20enja: 0.249
      WMT20jaen: 0.19
      JMMLU: 0.344
      JHumanEval: 0.021
    en_pre:
      avg: 0.363
      OpenBookQA: 0.312
      TriviaQA: 0.522
      HellaSwag: 0.544
      SQuAD2: 0.501
      XWINO: 0.888
      MMLU: 0.373
      GSM8K: 0.086
      MATH: 0.022
      BBH: 0.362
      HumanEval: 0.02
- id: llm-jp/llm-jp-3-13b
  model_id: llm-jp/llm-jp-3-13b
  name: llm-jp-3-13b
  date: '2024-09-25'
  params: 13
  active_params: 13
  family: llm-jp-3
  base: －
  sortkey: llm jp 3
  is_post: false
  url: https://huggingface.co/llm-jp/llm-jp-3-13b
  results:
    ja_pre:
      avg: 0.393
      JComQA: 0.65
      JEMHopQA: 0.525
      NIILC: 0.649
      JSQuAD: 0.882
      XLSum: 0.164
      MGSM: 0.16
      WMT20enja: 0.273
      WMT20jaen: 0.21
      JMMLU: 0.399
      JHumanEval: 0.023
    en_pre:
      avg: 0.399
      OpenBookQA: 0.332
      TriviaQA: 0.602
      HellaSwag: 0.57
      SQuAD2: 0.501
      XWINO: 0.902
      MMLU: 0.462
      GSM8K: 0.158
      MATH: 0.026
      BBH: 0.402
      HumanEval: 0.032
- id: llm-jp/llm-jp-3.1-1.8b-instruct4
  model_id: llm-jp/llm-jp-3.1-1.8b-instruct4
  name: llm-jp-3.1-1.8b-instruct4
  date: '2025-05-30'
  params: 1.8
  active_params: 1.8
  family: llm-jp-3
  pre_training: llm-jp-3.1-1.8b
  reasoning: N/A
  sortkey: llm jp 3.1 4
  is_post: true
  url: https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4
  results:
    ja_post:
      avg: 0.271
      JEMHopQA: 0.342
      MMLU-ProX: 0.195
      GPQA: 0.239
      MATH100: 0.212
      JHumanEval: 0.365
      __MIFEvalJa: 0.288
    en_post:
      avg: 0.178
      HellaSwag: 0.45
      MMLU-Pro: 0.163
      GPQA: 0.278
      MATH500: 0.146
      AIME: 0.0
      LCB: 0.03
    ja_mtb:
      avg: 0.657
      coding: 0.574
      extraction: 0.601
      humanities: 0.809
      math: 0.672
      reasoning: 0.446
      roleplay: 0.767
      stem: 0.697
      writing: 0.693
    en_mtb:
      avg: 0.548
      coding: 0.454
      extraction: 0.482
      humanities: 0.662
      math: 0.521
      reasoning: 0.364
      roleplay: 0.665
      stem: 0.563
      writing: 0.673
- id: llm-jp/llm-jp-3.1-13b-instruct4
  model_id: llm-jp/llm-jp-3.1-13b-instruct4
  name: llm-jp-3.1-13b-instruct4
  date: '2025-05-30'
  params: 14
  active_params: 14
  family: llm-jp-3
  pre_training: llm-jp-3.1-13b
  reasoning: N/A
  sortkey: llm jp 3.1 4
  is_post: true
  url: https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4
  results:
    ja_post:
      avg: 0.384
      JEMHopQA: 0.698
      MMLU-ProX: 0.296
      GPQA: 0.23
      MATH100: 0.232
      JHumanEval: 0.463
      __MIFEvalJa: 0.372
    en_post:
      avg: 0.244
      HellaSwag: 0.717
      MMLU-Pro: 0.252
      GPQA: 0.227
      MATH500: 0.188
      AIME: 0.0
      LCB: 0.082
    ja_mtb:
      avg: 0.733
      coding: 0.587
      extraction: 0.7
      humanities: 0.87
      math: 0.731
      reasoning: 0.559
      roleplay: 0.831
      stem: 0.775
      writing: 0.807
    en_mtb:
      avg: 0.682
      coding: 0.562
      extraction: 0.681
      humanities: 0.844
      math: 0.625
      reasoning: 0.512
      roleplay: 0.736
      stem: 0.715
      writing: 0.779
- id: google/medgemma-27b-it
  model_id: google/medgemma-27b-it
  name: MedGemma 27B IT
  date: '2025-07-09'
  params: 27
  active_params: 27
  family: Gemma 3
  pre_training: Gemma 3 27B
  reasoning: N/A
  sortkey: medgemma
  is_post: true
  url: https://huggingface.co/google/medgemma-27b-it
  results:
    ja_post:
      avg: 0.463
      JEMHopQA: 0.537
      MMLU-ProX: 0.606
      GPQA: 0.35
      MATH100: 0.818
      JHumanEval: 0.001
      __MIFEvalJa: 0.624
    en_post:
      avg: 0.495
      HellaSwag: 0.859
      MMLU-Pro: 0.654
      GPQA: 0.434
      MATH500: 0.824
      AIME: 0.2
      LCB: 0.001
    ja_mtb:
      avg: 0.778
      coding: 0.799
      extraction: 0.926
      humanities: 0.805
      math: 0.883
      reasoning: 0.646
      roleplay: 0.718
      stem: 0.758
      writing: 0.686
    en_mtb:
      avg: 0.83
      coding: 0.722
      extraction: 0.914
      humanities: 0.884
      math: 0.97
      reasoning: 0.735
      roleplay: 0.819
      stem: 0.858
      writing: 0.737
- id: o3-2025-04-16
  model_id: o3-2025-04-16
  name: o3 (o3-2025-04-16)
  date: '2025-04-16'
  params: 0
  active_params: 0
  family: o3
  pre_training: (private)
  reasoning: on (middle)
  sortkey: o3 (o3 2025 04 16)
  is_post: true
  url: https://platform.openai.com/docs/models
  results:
    ja_post:
      avg: 0.87
      JEMHopQA: 0.852
      MMLU-ProX: 0.835
      GPQA: 0.766
      MATH100: 0.97
      JHumanEval: 0.929
      __MIFEvalJa: 0.85
    en_post:
      avg: 0.846
      HellaSwag: 0.956
      MMLU-Pro: 0.857
      GPQA: 0.818
      MATH500: 0.978
      AIME: 0.817
      LCB: 0.649
    ja_mtb:
      avg: 0.903
      coding: 0.935
      extraction: 0.898
      humanities: 0.888
      math: 0.995
      reasoning: 0.809
      roleplay: 0.889
      stem: 0.941
      writing: 0.867
    en_mtb:
      avg: 0.917
      coding: 0.929
      extraction: 0.931
      humanities: 0.945
      math: 0.964
      reasoning: 0.836
      roleplay: 0.9
      stem: 0.938
      writing: 0.892
- id: o3-mini-2025-01-31
  model_id: o3-mini-2025-01-31
  name: o3-mini (o3-mini-2025-01-31)
  date: '2025-01-31'
  params: 0
  active_params: 0
  family: o3
  pre_training: (private)
  reasoning: on (middle)
  sortkey: o3 mini (o3 mini 2025 01 31)
  is_post: true
  url: https://platform.openai.com/docs/models
  results:
    ja_post:
      avg: 0.785
      JEMHopQA: 0.607
      MMLU-ProX: 0.76
      GPQA: 0.685
      MATH100: 0.939
      JHumanEval: 0.934
      __MIFEvalJa: 0.841
    en_post:
      avg: 0.767
      HellaSwag: 0.869
      MMLU-Pro: 0.792
      GPQA: 0.747
      MATH500: 0.958
      AIME: 0.733
      LCB: 0.503
    ja_mtb:
      avg: 0.88
      coding: 0.868
      extraction: 0.937
      humanities: 0.86
      math: 0.952
      reasoning: 0.802
      roleplay: 0.863
      stem: 0.893
      writing: 0.868
    en_mtb:
      avg: 0.901
      coding: 0.876
      extraction: 0.913
      humanities: 0.891
      math: 0.969
      reasoning: 0.865
      roleplay: 0.895
      stem: 0.914
      writing: 0.882
- id: microsoft/phi-4
  model_id: microsoft/phi-4
  name: Phi-4
  date: '2024-12-13'
  params: 15
  active_params: 15
  family: Phi 4
  pre_training: (private)
  reasoning: N/A
  sortkey: phi 4
  is_post: true
  url: https://huggingface.co/microsoft/phi-4
  results:
    ja_post:
      avg: 0.646
      JEMHopQA: 0.589
      MMLU-ProX: 0.638
      GPQA: 0.435
      MATH100: 0.798
      JHumanEval: 0.77
      __MIFEvalJa: 0.438
    en_post:
      avg: 0.547
      HellaSwag: 0.859
      MMLU-Pro: 0.63
      GPQA: 0.551
      MATH500: 0.8
      AIME: 0.217
      LCB: 0.227
    ja_mtb:
      avg: 0.822
      coding: 0.752
      extraction: 0.933
      humanities: 0.862
      math: 0.89
      reasoning: 0.629
      roleplay: 0.83
      stem: 0.845
      writing: 0.835
    en_mtb:
      avg: 0.881
      coding: 0.771
      extraction: 0.904
      humanities: 0.876
      math: 0.928
      reasoning: 0.933
      roleplay: 0.889
      stem: 0.879
      writing: 0.865
- id: microsoft/phi-4-reasoning-plus
  model_id: microsoft/phi-4-reasoning-plus
  name: Phi-4-reasoning-plus
  date: '2025-04-30'
  params: 15
  active_params: 15
  family: Phi 4
  pre_training: (private)
  reasoning: 'on'
  sortkey: phi 4 reasoning plus
  is_post: true
  url: https://huggingface.co/microsoft/phi-4-reasoning-plus
  results:
    ja_post:
      avg: 0.437
      JEMHopQA: 0.015
      MMLU-ProX: 0.118
      GPQA: 0.563
      MATH100: 0.737
      JHumanEval: 0.751
      __MIFEvalJa: 0.221
    en_post:
      avg: 0.469
      HellaSwag: 0.26
      MMLU-Pro: 0.113
      GPQA: 0.611
      MATH500: 0.77
      AIME: 0.583
      LCB: 0.478
    ja_mtb:
      avg: 0.374
      coding: 0.205
      extraction: 0.376
      humanities: 0.206
      math: 0.379
      reasoning: 0.283
      roleplay: 0.643
      stem: 0.162
      writing: 0.741
    en_mtb:
      avg: 0.426
      coding: 0.281
      extraction: 0.384
      humanities: 0.116
      math: 0.437
      reasoning: 0.322
      roleplay: 0.769
      stem: 0.299
      writing: 0.8
- id: pfnet/plamo-2-1b
  model_id: pfnet/plamo-2-1b
  name: PLaMo 2 1B
  date: '2025-02-21'
  params: 1.3
  active_params: 1.3
  family: PLaMo 2
  base: －
  sortkey: plamo 2
  is_post: false
  url: https://huggingface.co/pfnet/plamo-2-1b
  results:
    ja_pre:
      avg: 0.25
      JComQA: 0.203
      JEMHopQA: 0.463
      NIILC: 0.434
      JSQuAD: 0.626
      XLSum: 0.055
      MGSM: 0.052
      WMT20enja: 0.236
      WMT20jaen: 0.119
      JMMLU: 0.256
      JHumanEval: 0.057
    en_pre:
      avg: 0.274
      OpenBookQA: 0.28
      TriviaQA: 0.129
      HellaSwag: 0.425
      SQuAD2: 0.501
      XWINO: 0.807
      MMLU: 0.294
      GSM8K: 0.072
      MATH: 0.034
      BBH: 0.122
      HumanEval: 0.08
- id: pfnet/plamo-2-8b
  model_id: pfnet/plamo-2-8b
  name: PLaMo 2 8B
  date: '2025-02-21'
  params: 9.1
  active_params: 9.1
  family: PLaMo 2
  base: －
  sortkey: plamo 2
  is_post: false
  url: https://huggingface.co/pfnet/plamo-2-8b
  results:
    ja_pre:
      avg: 0.481
      JComQA: 0.909
      JEMHopQA: 0.474
      NIILC: 0.655
      JSQuAD: 0.91
      XLSum: 0.12
      MGSM: 0.508
      WMT20enja: 0.28
      WMT20jaen: 0.205
      JMMLU: 0.536
      JHumanEval: 0.213
    en_pre:
      avg: 0.474
      OpenBookQA: 0.346
      TriviaQA: 0.584
      HellaSwag: 0.56
      SQuAD2: 0.511
      XWINO: 0.89
      MMLU: 0.575
      GSM8K: 0.55
      MATH: 0.2
      BBH: 0.26
      HumanEval: 0.26
- id: Qwen/Qwen2.5-1.5B
  model_id: Qwen/Qwen2.5-1.5B
  name: Qwen2.5-1.5B
  date: '2024-09-19'
  params: 1.5
  active_params: 1.5
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-1.5B
  results:
    ja_pre:
      avg: 0.372
      JComQA: 0.8
      JEMHopQA: 0.383
      NIILC: 0.241
      JSQuAD: 0.849
      XLSum: 0.143
      MGSM: 0.292
      WMT20enja: 0.132
      WMT20jaen: 0.134
      JMMLU: 0.438
      JHumanEval: 0.308
    en_pre:
      avg: 0.49
      OpenBookQA: 0.342
      TriviaQA: 0.397
      HellaSwag: 0.499
      SQuAD2: 0.506
      XWINO: 0.851
      MMLU: 0.61
      GSM8K: 0.611
      MATH: 0.314
      BBH: 0.413
      HumanEval: 0.356
- id: Qwen/Qwen2.5-3B
  model_id: Qwen/Qwen2.5-3B
  name: Qwen2.5-3B
  date: '2024-09-19'
  params: 3.1
  active_params: 3.1
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-3B
  results:
    ja_pre:
      avg: 0.442
      JComQA: 0.847
      JEMHopQA: 0.475
      NIILC: 0.306
      JSQuAD: 0.878
      XLSum: 0.176
      MGSM: 0.46
      WMT20enja: 0.18
      WMT20jaen: 0.167
      JMMLU: 0.529
      JHumanEval: 0.404
    en_pre:
      avg: 0.534
      OpenBookQA: 0.36
      TriviaQA: 0.504
      HellaSwag: 0.553
      SQuAD2: 0.541
      XWINO: 0.872
      MMLU: 0.657
      GSM8K: 0.58
      MATH: 0.44
      BBH: 0.442
      HumanEval: 0.387
- id: Qwen/Qwen2.5-7B
  model_id: Qwen/Qwen2.5-7B
  name: Qwen2.5-7B
  date: '2024-09-19'
  params: 7.6
  active_params: 7.6
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-7B
  results:
    ja_pre:
      avg: 0.512
      JComQA: 0.924
      JEMHopQA: 0.459
      NIILC: 0.426
      JSQuAD: 0.907
      XLSum: 0.216
      MGSM: 0.616
      WMT20enja: 0.229
      WMT20jaen: 0.199
      JMMLU: 0.634
      JHumanEval: 0.507
    en_pre:
      avg: 0.63
      OpenBookQA: 0.392
      TriviaQA: 0.601
      HellaSwag: 0.6
      SQuAD2: 0.618
      XWINO: 0.888
      MMLU: 0.742
      GSM8K: 0.832
      MATH: 0.51
      BBH: 0.562
      HumanEval: 0.554
- id: Qwen/Qwen2.5-7B-Instruct
  model_id: Qwen/Qwen2.5-7B-Instruct
  name: Qwen2.5-7B-Instruct
  date: '2024-09-19'
  params: 7.6
  active_params: 7.6
  family: Qwen2
  pre_training: Qwen2.5-7B
  reasoning: N/A
  sortkey: qwen2.5
  is_post: true
  url: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
  results:
    ja_post:
      avg: 0.502
      JEMHopQA: 0.372
      MMLU-ProX: 0.452
      GPQA: 0.315
      MATH100: 0.636
      JHumanEval: 0.737
      __MIFEvalJa: 0.504
    en_post:
      avg: 0.454
      HellaSwag: 0.82
      MMLU-Pro: 0.554
      GPQA: 0.348
      MATH500: 0.742
      AIME: 0.1
      LCB: 0.158
    ja_mtb:
      avg: 0.688
      coding: 0.638
      extraction: 0.711
      humanities: 0.782
      math: 0.685
      reasoning: 0.494
      roleplay: 0.736
      stem: 0.73
      writing: 0.729
    en_mtb:
      avg: 0.797
      coding: 0.656
      extraction: 0.769
      humanities: 0.893
      math: 0.843
      reasoning: 0.662
      roleplay: 0.832
      stem: 0.886
      writing: 0.833
- id: Qwen/Qwen2.5-14B
  model_id: Qwen/Qwen2.5-14B
  name: Qwen2.5-14B
  date: '2024-09-19'
  params: 14
  active_params: 14
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-14B
  results:
    ja_pre:
      avg: 0.568
      JComQA: 0.958
      JEMHopQA: 0.567
      NIILC: 0.537
      JSQuAD: 0.923
      XLSum: 0.225
      MGSM: 0.74
      WMT20enja: 0.26
      WMT20jaen: 0.23
      JMMLU: 0.69
      JHumanEval: 0.55
    en_pre:
      avg: 0.66
      OpenBookQA: 0.412
      TriviaQA: 0.666
      HellaSwag: 0.642
      SQuAD2: 0.63
      XWINO: 0.899
      MMLU: 0.797
      GSM8K: 0.793
      MATH: 0.53
      BBH: 0.686
      HumanEval: 0.544
- id: Qwen/Qwen2.5-14B-Instruct
  model_id: Qwen/Qwen2.5-14B-Instruct
  name: Qwen2.5-14B-Instruct
  date: '2024-09-19'
  params: 15
  active_params: 15
  family: Qwen2.5
  pre_training: Qwen2.5-14B
  reasoning: N/A
  sortkey: qwen2.5
  is_post: true
  url: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
  results:
    ja_post:
      avg: 0.596
      JEMHopQA: 0.553
      MMLU-ProX: 0.556
      GPQA: 0.348
      MATH100: 0.768
      JHumanEval: 0.754
      __MIFEvalJa: 0.606
    en_post:
      avg: 0.514
      HellaSwag: 0.886
      MMLU-Pro: 0.652
      GPQA: 0.404
      MATH500: 0.794
      AIME: 0.133
      LCB: 0.215
    ja_mtb:
      avg: 0.799
      coding: 0.773
      extraction: 0.882
      humanities: 0.85
      math: 0.796
      reasoning: 0.646
      roleplay: 0.829
      stem: 0.795
      writing: 0.822
    en_mtb:
      avg: 0.865
      coding: 0.752
      extraction: 0.873
      humanities: 0.899
      math: 0.932
      reasoning: 0.861
      roleplay: 0.87
      stem: 0.894
      writing: 0.839
- id: Qwen/Qwen2.5-32B
  model_id: Qwen/Qwen2.5-32B
  name: Qwen2.5-32B
  date: '2024-09-19'
  params: 33
  active_params: 33
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-32B
  results:
    ja_pre:
      avg: 0.591
      JComQA: 0.961
      JEMHopQA: 0.561
      NIILC: 0.538
      JSQuAD: 0.925
      XLSum: 0.228
      MGSM: 0.808
      WMT20enja: 0.271
      WMT20jaen: 0.233
      JMMLU: 0.751
      JHumanEval: 0.637
    en_pre:
      avg: 0.67
      OpenBookQA: 0.406
      TriviaQA: 0.664
      HellaSwag: 0.656
      SQuAD2: 0.668
      XWINO: 0.913
      MMLU: 0.832
      GSM8K: 0.718
      MATH: 0.6
      BBH: 0.717
      HumanEval: 0.523
- id: Qwen/Qwen2.5-32B-Instruct
  model_id: Qwen/Qwen2.5-32B-Instruct
  name: Qwen2.5-32B-Instruct
  date: '2024-09-19'
  params: 33
  active_params: 33
  family: Qwen2.5
  pre_training: Qwen2.5-32B
  reasoning: N/A
  sortkey: qwen2.5
  is_post: true
  url: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
  results:
    ja_post:
      avg: 0.642
      JEMHopQA: 0.604
      MMLU-ProX: 0.623
      GPQA: 0.411
      MATH100: 0.768
      JHumanEval: 0.803
      __MIFEvalJa: 0.673
    en_post:
      avg: 0.543
      HellaSwag: 0.908
      MMLU-Pro: 0.64
      GPQA: 0.48
      MATH500: 0.812
      AIME: 0.15
      LCB: 0.27
    ja_mtb:
      avg: 0.819
      coding: 0.776
      extraction: 0.913
      humanities: 0.845
      math: 0.863
      reasoning: 0.706
      roleplay: 0.839
      stem: 0.802
      writing: 0.811
    en_mtb:
      avg: 0.869
      coding: 0.806
      extraction: 0.862
      humanities: 0.895
      math: 0.954
      reasoning: 0.817
      roleplay: 0.876
      stem: 0.89
      writing: 0.851
- id: Qwen/Qwen2.5-72B
  model_id: Qwen/Qwen2.5-72B
  name: Qwen2.5-72B
  date: '2024-09-19'
  params: 72
  active_params: 72
  family: Qwen2.5
  base: －
  sortkey: qwen2.5
  is_post: false
  url: https://huggingface.co/Qwen/Qwen2.5-72B
  results:
    ja_pre:
      avg: 0.623
      JComQA: 0.972
      JEMHopQA: 0.611
      NIILC: 0.619
      JSQuAD: 0.93
      XLSum: 0.279
      MGSM: 0.828
      WMT20enja: 0.287
      WMT20jaen: 0.252
      JMMLU: 0.804
      JHumanEval: 0.648
    en_pre:
      avg: 0.709
      OpenBookQA: 0.416
      TriviaQA: 0.76
      HellaSwag: 0.685
      SQuAD2: 0.693
      XWINO: 0.901
      MMLU: 0.861
      GSM8K: 0.87
      MATH: 0.626
      BBH: 0.727
      HumanEval: 0.554
- id: Qwen/Qwen3-0.6B
  model_id: Qwen/Qwen3-0.6B
  name: Qwen3-0.6B
  date: '2025-04-29'
  params: 0.5
  active_params: 0.5
  family: Qwen3
  pre_training: Qwen3-0.6B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-0.6B
  results:
    ja_post:
      avg: 0.353
      JEMHopQA: 0.221
      MMLU-ProX: 0.295
      GPQA: 0.237
      MATH100: 0.606
      JHumanEval: 0.408
      __MIFEvalJa: 0.438
    en_post:
      avg: 0.335
      HellaSwag: 0.425
      MMLU-Pro: 0.338
      GPQA: 0.283
      MATH500: 0.694
      AIME: 0.133
      LCB: 0.135
    ja_mtb:
      avg: 0.431
      coding: 0.332
      extraction: 0.423
      humanities: 0.46
      math: 0.626
      reasoning: 0.346
      roleplay: 0.418
      stem: 0.445
      writing: 0.402
    en_mtb:
      avg: 0.595
      coding: 0.376
      extraction: 0.678
      humanities: 0.673
      math: 0.803
      reasoning: 0.408
      roleplay: 0.551
      stem: 0.633
      writing: 0.637
- id: Qwen/Qwen3-0.6B-Base
  model_id: Qwen/Qwen3-0.6B-Base
  name: Qwen3-0.6B-Base
  date: '2025-04-29'
  params: 0.6
  active_params: 0.6
  family: Qwen3
  base: －
  sortkey: qwen3
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-0.6B-Base
  results:
    ja_pre:
      avg: 0.322
      JComQA: 0.705
      JEMHopQA: 0.344
      NIILC: 0.175
      JSQuAD: 0.807
      XLSum: 0.111
      MGSM: 0.3
      WMT20enja: 0.096
      WMT20jaen: 0.097
      JMMLU: 0.373
      JHumanEval: 0.216
    en_pre:
      avg: 0.397
      OpenBookQA: 0.268
      TriviaQA: 0.2
      HellaSwag: 0.41
      SQuAD2: 0.501
      XWINO: 0.782
      MMLU: 0.523
      GSM8K: 0.483
      MATH: 0.334
      BBH: 0.17
      HumanEval: 0.295
- id: Qwen/Qwen3-1.7B
  model_id: Qwen/Qwen3-1.7B
  name: Qwen3-1.7B
  date: '2025-04-29'
  params: 1.5
  active_params: 1.5
  family: Qwen3
  pre_training: Qwen3-1.7B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-1.7B
  results:
    ja_post:
      avg: 0.533
      JEMHopQA: 0.23
      MMLU-ProX: 0.514
      GPQA: 0.315
      MATH100: 0.859
      JHumanEval: 0.747
      __MIFEvalJa: 0.46
    en_post:
      avg: 0.531
      HellaSwag: 0.626
      MMLU-Pro: 0.56
      GPQA: 0.394
      MATH500: 0.904
      AIME: 0.383
      LCB: 0.315
    ja_mtb:
      avg: 0.662
      coding: 0.574
      extraction: 0.591
      humanities: 0.715
      math: 0.841
      reasoning: 0.567
      roleplay: 0.631
      stem: 0.765
      writing: 0.613
    en_mtb:
      avg: 0.779
      coding: 0.642
      extraction: 0.754
      humanities: 0.83
      math: 0.968
      reasoning: 0.686
      roleplay: 0.764
      stem: 0.785
      writing: 0.805
- id: Qwen/Qwen3-1.7B-Base
  model_id: Qwen/Qwen3-1.7B-Base
  name: Qwen3-1.7B-Base
  date: '2025-04-29'
  params: 1.7
  active_params: 1.7
  family: Qwen3
  base: －
  sortkey: qwen3
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-1.7B-Base
  results:
    ja_pre:
      avg: 0.423
      JComQA: 0.855
      JEMHopQA: 0.435
      NIILC: 0.3
      JSQuAD: 0.871
      XLSum: 0.133
      MGSM: 0.472
      WMT20enja: 0.16
      WMT20jaen: 0.154
      JMMLU: 0.501
      JHumanEval: 0.35
    en_pre:
      avg: 0.516
      OpenBookQA: 0.348
      TriviaQA: 0.362
      HellaSwag: 0.493
      SQuAD2: 0.504
      XWINO: 0.849
      MMLU: 0.626
      GSM8K: 0.629
      MATH: 0.456
      BBH: 0.431
      HumanEval: 0.462
- id: Qwen/Qwen3-4B
  model_id: Qwen/Qwen3-4B
  name: Qwen3-4B
  date: '2025-04-29'
  params: 3.1
  active_params: 3.1
  family: Qwen3
  pre_training: Qwen3-4B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-4B
  results:
    ja_post:
      avg: 0.646
      JEMHopQA: 0.389
      MMLU-ProX: 0.643
      GPQA: 0.44
      MATH100: 0.919
      JHumanEval: 0.838
      __MIFEvalJa: 0.562
    en_post:
      avg: 0.672
      HellaSwag: 0.79
      MMLU-Pro: 0.69
      GPQA: 0.515
      MATH500: 0.938
      AIME: 0.6
      LCB: 0.499
    ja_mtb:
      avg: 0.797
      coding: 0.696
      extraction: 0.818
      humanities: 0.855
      math: 0.947
      reasoning: 0.729
      roleplay: 0.747
      stem: 0.826
      writing: 0.76
    en_mtb:
      avg: 0.839
      coding: 0.737
      extraction: 0.831
      humanities: 0.884
      math: 0.947
      reasoning: 0.735
      roleplay: 0.87
      stem: 0.861
      writing: 0.845
- id: Qwen/Qwen3-4B-Base
  model_id: Qwen/Qwen3-4B-Base
  name: Qwen3-4B-Base
  date: '2025-04-29'
  params: 4.0
  active_params: 4.0
  family: Qwen3
  base: －
  sortkey: qwen3
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-4B-Base
  results:
    ja_pre:
      avg: 0.511
      JComQA: 0.91
      JEMHopQA: 0.477
      NIILC: 0.407
      JSQuAD: 0.908
      XLSum: 0.169
      MGSM: 0.644
      WMT20enja: 0.214
      WMT20jaen: 0.197
      JMMLU: 0.649
      JHumanEval: 0.537
    en_pre:
      avg: 0.61
      OpenBookQA: 0.382
      TriviaQA: 0.508
      HellaSwag: 0.555
      SQuAD2: 0.588
      XWINO: 0.891
      MMLU: 0.729
      GSM8K: 0.719
      MATH: 0.52
      BBH: 0.594
      HumanEval: 0.617
- id: Qwen/Qwen3-8B-Base
  model_id: Qwen/Qwen3-8B-Base
  name: Qwen3-8B-Base
  date: '2025-04-29'
  params: 8.2
  active_params: 8.2
  family: Qwen3
  base: －
  sortkey: qwen3
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-8B-Base
  results:
    ja_pre:
      avg: 0.551
      JComQA: 0.927
      JEMHopQA: 0.537
      NIILC: 0.475
      JSQuAD: 0.912
      XLSum: 0.207
      MGSM: 0.716
      WMT20enja: 0.241
      WMT20jaen: 0.215
      JMMLU: 0.689
      JHumanEval: 0.595
    en_pre:
      avg: 0.667
      OpenBookQA: 0.382
      TriviaQA: 0.618
      HellaSwag: 0.594
      SQuAD2: 0.602
      XWINO: 0.903
      MMLU: 0.765
      GSM8K: 0.855
      MATH: 0.622
      BBH: 0.655
      HumanEval: 0.669
- id: Qwen/Qwen3-8B
  model_id: Qwen/Qwen3-8B
  name: Qwen3-8B
  date: '2025-04-29'
  params: 8.2
  active_params: 8.2
  family: Qwen3
  pre_training: Qwen3-8B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-8B
  results:
    ja_post:
      avg: 0.691
      JEMHopQA: 0.468
      MMLU-ProX: 0.696
      GPQA: 0.491
      MATH100: 0.929
      JHumanEval: 0.869
      __MIFEvalJa: 0.575
    en_post:
      avg: 0.715
      HellaSwag: 0.851
      MMLU-Pro: 0.713
      GPQA: 0.561
      MATH500: 0.942
      AIME: 0.7
      LCB: 0.525
    ja_mtb:
      avg: 0.845
      coding: 0.757
      extraction: 0.834
      humanities: 0.89
      math: 0.996
      reasoning: 0.823
      roleplay: 0.829
      stem: 0.822
      writing: 0.806
    en_mtb:
      avg: 0.851
      coding: 0.804
      extraction: 0.831
      humanities: 0.892
      math: 0.98
      reasoning: 0.713
      roleplay: 0.858
      stem: 0.876
      writing: 0.854
- id: Qwen/Qwen3-14B-Base
  model_id: Qwen/Qwen3-14B-Base
  name: Qwen3-14B-Base
  date: '2025-04-29'
  params: 15
  active_params: 15
  family: Qwen3
  base: －
  sortkey: qwen3
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-14B-Base
  results:
    ja_pre:
      avg: 0.591
      JComQA: 0.956
      JEMHopQA: 0.579
      NIILC: 0.502
      JSQuAD: 0.921
      XLSum: 0.261
      MGSM: 0.768
      WMT20enja: 0.26
      WMT20jaen: 0.229
      JMMLU: 0.729
      JHumanEval: 0.709
    en_pre:
      avg: 0.66
      OpenBookQA: 0.416
      TriviaQA: 0.657
      HellaSwag: 0.625
      SQuAD2: 0.669
      XWINO: 0.901
      MMLU: 0.806
      GSM8K: 0.799
      MATH: 0.548
      BBH: 0.466
      HumanEval: 0.709
- id: Qwen/Qwen3-14B
  model_id: Qwen/Qwen3-14B
  name: Qwen3-14B
  date: '2025-04-29'
  params: 15
  active_params: 15
  family: Qwen3
  pre_training: Qwen3-14B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-14B
  results:
    ja_post:
      avg: 0.75
      JEMHopQA: 0.609
      MMLU-ProX: 0.737
      GPQA: 0.556
      MATH100: 0.939
      JHumanEval: 0.91
      __MIFEvalJa: 0.624
    en_post:
      avg: 0.763
      HellaSwag: 0.89
      MMLU-Pro: 0.77
      GPQA: 0.611
      MATH500: 0.972
      AIME: 0.75
      LCB: 0.587
    ja_mtb:
      avg: 0.874
      coding: 0.85
      extraction: 0.839
      humanities: 0.903
      math: 0.994
      reasoning: 0.824
      roleplay: 0.839
      stem: 0.919
      writing: 0.827
    en_mtb:
      avg: 0.882
      coding: 0.843
      extraction: 0.849
      humanities: 0.904
      math: 0.971
      reasoning: 0.805
      roleplay: 0.878
      stem: 0.919
      writing: 0.89
- id: Qwen/Qwen3-32B
  model_id: Qwen/Qwen3-32B
  name: Qwen3-32B
  date: '2025-04-29'
  params: 33
  active_params: 33
  family: Qwen3
  pre_training: Qwen3-32B-Base
  reasoning: 'on'
  sortkey: qwen3
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-32B
  results:
    ja_post:
      avg: 0.756
      JEMHopQA: 0.588
      MMLU-ProX: 0.746
      GPQA: 0.571
      MATH100: 0.949
      JHumanEval: 0.923
      __MIFEvalJa: 0.681
    en_post:
      avg: 0.768
      HellaSwag: 0.901
      MMLU-Pro: 0.779
      GPQA: 0.646
      MATH500: 0.964
      AIME: 0.717
      LCB: 0.602
    ja_mtb:
      avg: 0.875
      coding: 0.794
      extraction: 0.871
      humanities: 0.871
      math: 0.997
      reasoning: 0.836
      roleplay: 0.881
      stem: 0.917
      writing: 0.83
    en_mtb:
      avg: 0.892
      coding: 0.86
      extraction: 0.91
      humanities: 0.905
      math: 0.979
      reasoning: 0.796
      roleplay: 0.899
      stem: 0.919
      writing: 0.869
- id: Qwen/Qwen3-30B-A3B-Base
  model_id: Qwen/Qwen3-30B-A3B-Base
  name: Qwen3-30B-A3B-Base
  date: '2025-04-29'
  params: 31
  active_params: 3.3
  family: Qwen3
  base: －
  sortkey: qwen3 a
  is_post: false
  url: https://huggingface.co/Qwen/Qwen3-30B-A3B-Base
  results:
    ja_pre:
      avg: 0.58
      JComQA: 0.927
      JEMHopQA: 0.601
      NIILC: 0.525
      JSQuAD: 0.918
      XLSum: 0.223
      MGSM: 0.74
      WMT20enja: 0.26
      WMT20jaen: 0.224
      JMMLU: 0.743
      JHumanEval: 0.641
    en_pre:
      avg: 0.691
      OpenBookQA: 0.414
      TriviaQA: 0.653
      HellaSwag: 0.631
      SQuAD2: 0.619
      XWINO: 0.901
      MMLU: 0.812
      GSM8K: 0.828
      MATH: 0.634
      BBH: 0.72
      HumanEval: 0.694
- id: Qwen/Qwen3-235B-A22B-Instruct-2507
  model_id: Qwen/Qwen3-235B-A22B-Instruct-2507
  name: Qwen3-235B-A22B-Instruct-2507
  date: '2025-07-23'
  params: 235
  active_params: 22
  family: Qwen3
  pre_training: (private)
  reasoning: N/A
  sortkey: qwen3 a 2507
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507
  results:
    ja_post:
      avg: 0.821
      JEMHopQA: 0.735
      MMLU-ProX: 0.799
      GPQA: 0.701
      MATH100: 0.97
      JHumanEval: 0.9
      __MIFEvalJa: 0.73
    en_post:
      avg: 0.771
      HellaSwag: 0.94
      MMLU-Pro: 0.824
      GPQA: 0.586
      MATH500: 0.982
      AIME: 0.767
      LCB: 0.529
    ja_mtb:
      avg: 0.915
      coding: 0.943
      extraction: 0.938
      humanities: 0.907
      math: 0.987
      reasoning: 0.826
      roleplay: 0.893
      stem: 0.933
      writing: 0.891
    en_mtb:
      avg: 0.911
      coding: 0.888
      extraction: 0.859
      humanities: 0.925
      math: 0.99
      reasoning: 0.873
      roleplay: 0.911
      stem: 0.94
      writing: 0.905
- id: Qwen/Qwen3-235B-A22B-Thinking-2507
  model_id: Qwen/Qwen3-235B-A22B-Thinking-2507
  name: Qwen3-235B-A22B-Thinking-2507
  date: '2025-07-23'
  params: 235
  active_params: 22
  family: Qwen3
  pre_training: (private)
  reasoning: 'on'
  sortkey: qwen3 a thinking 2507
  is_post: true
  url: https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507
  results:
    ja_post:
      avg: 0.823
      JEMHopQA: 0.651
      MMLU-ProX: 0.819
      GPQA: 0.739
      MATH100: 0.97
      JHumanEval: 0.938
      __MIFEvalJa: 0.783
    en_post:
      avg: 0.856
      HellaSwag: 0.931
      MMLU-Pro: 0.845
      GPQA: 0.803
      MATH500: 0.98
      AIME: 0.883
      LCB: 0.692
    ja_mtb:
      avg: 0.904
      coding: 0.896
      extraction: 0.878
      humanities: 0.933
      math: 0.985
      reasoning: 0.851
      roleplay: 0.876
      stem: 0.955
      writing: 0.861
    en_mtb:
      avg: 0.922
      coding: 0.877
      extraction: 0.899
      humanities: 0.945
      math: 0.998
      reasoning: 0.886
      roleplay: 0.908
      stem: 0.952
      writing: 0.908
- id: sbintuitions/sarashina2-7b
  model_id: sbintuitions/sarashina2-7b
  name: Sarashina2-7B
  date: '2024-06-14'
  params: 7.3
  active_params: 7.3
  family: Sarashina2
  base: －
  sortkey: sarashina2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2-7b
  results:
    ja_pre:
      avg: 0.395
      JComQA: 0.742
      JEMHopQA: 0.509
      NIILC: 0.634
      JSQuAD: 0.868
      XLSum: 0.141
      MGSM: 0.08
      WMT20enja: 0.273
      WMT20jaen: 0.201
      JMMLU: 0.384
      JHumanEval: 0.121
    en_pre:
      avg: 0.383
      OpenBookQA: 0.346
      TriviaQA: 0.479
      HellaSwag: 0.532
      SQuAD2: 0.501
      XWINO: 0.892
      MMLU: 0.425
      GSM8K: 0.101
      MATH: 0.034
      BBH: 0.373
      HumanEval: 0.146
- id: sbintuitions/sarashina2-13b
  model_id: sbintuitions/sarashina2-13b
  name: Sarashina2-13B
  date: '2024-06-14'
  params: 13
  active_params: 13
  family: Sarashina2
  base: －
  sortkey: sarashina2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2-13b
  results:
    ja_pre:
      avg: 0.445
      JComQA: 0.85
      JEMHopQA: 0.557
      NIILC: 0.661
      JSQuAD: 0.898
      XLSum: 0.158
      MGSM: 0.188
      WMT20enja: 0.284
      WMT20jaen: 0.221
      JMMLU: 0.473
      JHumanEval: 0.161
    en_pre:
      avg: 0.418
      OpenBookQA: 0.34
      TriviaQA: 0.548
      HellaSwag: 0.562
      SQuAD2: 0.501
      XWINO: 0.896
      MMLU: 0.496
      GSM8K: 0.158
      MATH: 0.036
      BBH: 0.442
      HumanEval: 0.198
- id: sbintuitions/sarashina2-70b
  model_id: sbintuitions/sarashina2-70b
  name: Sarashina2-70B
  date: '2024-06-14'
  params: 70
  active_params: 70
  family: Sarashina2
  base: －
  sortkey: sarashina2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2-70b
  results:
    ja_pre:
      avg: 0.53
      JComQA: 0.929
      JEMHopQA: 0.717
      NIILC: 0.668
      JSQuAD: 0.929
      XLSum: 0.19
      MGSM: 0.488
      WMT20enja: 0.313
      WMT20jaen: 0.243
      JMMLU: 0.592
      JHumanEval: 0.235
    en_pre:
      avg: 0.491
      OpenBookQA: 0.388
      TriviaQA: 0.537
      HellaSwag: 0.628
      SQuAD2: 0.675
      XWINO: 0.917
      MMLU: 0.63
      GSM8K: 0.011
      MATH: 0.206
      BBH: 0.639
      HumanEval: 0.281
- id: sbintuitions/sarashina2.2-0.5b
  model_id: sbintuitions/sarashina2.2-0.5b
  name: Sarashina2.2 0.5B
  date: '2025-03-07'
  params: 0.8
  active_params: 0.8
  family: Sarashina2
  base: －
  sortkey: sarashina2.2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2.2-0.5b
  results:
    ja_pre:
      avg: 0.296
      JComQA: 0.211
      JEMHopQA: 0.472
      NIILC: 0.451
      JSQuAD: 0.824
      XLSum: 0.091
      MGSM: 0.196
      WMT20enja: 0.201
      WMT20jaen: 0.111
      JMMLU: 0.253
      JHumanEval: 0.148
    en_pre:
      avg: 0.339
      OpenBookQA: 0.302
      TriviaQA: 0.203
      HellaSwag: 0.42
      SQuAD2: 0.501
      XWINO: 0.794
      MMLU: 0.262
      GSM8K: 0.246
      MATH: 0.13
      BBH: 0.312
      HumanEval: 0.223
- id: sbintuitions/sarashina2.2-1b
  model_id: sbintuitions/sarashina2.2-1b
  name: Sarashina2.2 1B
  date: '2025-03-07'
  params: 1.4
  active_params: 1.4
  family: Sarashina2
  base: －
  sortkey: sarashina2.2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2.2-1b
  results:
    ja_pre:
      avg: 0.392
      JComQA: 0.649
      JEMHopQA: 0.462
      NIILC: 0.523
      JSQuAD: 0.858
      XLSum: 0.1
      MGSM: 0.388
      WMT20enja: 0.219
      WMT20jaen: 0.136
      JMMLU: 0.371
      JHumanEval: 0.215
    en_pre:
      avg: 0.415
      OpenBookQA: 0.324
      TriviaQA: 0.289
      HellaSwag: 0.469
      SQuAD2: 0.502
      XWINO: 0.827
      MMLU: 0.4
      GSM8K: 0.403
      MATH: 0.206
      BBH: 0.385
      HumanEval: 0.342
- id: sbintuitions/sarashina2.2-3b
  model_id: sbintuitions/sarashina2.2-3b
  name: Sarashina2.2 3B
  date: '2025-03-07'
  params: 3.4
  active_params: 3.4
  family: Sarashina2
  base: －
  sortkey: sarashina2.2
  is_post: false
  url: https://huggingface.co/sbintuitions/sarashina2.2-3b
  results:
    ja_pre:
      avg: 0.516
      JComQA: 0.911
      JEMHopQA: 0.563
      NIILC: 0.642
      JSQuAD: 0.906
      XLSum: 0.162
      MGSM: 0.596
      WMT20enja: 0.273
      WMT20jaen: 0.202
      JMMLU: 0.541
      JHumanEval: 0.36
    en_pre:
      avg: 0.532
      OpenBookQA: 0.362
      TriviaQA: 0.447
      HellaSwag: 0.538
      SQuAD2: 0.513
      XWINO: 0.877
      MMLU: 0.572
      GSM8K: 0.624
      MATH: 0.31
      BBH: 0.551
      HumanEval: 0.53
- id: sbintuitions/sarashina2.2-3b-instruct-v0.1
  model_id: sbintuitions/sarashina2.2-3b-instruct-v0.1
  name: Sarashina2.2 3B Instruct v0.1
  date: '2025-03-07'
  params: 3.4
  active_params: 3.4
  family: Sarashina2
  pre_training: Sarashina2.2 3B
  reasoning: N/A
  sortkey: sarashina2.2 v0.1
  is_post: true
  url: https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1
  results:
    ja_post:
      avg: 0.4
      JEMHopQA: 0.434
      MMLU-ProX: 0.335
      GPQA: 0.301
      MATH100: 0.465
      JHumanEval: 0.464
      __MIFEvalJa: 0.288
    en_post:
      avg: 0.318
      HellaSwag: 0.613
      MMLU-Pro: 0.329
      GPQA: 0.293
      MATH500: 0.57
      AIME: 0.017
      LCB: 0.086
    ja_mtb:
      avg: 0.721
      coding: 0.579
      extraction: 0.68
      humanities: 0.862
      math: 0.828
      reasoning: 0.467
      roleplay: 0.832
      stem: 0.766
      writing: 0.752
    en_mtb:
      avg: 0.708
      coding: 0.499
      extraction: 0.642
      humanities: 0.863
      math: 0.747
      reasoning: 0.552
      roleplay: 0.783
      stem: 0.827
      writing: 0.75
- id: SakanaAI/TinySwallow-1.5B
  model_id: SakanaAI/TinySwallow-1.5B
  name: TinySwallow-1.5B
  date: '2025-01-30'
  params: 1.5
  active_params: 1.5
  family: Qwen2.5
  base: Qwen2.5-1.5B-Instruct
  sortkey: tinyswallow
  is_post: false
  url: https://huggingface.co/SakanaAI/TinySwallow-1.5B
  results:
    ja_pre:
      avg: 0.402
      JComQA: 0.84
      JEMHopQA: 0.437
      NIILC: 0.474
      JSQuAD: 0.839
      XLSum: 0.173
      MGSM: 0.256
      WMT20enja: 0.201
      WMT20jaen: 0.125
      JMMLU: 0.446
      JHumanEval: 0.231
    en_pre:
      avg: 0.413
      OpenBookQA: 0.308
      TriviaQA: 0.332
      HellaSwag: 0.468
      SQuAD2: 0.501
      XWINO: 0.85
      MMLU: 0.546
      GSM8K: 0.379
      MATH: 0.162
      BBH: 0.328
      HumanEval: 0.254
