# Evaluation results in Swallow LLM Leaderboard
# Copyright (c) 2025 Swallow LLM team
# This file is distributed under Creative Commons Attribution 4.0 (CC-BY 4.0) License

- id: CohereForAI/aya-expanse-8b
  name: Aya Expanse 8B
  date: '2024-10-24'
  params: 8.0
  base_model: (private)
  sortkey: aya expanse
  url: https://huggingface.co/CohereForAI/aya-expanse-8b
  results:
    ja_basic:
      Ja Avg: 0.445
      JComQA: 0.922
      JEMHopQA: 0.467
      NIILC: 0.385
      JSQuAD: 0.867
      XL-Sum: 0.211
      MGSM: 0.608
      WMT20-en-ja: 0.261
      WMT20-ja-en: 0.206
      JMMLU: 0.521
      JHumanEval: 0.001
    ja_mtb:
      JMT Avg: 0.637
      coding: 0.494
      extraction: 0.718
      humanities: 0.855
      math: 0.398
      reasoning: 0.433
      roleplay: 0.737
      stem: 0.677
      writing: 0.787
    en_basic:
      En Avg: 0.539
      OpenBookQA: 0.384
      TriviaQA: 0.591
      HellaSwag: 0.605
      SQuAD2: 0.664
      XWINO: 0.892
      MMLU: 0.629
      GSM8K: 0.756
      MATH: 0.284
      BBH: 0.59
      HumanEval: 0.0
    other:
      GPQA: 0.259
- id: CohereForAI/aya-expanse-32b
  name: Aya Expanse 32B
  date: '2024-10-24'
  params: 32
  base_model: (private)
  sortkey: aya expanse
  url: https://huggingface.co/CohereForAI/aya-expanse-32b
  results:
    ja_basic:
      Ja Avg: 0.512
      JComQA: 0.965
      JEMHopQA: 0.554
      NIILC: 0.586
      JSQuAD: 0.812
      XL-Sum: 0.295
      MGSM: 0.716
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.245
      JMMLU: 0.655
      JHumanEval: 0.001
    ja_mtb:
      JMT Avg: 0.713
      coding: 0.548
      extraction: 0.72
      humanities: 0.846
      math: 0.657
      reasoning: 0.602
      roleplay: 0.824
      stem: 0.712
      writing: 0.794
    en_basic:
      En Avg: 0.614
      OpenBookQA: 0.42
      TriviaQA: 0.757
      HellaSwag: 0.668
      SQuAD2: 0.68
      XWINO: 0.912
      MMLU: 0.743
      GSM8K: 0.857
      MATH: 0.344
      BBH: 0.757
      HumanEval: 0.005
    other:
      GPQA: 0.152
- id: cyberagent/calm3-22b-chat
  name: CyberAgentLM3-22B-chat
  date: '2024-07-09'
  params: 22
  base_model: (private)
  sortkey: cyberagentlm3
  url: https://huggingface.co/cyberagent/calm3-22b-chat
  results:
    ja_basic:
      Ja Avg: 0.471
      JComQA: 0.934
      JEMHopQA: 0.51
      NIILC: 0.648
      JSQuAD: 0.911
      XL-Sum: 0.104
      MGSM: 0.576
      WMT20-en-ja: 0.275
      WMT20-ja-en: 0.215
      JMMLU: 0.541
      JHumanEval: 0.001
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.519
      extraction: 0.744
      humanities: 0.859
      math: 0.605
      reasoning: 0.548
      roleplay: 0.784
      stem: 0.7
      writing: 0.772
    en_basic:
      En Avg: 0.527
      OpenBookQA: 0.372
      TriviaQA: 0.619
      HellaSwag: 0.597
      SQuAD2: 0.603
      XWINO: 0.905
      MMLU: 0.602
      GSM8K: 0.697
      MATH: 0.274
      BBH: 0.599
      HumanEval: 0.0
    other:
      GPQA: 0.257
- id: tiiuae/Falcon3-1B-Base
  name: Falcon3-1B-Base
  date: '2024-12-19'
  params: 1.7
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-1B-Base
  results:
    ja_basic:
      Ja Avg: 0.129
      JComQA: 0.216
      JEMHopQA: 0.251
      NIILC: 0.062
      JSQuAD: 0.281
      XL-Sum: 0.085
      MGSM: 0.008
      WMT20-en-ja: 0.012
      WMT20-ja-en: 0.02
      JMMLU: 0.264
      JHumanEval: 0.088
    en_basic:
      En Avg: 0.376
      OpenBookQA: 0.316
      TriviaQA: 0.296
      HellaSwag: 0.458
      SQuAD2: 0.501
      XWINO: 0.816
      MMLU: 0.449
      GSM8K: 0.337
      MATH: 0.14
      BBH: 0.323
      HumanEval: 0.125
    other:
      GPQA: 0.025
- id: tiiuae/Falcon3-1B-Instruct
  name: Falcon3-1B-Instruct
  date: '2024-12-19'
  params: 1.7
  base_model: Falcon3-1B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-1B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.169
      JComQA: 0.24
      JEMHopQA: 0.312
      NIILC: 0.132
      JSQuAD: 0.454
      XL-Sum: 0.101
      MGSM: 0.02
      WMT20-en-ja: 0.028
      WMT20-ja-en: 0.032
      JMMLU: 0.281
      JHumanEval: 0.089
    ja_mtb:
      JMT Avg: 0.161
      coding: 0.176
      extraction: 0.178
      humanities: 0.121
      math: 0.161
      reasoning: 0.224
      roleplay: 0.154
      stem: 0.124
      writing: 0.148
    en_basic:
      En Avg: 0.381
      OpenBookQA: 0.344
      TriviaQA: 0.261
      HellaSwag: 0.48
      SQuAD2: 0.501
      XWINO: 0.815
      MMLU: 0.459
      GSM8K: 0.391
      MATH: 0.13
      BBH: 0.33
      HumanEval: 0.101
    other:
      GPQA: 0.013
- id: tiiuae/Falcon3-3B-Base
  name: Falcon3-3B-Base
  date: '2024-12-19'
  params: 3.2
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-3B-Base
  results:
    ja_basic:
      Ja Avg: 0.209
      JComQA: 0.281
      JEMHopQA: 0.333
      NIILC: 0.113
      JSQuAD: 0.517
      XL-Sum: 0.12
      MGSM: 0.096
      WMT20-en-ja: 0.031
      WMT20-ja-en: 0.051
      JMMLU: 0.319
      JHumanEval: 0.229
    en_basic:
      En Avg: 0.495
      OpenBookQA: 0.312
      TriviaQA: 0.346
      HellaSwag: 0.492
      SQuAD2: 0.503
      XWINO: 0.847
      MMLU: 0.567
      GSM8K: 0.634
      MATH: 0.344
      BBH: 0.553
      HumanEval: 0.348
    other:
      GPQA: 0.179
- id: tiiuae/Falcon3-3B-Instruct
  name: Falcon3-3B-Instruct
  date: '2024-12-19'
  params: 3.2
  base_model: Falcon3-3B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.232
      JComQA: 0.421
      JEMHopQA: 0.16
      NIILC: 0.113
      JSQuAD: 0.632
      XL-Sum: 0.141
      MGSM: 0.092
      WMT20-en-ja: 0.062
      WMT20-ja-en: 0.058
      JMMLU: 0.331
      JHumanEval: 0.308
    ja_mtb:
      JMT Avg: 0.26
      coding: 0.329
      extraction: 0.392
      humanities: 0.219
      math: 0.199
      reasoning: 0.267
      roleplay: 0.234
      stem: 0.229
      writing: 0.208
    en_basic:
      En Avg: 0.526
      OpenBookQA: 0.372
      TriviaQA: 0.286
      HellaSwag: 0.541
      SQuAD2: 0.513
      XWINO: 0.818
      MMLU: 0.562
      GSM8K: 0.712
      MATH: 0.44
      BBH: 0.562
      HumanEval: 0.454
    other:
      GPQA: 0.208
- id: tiiuae/Falcon3-7B-Base
  name: Falcon3-7B-Base
  date: '2024-12-19'
  params: 7.5
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-7B-Base
  results:
    ja_basic:
      Ja Avg: 0.337
      JComQA: 0.634
      JEMHopQA: 0.412
      NIILC: 0.18
      JSQuAD: 0.788
      XL-Sum: 0.173
      MGSM: 0.244
      WMT20-en-ja: 0.078
      WMT20-ja-en: 0.119
      JMMLU: 0.385
      JHumanEval: 0.361
    en_basic:
      En Avg: 0.596
      OpenBookQA: 0.354
      TriviaQA: 0.552
      HellaSwag: 0.566
      SQuAD2: 0.539
      XWINO: 0.881
      MMLU: 0.701
      GSM8K: 0.766
      MATH: 0.438
      BBH: 0.692
      HumanEval: 0.476
    other:
      GPQA: 0.252
- id: tiiuae/Falcon3-7B-Instruct
  name: Falcon3-7B-Instruct
  date: '2024-12-19'
  params: 7.5
  base_model: Falcon3-7B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.364
      JComQA: 0.684
      JEMHopQA: 0.436
      NIILC: 0.152
      JSQuAD: 0.816
      XL-Sum: 0.177
      MGSM: 0.32
      WMT20-en-ja: 0.094
      WMT20-ja-en: 0.126
      JMMLU: 0.415
      JHumanEval: 0.416
    ja_mtb:
      JMT Avg: 0.377
      coding: 0.549
      extraction: 0.506
      humanities: 0.34
      math: 0.406
      reasoning: 0.257
      roleplay: 0.299
      stem: 0.34
      writing: 0.317
    en_basic:
      En Avg: 0.618
      OpenBookQA: 0.394
      TriviaQA: 0.517
      HellaSwag: 0.611
      SQuAD2: 0.525
      XWINO: 0.855
      MMLU: 0.705
      GSM8K: 0.773
      MATH: 0.542
      BBH: 0.711
      HumanEval: 0.551
    other:
      GPQA: 0.277
- id: tiiuae/Falcon3-10B-Base
  name: Falcon3-10B-Base
  date: '2024-12-19'
  params: 10
  base_model: ''
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-10B-Base
  results:
    ja_basic:
      Ja Avg: 0.383
      JComQA: 0.68
      JEMHopQA: 0.443
      NIILC: 0.187
      JSQuAD: 0.854
      XL-Sum: 0.187
      MGSM: 0.376
      WMT20-en-ja: 0.103
      WMT20-ja-en: 0.139
      JMMLU: 0.435
      JHumanEval: 0.426
    en_basic:
      En Avg: 0.639
      OpenBookQA: 0.368
      TriviaQA: 0.579
      HellaSwag: 0.596
      SQuAD2: 0.603
      XWINO: 0.901
      MMLU: 0.732
      GSM8K: 0.802
      MATH: 0.492
      BBH: 0.776
      HumanEval: 0.543
    other:
      GPQA: 0.259
- id: tiiuae/Falcon3-10B-Instruct
  name: Falcon3-10B-Instruct
  date: '2024-12-19'
  params: 10
  base_model: Falcon3-10B-Base
  sortkey: falcon3
  url: https://huggingface.co/tiiuae/Falcon3-10B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.367
      JComQA: 0.69
      JEMHopQA: 0.221
      NIILC: 0.122
      JSQuAD: 0.853
      XL-Sum: 0.192
      MGSM: 0.392
      WMT20-en-ja: 0.108
      WMT20-ja-en: 0.135
      JMMLU: 0.442
      JHumanEval: 0.515
    ja_mtb:
      JMT Avg: 0.413
      coding: 0.509
      extraction: 0.545
      humanities: 0.382
      math: 0.48
      reasoning: 0.356
      roleplay: 0.335
      stem: 0.373
      writing: 0.324
    en_basic:
      En Avg: 0.633
      OpenBookQA: 0.424
      TriviaQA: 0.504
      HellaSwag: 0.64
      SQuAD2: 0.549
      XWINO: 0.875
      MMLU: 0.73
      GSM8K: 0.793
      MATH: 0.462
      BBH: 0.729
      HumanEval: 0.627
    other:
      GPQA: 0.335
- id: google/gemma-2-2b
  name: Gemma 2 2B
  date: '2024-06-27'
  params: 2.6
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-2b
  results:
    ja_basic:
      Ja Avg: 0.348
      JComQA: 0.721
      JEMHopQA: 0.472
      NIILC: 0.316
      JSQuAD: 0.81
      XL-Sum: 0.083
      MGSM: 0.124
      WMT20-en-ja: 0.203
      WMT20-ja-en: 0.19
      JMMLU: 0.388
      JHumanEval: 0.177
    en_basic:
      En Avg: 0.439
      OpenBookQA: 0.342
      TriviaQA: 0.552
      HellaSwag: 0.552
      SQuAD2: 0.501
      XWINO: 0.89
      MMLU: 0.53
      GSM8K: 0.249
      MATH: 0.176
      BBH: 0.415
      HumanEval: 0.188
    other:
      GPQA: 0.049
- id: google/gemma-2-2b-it
  name: Gemma 2 2B IT
  date: '2024-06-27'
  params: 2.6
  base_model: Gemma 2 2B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-2b-it
  results:
    ja_basic:
      Ja Avg: 0.392
      JComQA: 0.862
      JEMHopQA: 0.348
      NIILC: 0.315
      JSQuAD: 0.879
      XL-Sum: 0.117
      MGSM: 0.252
      WMT20-en-ja: 0.207
      WMT20-ja-en: 0.183
      JMMLU: 0.437
      JHumanEval: 0.321
    ja_mtb:
      JMT Avg: 0.569
      coding: 0.454
      extraction: 0.587
      humanities: 0.693
      math: 0.524
      reasoning: 0.445
      roleplay: 0.654
      stem: 0.567
      writing: 0.63
    en_basic:
      En Avg: 0.489
      OpenBookQA: 0.354
      TriviaQA: 0.502
      HellaSwag: 0.52
      SQuAD2: 0.548
      XWINO: 0.878
      MMLU: 0.569
      GSM8K: 0.44
      MATH: 0.23
      BBH: 0.464
      HumanEval: 0.382
    other:
      GPQA: 0.08
- id: google/gemma-2-9b
  name: Gemma 2 9B
  date: '2024-06-27'
  params: 9.2
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-9b
  results:
    ja_basic:
      Ja Avg: 0.5
      JComQA: 0.904
      JEMHopQA: 0.573
      NIILC: 0.524
      JSQuAD: 0.898
      XL-Sum: 0.168
      MGSM: 0.456
      WMT20-en-ja: 0.269
      WMT20-ja-en: 0.236
      JMMLU: 0.623
      JHumanEval: 0.345
    en_basic:
      En Avg: 0.597
      OpenBookQA: 0.382
      TriviaQA: 0.718
      HellaSwag: 0.626
      SQuAD2: 0.506
      XWINO: 0.907
      MMLU: 0.706
      GSM8K: 0.688
      MATH: 0.338
      BBH: 0.704
      HumanEval: 0.39
    other:
      GPQA: 0.221
- id: google/gemma-2-9b-it
  name: Gemma 2 9B IT
  date: '2024-06-27'
  params: 9.2
  base_model: Gemma 2 9B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-9b-it
  results:
    ja_basic:
      Ja Avg: 0.535
      JComQA: 0.931
      JEMHopQA: 0.532
      NIILC: 0.527
      JSQuAD: 0.876
      XL-Sum: 0.149
      MGSM: 0.636
      WMT20-en-ja: 0.273
      WMT20-ja-en: 0.239
      JMMLU: 0.623
      JHumanEval: 0.559
    ja_mtb:
      JMT Avg: 0.736
      coding: 0.652
      extraction: 0.765
      humanities: 0.857
      math: 0.614
      reasoning: 0.673
      roleplay: 0.811
      stem: 0.713
      writing: 0.8
    en_basic:
      En Avg: 0.649
      OpenBookQA: 0.432
      TriviaQA: 0.658
      HellaSwag: 0.605
      SQuAD2: 0.659
      XWINO: 0.904
      MMLU: 0.723
      GSM8K: 0.779
      MATH: 0.394
      BBH: 0.719
      HumanEval: 0.613
    other:
      GPQA: 0.065
- id: google/gemma-2-27b
  name: Gemma 2 27B
  date: '2024-06-27'
  params: 27
  base_model: ''
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-27b
  results:
    ja_basic:
      Ja Avg: 0.546
      JComQA: 0.936
      JEMHopQA: 0.553
      NIILC: 0.573
      JSQuAD: 0.916
      XL-Sum: 0.194
      MGSM: 0.596
      WMT20-en-ja: 0.295
      WMT20-ja-en: 0.251
      JMMLU: 0.659
      JHumanEval: 0.49
    en_basic:
      En Avg: 0.655
      OpenBookQA: 0.412
      TriviaQA: 0.78
      HellaSwag: 0.675
      SQuAD2: 0.549
      XWINO: 0.921
      MMLU: 0.754
      GSM8K: 0.757
      MATH: 0.438
      BBH: 0.76
      HumanEval: 0.508
    other:
      GPQA: 0.125
- id: google/gemma-2-27b-it
  name: Gemma 2 27B IT
  date: '2024-06-27'
  params: 27
  base_model: Gemma 2 27B
  sortkey: gemma 2
  url: https://huggingface.co/google/gemma-2-27b-it
  results:
    ja_basic:
      Ja Avg: 0.567
      JComQA: 0.956
      JEMHopQA: 0.541
      NIILC: 0.576
      JSQuAD: 0.883
      XL-Sum: 0.166
      MGSM: 0.704
      WMT20-en-ja: 0.29
      WMT20-ja-en: 0.249
      JMMLU: 0.67
      JHumanEval: 0.638
    ja_mtb:
      JMT Avg: 0.768
      coding: 0.727
      extraction: 0.809
      humanities: 0.874
      math: 0.719
      reasoning: 0.639
      roleplay: 0.81
      stem: 0.74
      writing: 0.826
    en_basic:
      En Avg: 0.703
      OpenBookQA: 0.458
      TriviaQA: 0.766
      HellaSwag: 0.655
      SQuAD2: 0.669
      XWINO: 0.909
      MMLU: 0.762
      GSM8K: 0.851
      MATH: 0.466
      BBH: 0.79
      HumanEval: 0.707
    other:
      GPQA: 0.087
- id: rinna/gemma-2-baku-2b
  name: Gemma 2 Baku 2B
  date: '2024-10-03'
  params: 2.6
  base_model: ''
  sortkey: gemma 2 baku
  url: https://huggingface.co/rinna/gemma-2-baku-2b
  results:
    ja_basic:
      Ja Avg: 0.372
      JComQA: 0.76
      JEMHopQA: 0.475
      NIILC: 0.443
      JSQuAD: 0.843
      XL-Sum: 0.121
      MGSM: 0.124
      WMT20-en-ja: 0.255
      WMT20-ja-en: 0.187
      JMMLU: 0.376
      JHumanEval: 0.137
    en_basic:
      En Avg: 0.4
      OpenBookQA: 0.314
      TriviaQA: 0.475
      HellaSwag: 0.533
      SQuAD2: 0.501
      XWINO: 0.881
      MMLU: 0.493
      GSM8K: 0.168
      MATH: 0.11
      BBH: 0.376
      HumanEval: 0.15
    other:
      GPQA: 0.029
- id: rinna/gemma-2-baku-2b-it
  name: Gemma 2 Baku 2B IT
  date: '2024-10-03'
  params: 2.6
  base_model: Gemma 2 Baku 2B
  sortkey: gemma 2 baku
  url: https://huggingface.co/rinna/gemma-2-baku-2b-it
  results:
    ja_basic:
      Ja Avg: 0.366
      JComQA: 0.855
      JEMHopQA: 0.228
      NIILC: 0.39
      JSQuAD: 0.877
      XL-Sum: 0.115
      MGSM: 0.172
      WMT20-en-ja: 0.255
      WMT20-ja-en: 0.19
      JMMLU: 0.415
      JHumanEval: 0.165
    ja_mtb:
      JMT Avg: 0.59
      coding: 0.47
      extraction: 0.625
      humanities: 0.81
      math: 0.414
      reasoning: 0.382
      roleplay: 0.713
      stem: 0.609
      writing: 0.697
    en_basic:
      En Avg: 0.361
      OpenBookQA: 0.342
      TriviaQA: 0.416
      HellaSwag: 0.511
      SQuAD2: 0.522
      XWINO: 0.871
      MMLU: 0.526
      GSM8K: 0.027
      MATH: 0.174
      BBH: 0.063
      HumanEval: 0.158
    other:
      GPQA: 0.051
- id: google/gemma-2-2b-jpn-it
  name: Gemma 2 JPN
  date: '2024-06-27'
  params: 2.6
  base_model: Gemma 2 2B
  sortkey: gemma 2 jpn
  url: https://huggingface.co/google/gemma-2-2b-jpn-it
  results:
    ja_basic:
      Ja Avg: 0.377
      JComQA: 0.845
      JEMHopQA: 0.321
      NIILC: 0.291
      JSQuAD: 0.877
      XL-Sum: 0.131
      MGSM: 0.192
      WMT20-en-ja: 0.204
      WMT20-ja-en: 0.18
      JMMLU: 0.418
      JHumanEval: 0.311
    ja_mtb:
      JMT Avg: 0.55
      coding: 0.467
      extraction: 0.488
      humanities: 0.741
      math: 0.379
      reasoning: 0.406
      roleplay: 0.66
      stem: 0.589
      writing: 0.672
    en_basic:
      En Avg: 0.471
      OpenBookQA: 0.37
      TriviaQA: 0.503
      HellaSwag: 0.532
      SQuAD2: 0.539
      XWINO: 0.879
      MMLU: 0.557
      GSM8K: 0.351
      MATH: 0.132
      BBH: 0.451
      HumanEval: 0.392
    other:
      GPQA: 0.033
- id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1
  name: Gemma-2-Llama Swallow 2B IT
  date: ''
  params: 2.6
  base_model: Gemma2-Llama Swallow 2B
  sortkey: gemma 2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1
  results:
    ja_basic:
      Ja Avg: 0.424
      JComQA: 0.862
      JEMHopQA: 0.367
      NIILC: 0.483
      JSQuAD: 0.881
      XL-Sum: 0.145
      MGSM: 0.288
      WMT20-en-ja: 0.258
      WMT20-ja-en: 0.2
      JMMLU: 0.485
      JHumanEval: 0.267
    ja_mtb:
      JMT Avg: 0.597
      coding: 0.438
      extraction: 0.533
      humanities: 0.781
      math: 0.557
      reasoning: 0.404
      roleplay: 0.706
      stem: 0.674
      writing: 0.682
    en_basic:
      En Avg: 0.431
      OpenBookQA: 0.332
      TriviaQA: 0.417
      HellaSwag: 0.529
      SQuAD2: 0.506
      XWINO: 0.856
      MMLU: 0.53
      GSM8K: 0.284
      MATH: 0.15
      BBH: 0.405
      HumanEval: 0.301
    other:
      GPQA: 0.141
- id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1
  name: Gemma-2-Llama Swallow 9B IT
  date: ''
  params: 9.2
  base_model: Gemma 2 9B
  sortkey: gemma 2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1
  results:
    ja_basic:
      Ja Avg: 0.546
      JComQA: 0.946
      JEMHopQA: 0.606
      NIILC: 0.643
      JSQuAD: 0.852
      XL-Sum: 0.17
      MGSM: 0.624
      WMT20-en-ja: 0.296
      WMT20-ja-en: 0.238
      JMMLU: 0.639
      JHumanEval: 0.446
    ja_mtb:
      JMT Avg: 0.749
      coding: 0.592
      extraction: 0.796
      humanities: 0.872
      math: 0.742
      reasoning: 0.638
      roleplay: 0.802
      stem: 0.745
      writing: 0.803
    en_basic:
      En Avg: 0.611
      OpenBookQA: 0.404
      TriviaQA: 0.64
      HellaSwag: 0.609
      SQuAD2: 0.623
      XWINO: 0.9
      MMLU: 0.68
      GSM8K: 0.71
      MATH: 0.392
      BBH: 0.663
      HumanEval: 0.491
    other:
      GPQA: 0.08
- id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1
  name: Gemma-2-Llama Swallow 27B IT
  date: ''
  params: 27
  base_model: Gemma 2 27B
  sortkey: gemma 2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1
  results:
    ja_basic:
      Ja Avg: 0.602
      JComQA: 0.969
      JEMHopQA: 0.654
      NIILC: 0.658
      JSQuAD: 0.891
      XL-Sum: 0.194
      MGSM: 0.764
      WMT20-en-ja: 0.316
      WMT20-ja-en: 0.258
      JMMLU: 0.686
      JHumanEval: 0.635
    ja_mtb:
      JMT Avg: 0.759
      coding: 0.618
      extraction: 0.839
      humanities: 0.873
      math: 0.741
      reasoning: 0.608
      roleplay: 0.814
      stem: 0.739
      writing: 0.836
    en_basic:
      En Avg: 0.687
      OpenBookQA: 0.424
      TriviaQA: 0.747
      HellaSwag: 0.663
      SQuAD2: 0.664
      XWINO: 0.911
      MMLU: 0.749
      GSM8K: 0.821
      MATH: 0.442
      BBH: 0.772
      HumanEval: 0.682
    other:
      GPQA: 0.259
- id: google/gemma-3-1b-pt
  name: Gemma 3 1B
  date: '2025-03-12'
  params: 1
  base_model: ''
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-1b-pt
  results:
    ja_basic:
      Ja Avg: 0.223
      JComQA: 0.237
      JEMHopQA: 0.41
      NIILC: 0.252
      JSQuAD: 0.631
      XL-Sum: 0.079
      MGSM: 0.024
      WMT20-en-ja: 0.15
      WMT20-ja-en: 0.136
      JMMLU: 0.239
      JHumanEval: 0.073
    en_basic:
      En Avg: 0.31
      OpenBookQA: 0.304
      TriviaQA: 0.358
      HellaSwag: 0.471
      SQuAD2: 0.501
      XWINO: 0.832
      MMLU: 0.262
      GSM8K: 0.016
      MATH: 0.008
      BBH: 0.276
      HumanEval: 0.07
    other:
      GPQA: 0.02
- id: google/gemma-3-1b-it
  name: Gemma 3 1B IT
  date: '2025-03-12'
  params: 1
  base_model: Gemma 3 1B
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-1b-it
  results:
    ja_basic:
      Ja Avg: 0.285
      JComQA: 0.526
      JEMHopQA: 0.33
      NIILC: 0.237
      JSQuAD: 0.7
      XL-Sum: 0.113
      MGSM: 0.088
      WMT20-en-ja: 0.166
      WMT20-ja-en: 0.115
      JMMLU: 0.332
      JHumanEval: 0.245
    ja_mtb:
      JMT Avg: 0.51
      coding: 0.379
      extraction: 0.497
      humanities: 0.68
      math: 0.385
      reasoning: 0.322
      roleplay: 0.628
      stem: 0.54
      writing: 0.651
    en_basic:
      En Avg: 0.392
      OpenBookQA: 0.272
      TriviaQA: 0.229
      HellaSwag: 0.421
      SQuAD2: 0.501
      XWINO: 0.786
      MMLU: 0.398
      GSM8K: 0.256
      MATH: 0.34
      BBH: 0.379
      HumanEval: 0.335
    other:
      GPQA: 0.167
- id: google/gemma-3-4b-pt
  name: Gemma 3 4B
  date: '2025-03-12'
  params: 4.3
  base_model: ''
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-4b-pt
  results:
    ja_basic:
      Ja Avg: 0.417
      JComQA: 0.851
      JEMHopQA: 0.432
      NIILC: 0.41
      JSQuAD: 0.887
      XL-Sum: 0.139
      MGSM: 0.248
      WMT20-en-ja: 0.23
      WMT20-ja-en: 0.205
      JMMLU: 0.499
      JHumanEval: 0.273
    en_basic:
      En Avg: 0.501
      OpenBookQA: 0.36
      TriviaQA: 0.603
      HellaSwag: 0.576
      SQuAD2: 0.502
      XWINO: 0.895
      MMLU: 0.596
      GSM8K: 0.376
      MATH: 0.258
      BBH: 0.495
      HumanEval: 0.351
    other:
      GPQA: 0.156
- id: google/gemma-3-4b-it
  name: Gemma 3 4B IT
  date: '2025-03-12'
  params: 4.3
  base_model: Gemma 3 4B
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-4b-it
  results:
    ja_basic:
      Ja Avg: 0.416
      JComQA: 0.818
      JEMHopQA: 0.444
      NIILC: 0.404
      JSQuAD: 0.801
      XL-Sum: 0.134
      MGSM: 0.332
      WMT20-en-ja: 0.217
      WMT20-ja-en: 0.169
      JMMLU: 0.477
      JHumanEval: 0.365
    ja_mtb:
      JMT Avg: 0.724
      coding: 0.603
      extraction: 0.724
      humanities: 0.798
      math: 0.767
      reasoning: 0.498
      roleplay: 0.803
      stem: 0.775
      writing: 0.822
    en_basic:
      En Avg: 0.566
      OpenBookQA: 0.412
      TriviaQA: 0.5
      HellaSwag: 0.56
      SQuAD2: 0.552
      XWINO: 0.872
      MMLU: 0.583
      GSM8K: 0.769
      MATH: 0.306
      BBH: 0.598
      HumanEval: 0.513
    other:
      GPQA: 0.25
- id: google/gemma-3-12b-pt
  name: Gemma 3 12B
  date: '2025-03-12'
  params: 12
  base_model: ''
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-12b-pt
  results:
    ja_basic:
      Ja Avg: 0.518
      JComQA: 0.787
      JEMHopQA: 0.563
      NIILC: 0.569
      JSQuAD: 0.911
      XL-Sum: 0.194
      MGSM: 0.584
      WMT20-en-ja: 0.288
      WMT20-ja-en: 0.244
      JMMLU: 0.659
      JHumanEval: 0.385
    en_basic:
      En Avg: 0.619
      OpenBookQA: 0.398
      TriviaQA: 0.747
      HellaSwag: 0.637
      SQuAD2: 0.524
      XWINO: 0.917
      MMLU: 0.737
      GSM8K: 0.703
      MATH: 0.398
      BBH: 0.683
      HumanEval: 0.445
    other:
      GPQA: 0.004
- id: google/gemma-3-12b-it
  name: Gemma 3 12B IT
  date: '2025-03-12'
  params: 12
  base_model: Gemma 3 12B
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-12b-it
  results:
    ja_basic:
      Ja Avg: 0.553
      JComQA: 0.935
      JEMHopQA: 0.566
      NIILC: 0.542
      JSQuAD: 0.808
      XL-Sum: 0.148
      MGSM: 0.724
      WMT20-en-ja: 0.289
      WMT20-ja-en: 0.239
      JMMLU: 0.645
      JHumanEval: 0.637
    ja_mtb:
      JMT Avg: 0.821
      coding: 0.807
      extraction: 0.814
      humanities: 0.871
      math: 0.886
      reasoning: 0.623
      roleplay: 0.847
      stem: 0.858
      writing: 0.863
    en_basic:
      En Avg: 0.717
      OpenBookQA: 0.422
      TriviaQA: 0.665
      HellaSwag: 0.639
      SQuAD2: 0.649
      XWINO: 0.901
      MMLU: 0.721
      GSM8K: 0.867
      MATH: 0.796
      BBH: 0.802
      HumanEval: 0.712
    other:
      GPQA: 0.315
- id: google/gemma-3-27b-pt
  name: Gemma 3 27B
  date: '2025-03-12'
  params: 27
  base_model: ''
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-27b-pt
  results:
    ja_basic:
      Ja Avg: 0.574
      JComQA: 0.944
      JEMHopQA: 0.582
      NIILC: 0.627
      JSQuAD: 0.915
      XL-Sum: 0.21
      MGSM: 0.704
      WMT20-en-ja: 0.301
      WMT20-ja-en: 0.255
      JMMLU: 0.724
      JHumanEval: 0.473
    en_basic:
      En Avg: 0.677
      OpenBookQA: 0.414
      TriviaQA: 0.809
      HellaSwag: 0.667
      SQuAD2: 0.618
      XWINO: 0.923
      MMLU: 0.78
      GSM8K: 0.801
      MATH: 0.52
      BBH: 0.732
      HumanEval: 0.507
    other:
      GPQA: 0.022
- id: google/gemma-3-27b-it
  name: Gemma 3 27B IT
  date: '2025-03-12'
  params: 27
  base_model: Gemma 3 27B
  sortkey: gemma 3
  url: https://huggingface.co/google/gemma-3-27b-it
  results:
    ja_basic:
      Ja Avg: 0.591
      JComQA: 0.946
      JEMHopQA: 0.592
      NIILC: 0.584
      JSQuAD: 0.867
      XL-Sum: 0.142
      MGSM: 0.764
      WMT20-en-ja: 0.307
      WMT20-ja-en: 0.253
      JMMLU: 0.716
      JHumanEval: 0.736
    ja_mtb:
      JMT Avg: 0.855
      coding: 0.804
      extraction: 0.927
      humanities: 0.879
      math: 0.876
      reasoning: 0.774
      roleplay: 0.846
      stem: 0.848
      writing: 0.882
    en_basic:
      En Avg: 0.758
      OpenBookQA: 0.418
      TriviaQA: 0.744
      HellaSwag: 0.661
      SQuAD2: 0.687
      XWINO: 0.906
      MMLU: 0.774
      GSM8K: 0.916
      MATH: 0.852
      BBH: 0.793
      HumanEval: 0.829
    other:
      GPQA: 0.444
- id: tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1
  name: Gemma2-Llama Swallow 2B
  date: ''
  params: 2.6
  base_model: ''
  sortkey: gemma2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-pt-v0.1
  results:
    ja_basic:
      Ja Avg: 0.421
      JComQA: 0.83
      JEMHopQA: 0.509
      NIILC: 0.549
      JSQuAD: 0.863
      XL-Sum: 0.119
      MGSM: 0.172
      WMT20-en-ja: 0.261
      WMT20-ja-en: 0.195
      JMMLU: 0.461
      JHumanEval: 0.251
    en_basic:
      En Avg: 0.426
      OpenBookQA: 0.312
      TriviaQA: 0.435
      HellaSwag: 0.516
      SQuAD2: 0.501
      XWINO: 0.871
      MMLU: 0.538
      GSM8K: 0.275
      MATH: 0.144
      BBH: 0.384
      HumanEval: 0.286
    other:
      GPQA: 0.051
- id: tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1
  name: Gemma2-Llama Swallow 9B
  date: ''
  params: 9.2
  base_model: ''
  sortkey: gemma2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-pt-v0.1
  results:
    ja_basic:
      Ja Avg: 0.558
      JComQA: 0.95
      JEMHopQA: 0.643
      NIILC: 0.677
      JSQuAD: 0.897
      XL-Sum: 0.187
      MGSM: 0.56
      WMT20-en-ja: 0.304
      WMT20-ja-en: 0.247
      JMMLU: 0.65
      JHumanEval: 0.462
    en_basic:
      En Avg: 0.595
      OpenBookQA: 0.362
      TriviaQA: 0.659
      HellaSwag: 0.602
      SQuAD2: 0.532
      XWINO: 0.906
      MMLU: 0.687
      GSM8K: 0.678
      MATH: 0.33
      BBH: 0.664
      HumanEval: 0.529
    other:
      GPQA: 0.232
- id: tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1
  name: Gemma2-Llama Swallow 27B
  date: ''
  params: 27
  base_model: ''
  sortkey: gemma2 llama swallow
  url: https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-pt-v0.1
  results:
    ja_basic:
      Ja Avg: 0.594
      JComQA: 0.958
      JEMHopQA: 0.66
      NIILC: 0.671
      JSQuAD: 0.924
      XL-Sum: 0.2
      MGSM: 0.644
      WMT20-en-ja: 0.321
      WMT20-ja-en: 0.255
      JMMLU: 0.679
      JHumanEval: 0.629
    en_basic:
      En Avg: 0.665
      OpenBookQA: 0.414
      TriviaQA: 0.756
      HellaSwag: 0.652
      SQuAD2: 0.597
      XWINO: 0.915
      MMLU: 0.749
      GSM8K: 0.732
      MATH: 0.416
      BBH: 0.765
      HumanEval: 0.658
    other:
      GPQA: 0.279
- id: gpt-3.5-turbo-0125
  name: GPT-3.5 (gpt-3.5-turbo-0125)
  date: '2024-01-25'
  params: 0
  base_model: (private)
  sortkey: gpt 3.5 (gpt 3.5 turbo 0125)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.515
      JComQA: 0.922
      JEMHopQA: 0.456
      NIILC: 0.447
      JSQuAD: 0.893
      XL-Sum: 0.215
      MGSM: 0.572
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.243
      JMMLU: 0.499
      JHumanEval: 0.616
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.693
      extraction: 0.789
      humanities: 0.773
      math: 0.665
      reasoning: 0.462
      roleplay: 0.728
      stem: 0.644
      writing: 0.775
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.29
- id: gpt-4-0613
  name: GPT-4 (gpt-4-0613)
  date: '2023-06-13'
  params: 0
  base_model: (private)
  sortkey: gpt 4 (gpt 4 0613)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.621
      JComQA: 0.966
      JEMHopQA: 0.651
      NIILC: 0.616
      JSQuAD: 0.927
      XL-Sum: 0.22
      MGSM: 0.832
      WMT20-en-ja: 0.286
      WMT20-ja-en: 0.263
      JMMLU: 0.768
      JHumanEval: 0.68
    ja_mtb:
      JMT Avg: 0.771
      coding: 0.645
      extraction: 0.913
      humanities: 0.838
      math: 0.698
      reasoning: 0.666
      roleplay: 0.81
      stem: 0.764
      writing: 0.834
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: -0.001
- id: gpt-4-turbo-2024-04-09
  name: GPT-4-turbo (gpt-4-turbo-2024-04-09)
  date: '2024-04-09'
  params: 0
  base_model: (private)
  sortkey: gpt 4 turbo (gpt 4 turbo 2024 04 09)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.626
      JComQA: 0.971
      JEMHopQA: 0.69
      NIILC: 0.615
      JSQuAD: 0.878
      XL-Sum: 0.201
      MGSM: 0.848
      WMT20-en-ja: 0.295
      WMT20-ja-en: 0.239
      JMMLU: 0.753
      JHumanEval: 0.773
    ja_mtb:
      JMT Avg: 0.837
      coding: 0.842
      extraction: 0.891
      humanities: 0.863
      math: 0.865
      reasoning: 0.673
      roleplay: 0.861
      stem: 0.844
      writing: 0.854
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.304
- id: gpt-4.5-preview-2025-02-27
  name: GPT-4.5 (gpt-4.5-preview-2025-02-27)
  date: '2025-02-27'
  params: 0
  base_model: (private)
  sortkey: gpt 4.5 (gpt 4.5 preview 2025 02 27)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: -0.01
      JComQA: -0.01
      JEMHopQA: -0.01
      NIILC: -0.01
      JSQuAD: -0.01
      XL-Sum: -0.01
      MGSM: -0.01
      WMT20-en-ja: -0.01
      WMT20-ja-en: -0.01
      JMMLU: -0.01
      JHumanEval: -0.01
    ja_mtb:
      JMT Avg: 0.884
      coding: 0.859
      extraction: 0.93
      humanities: 0.898
      math: 0.98
      reasoning: 0.747
      roleplay: 0.859
      stem: 0.906
      writing: 0.893
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: -0.001
- id: gpt-4o-2024-05-13
  name: GPT-4o (gpt-4o-2024-05-13)
  date: '2024-05-13'
  params: 0
  base_model: (private)
  sortkey: gpt 4o (gpt 4o 2024 05 13)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.649
      JComQA: 0.979
      JEMHopQA: 0.737
      NIILC: 0.722
      JSQuAD: 0.892
      XL-Sum: 0.14
      MGSM: 0.86
      WMT20-en-ja: 0.314
      WMT20-ja-en: 0.237
      JMMLU: 0.794
      JHumanEval: 0.813
    ja_mtb:
      JMT Avg: 0.848
      coding: 0.859
      extraction: 0.93
      humanities: 0.882
      math: 0.917
      reasoning: 0.631
      roleplay: 0.858
      stem: 0.858
      writing: 0.851
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.306
- id: gpt-4o-2024-08-06
  name: GPT-4o (gpt-4o-2024-08-06)
  date: '2024-08-06'
  params: 0
  base_model: (private)
  sortkey: gpt 4o (gpt 4o 2024 08 06)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.646
      JComQA: 0.982
      JEMHopQA: 0.731
      NIILC: 0.709
      JSQuAD: 0.889
      XL-Sum: 0.17
      MGSM: 0.864
      WMT20-en-ja: 0.314
      WMT20-ja-en: 0.254
      JMMLU: 0.797
      JHumanEval: 0.752
    ja_mtb:
      JMT Avg: 0.848
      coding: 0.855
      extraction: 0.926
      humanities: 0.88
      math: 0.872
      reasoning: 0.706
      roleplay: 0.862
      stem: 0.838
      writing: 0.849
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.317
- id: gpt-4o-mini-2024-07-18
  name: GPT-4o-mini (gpt-4o-mini-2024-07-18)
  date: '2024-08-06'
  params: 0
  base_model: (private)
  sortkey: gpt 4o mini (gpt 4o mini 2024 07 18)
  url: https://platform.openai.com/docs/models
  results:
    ja_basic:
      Ja Avg: 0.58
      JComQA: 0.961
      JEMHopQA: 0.464
      NIILC: 0.591
      JSQuAD: 0.902
      XL-Sum: 0.16
      MGSM: 0.832
      WMT20-en-ja: 0.299
      WMT20-ja-en: 0.241
      JMMLU: 0.679
      JHumanEval: 0.675
    ja_mtb:
      JMT Avg: 0.824
      coding: 0.825
      extraction: 0.865
      humanities: 0.857
      math: 0.843
      reasoning: 0.665
      roleplay: 0.846
      stem: 0.855
      writing: 0.84
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: 0.275
- id: meta-llama/Meta-Llama-3-8B
  name: Llama 3 8B
  date: '2024-04-18'
  params: 8.0
  base_model: ''
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-8B
  results:
    ja_basic:
      Ja Avg: 0.429
      JComQA: 0.835
      JEMHopQA: 0.436
      NIILC: 0.41
      JSQuAD: 0.892
      XL-Sum: 0.177
      MGSM: 0.312
      WMT20-en-ja: 0.221
      WMT20-ja-en: 0.206
      JMMLU: 0.455
      JHumanEval: 0.344
    en_basic:
      En Avg: 0.542
      OpenBookQA: 0.38
      TriviaQA: 0.712
      HellaSwag: 0.612
      SQuAD2: 0.502
      XWINO: 0.905
      MMLU: 0.651
      GSM8K: 0.487
      MATH: 0.18
      BBH: 0.62
      HumanEval: 0.376
    other:
      GPQA: 0.136
- id: meta-llama/Meta-Llama-3-8B-Instruct
  name: Llama 3 8B Instruct
  date: '2024-04-18'
  params: 8.0
  base_model: Llama 3 8B
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.43
      JComQA: 0.88
      JEMHopQA: 0.417
      NIILC: 0.385
      JSQuAD: 0.891
      XL-Sum: 0.126
      MGSM: 0.424
      WMT20-en-ja: 0.214
      WMT20-ja-en: 0.202
      JMMLU: 0.468
      JHumanEval: 0.296
    ja_mtb:
      JMT Avg: 0.529
      coding: 0.467
      extraction: 0.706
      humanities: 0.692
      math: 0.31
      reasoning: 0.433
      roleplay: 0.542
      stem: 0.532
      writing: 0.546
    en_basic:
      En Avg: 0.605
      OpenBookQA: 0.388
      TriviaQA: 0.67
      HellaSwag: 0.583
      SQuAD2: 0.611
      XWINO: 0.892
      MMLU: 0.657
      GSM8K: 0.745
      MATH: 0.306
      BBH: 0.646
      HumanEval: 0.554
    other:
      GPQA: 0.257
- id: meta-llama/Meta-Llama-3-70B
  name: Llama 3 70B
  date: '2024-04-18'
  params: 70
  base_model: ''
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-70B
  results:
    ja_basic:
      Ja Avg: 0.569
      JComQA: 0.946
      JEMHopQA: 0.606
      NIILC: 0.589
      JSQuAD: 0.922
      XL-Sum: 0.228
      MGSM: 0.664
      WMT20-en-ja: 0.286
      WMT20-ja-en: 0.252
      JMMLU: 0.705
      JHumanEval: 0.491
    en_basic:
      En Avg: 0.689
      OpenBookQA: 0.44
      TriviaQA: 0.826
      HellaSwag: 0.69
      SQuAD2: 0.618
      XWINO: 0.92
      MMLU: 0.787
      GSM8K: 0.801
      MATH: 0.446
      BBH: 0.829
      HumanEval: 0.527
    other:
      GPQA: 0.268
- id: meta-llama/Meta-Llama-3-70B-Instruct
  name: Llama 3 70B Instruct
  date: '2024-04-18'
  params: 70
  base_model: Llama 3 70B
  sortkey: llama 3
  url: https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.578
      JComQA: 0.94
      JEMHopQA: 0.615
      NIILC: 0.557
      JSQuAD: 0.913
      XL-Sum: 0.191
      MGSM: 0.716
      WMT20-en-ja: 0.269
      WMT20-ja-en: 0.234
      JMMLU: 0.68
      JHumanEval: 0.662
    ja_mtb:
      JMT Avg: 0.64
      coding: 0.588
      extraction: 0.884
      humanities: 0.715
      math: 0.637
      reasoning: 0.487
      roleplay: 0.594
      stem: 0.598
      writing: 0.619
    en_basic:
      En Avg: 0.729
      OpenBookQA: 0.438
      TriviaQA: 0.8
      HellaSwag: 0.655
      SQuAD2: 0.696
      XWINO: 0.914
      MMLU: 0.8
      GSM8K: 0.909
      MATH: 0.474
      BBH: 0.833
      HumanEval: 0.774
    other:
      GPQA: 0.036
- id: elyza/Llama-3-ELYZA-JP-8B
  name: Llama-3-ELYZA-JP-8B
  date: '2024-06-26'
  params: 8.0
  base_model: (private)
  sortkey: llama 3 elyza jp
  url: https://huggingface.co/elyza/Llama-3-ELYZA-JP-8B
  results:
    ja_basic:
      Ja Avg: 0.471
      JComQA: 0.897
      JEMHopQA: 0.498
      NIILC: 0.496
      JSQuAD: 0.906
      XL-Sum: 0.168
      MGSM: 0.436
      WMT20-en-ja: 0.25
      WMT20-ja-en: 0.185
      JMMLU: 0.487
      JHumanEval: 0.388
    ja_mtb:
      JMT Avg: 0.587
      coding: 0.389
      extraction: 0.706
      humanities: 0.647
      math: 0.426
      reasoning: 0.613
      roleplay: 0.684
      stem: 0.533
      writing: 0.697
    en_basic:
      En Avg: 0.495
      OpenBookQA: 0.318
      TriviaQA: 0.551
      HellaSwag: 0.523
      SQuAD2: 0.6
      XWINO: 0.882
      MMLU: 0.587
      GSM8K: 0.558
      MATH: 0.164
      BBH: 0.321
      HumanEval: 0.449
    other:
      GPQA: 0.221
- id: turing-motors/Llama-3-heron-brain-8B-v0.3
  name: Llama 3 heron brain 8B v0.3
  date: '2024-07-01'
  params: 8.0
  base_model: Llama 3 Swallow 8B
  sortkey: llama 3 heron brain v0.3
  url: https://huggingface.co/turing-motors/Llama-3-heron-brain-8B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.488
      JComQA: 0.923
      JEMHopQA: 0.493
      NIILC: 0.569
      JSQuAD: 0.906
      XL-Sum: 0.218
      MGSM: 0.456
      WMT20-en-ja: 0.277
      WMT20-ja-en: 0.217
      JMMLU: 0.499
      JHumanEval: 0.318
    ja_mtb:
      JMT Avg: 0.497
      coding: 0.362
      extraction: 0.566
      humanities: 0.602
      math: 0.315
      reasoning: 0.426
      roleplay: 0.586
      stem: 0.567
      writing: 0.55
    en_basic:
      En Avg: 0.551
      OpenBookQA: 0.362
      TriviaQA: 0.656
      HellaSwag: 0.569
      SQuAD2: 0.581
      XWINO: 0.901
      MMLU: 0.621
      GSM8K: 0.578
      MATH: 0.222
      BBH: 0.641
      HumanEval: 0.38
    other:
      GPQA: 0.0
- id: turing-motors/Llama-3-heron-brain-70B-v0.3
  name: Llama 3 heron brain 70B v0.3
  date: '2024-07-01'
  params: 70
  base_model: Llama 3 Swallow 70B
  sortkey: llama 3 heron brain v0.3
  url: https://huggingface.co/turing-motors/Llama-3-heron-brain-70B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.615
      JComQA: 0.965
      JEMHopQA: 0.652
      NIILC: 0.679
      JSQuAD: 0.922
      XL-Sum: 0.261
      MGSM: 0.772
      WMT20-en-ja: 0.309
      WMT20-ja-en: 0.258
      JMMLU: 0.707
      JHumanEval: 0.623
    ja_mtb:
      JMT Avg: 0.683
      coding: 0.51
      extraction: 0.87
      humanities: 0.776
      math: 0.68
      reasoning: 0.513
      roleplay: 0.727
      stem: 0.692
      writing: 0.693
    en_basic:
      En Avg: 0.715
      OpenBookQA: 0.446
      TriviaQA: 0.811
      HellaSwag: 0.668
      SQuAD2: 0.706
      XWINO: 0.919
      MMLU: 0.79
      GSM8K: 0.877
      MATH: 0.508
      BBH: 0.759
      HumanEval: 0.668
    other:
      GPQA: 0.346
- id: tokyotech-llm/Llama-3-Swallow-8B-v0.1
  name: Llama 3 Swallow 8B
  date: '2024-07-01'
  params: 8.0
  base_model: ''
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.471
      JComQA: 0.896
      JEMHopQA: 0.478
      NIILC: 0.546
      JSQuAD: 0.9
      XL-Sum: 0.198
      MGSM: 0.44
      WMT20-en-ja: 0.276
      WMT20-ja-en: 0.222
      JMMLU: 0.471
      JHumanEval: 0.282
    en_basic:
      En Avg: 0.523
      OpenBookQA: 0.35
      TriviaQA: 0.656
      HellaSwag: 0.59
      SQuAD2: 0.519
      XWINO: 0.901
      MMLU: 0.615
      GSM8K: 0.483
      MATH: 0.182
      BBH: 0.598
      HumanEval: 0.337
    other:
      GPQA: 0.188
- id: tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1
  name: Llama 3 Swallow 8B Instruct
  date: '2024-07-01'
  params: 8.0
  base_model: Llama 3 Swallow 8B
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.481
      JComQA: 0.912
      JEMHopQA: 0.496
      NIILC: 0.517
      JSQuAD: 0.905
      XL-Sum: 0.128
      MGSM: 0.492
      WMT20-en-ja: 0.253
      WMT20-ja-en: 0.227
      JMMLU: 0.481
      JHumanEval: 0.394
    ja_mtb:
      JMT Avg: 0.427
      coding: 0.411
      extraction: 0.575
      humanities: 0.476
      math: 0.309
      reasoning: 0.305
      roleplay: 0.499
      stem: 0.438
      writing: 0.406
    en_basic:
      En Avg: 0.56
      OpenBookQA: 0.37
      TriviaQA: 0.655
      HellaSwag: 0.585
      SQuAD2: 0.567
      XWINO: 0.899
      MMLU: 0.633
      GSM8K: 0.592
      MATH: 0.244
      BBH: 0.639
      HumanEval: 0.42
    other:
      GPQA: 0.257
- id: tokyotech-llm/Llama-3-Swallow-70B-v0.1
  name: Llama 3 Swallow 70B
  date: '2024-07-01'
  params: 70
  base_model: ''
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.594
      JComQA: 0.968
      JEMHopQA: 0.675
      NIILC: 0.684
      JSQuAD: 0.923
      XL-Sum: 0.239
      MGSM: 0.708
      WMT20-en-ja: 0.307
      WMT20-ja-en: 0.255
      JMMLU: 0.706
      JHumanEval: 0.477
    en_basic:
      En Avg: 0.672
      OpenBookQA: 0.43
      TriviaQA: 0.823
      HellaSwag: 0.682
      SQuAD2: 0.628
      XWINO: 0.923
      MMLU: 0.774
      GSM8K: 0.817
      MATH: 0.414
      BBH: 0.734
      HumanEval: 0.499
    other:
      GPQA: 0.319
- id: tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1
  name: Llama 3 Swallow 70B Instruct
  date: '2024-07-01'
  params: 70
  base_model: Llama 3 Swallow 70B
  sortkey: llama 3 swallow
  url: https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.571
      JComQA: 0.963
      JEMHopQA: 0.627
      NIILC: 0.598
      JSQuAD: 0.921
      XL-Sum: 0.139
      MGSM: 0.672
      WMT20-en-ja: 0.272
      WMT20-ja-en: 0.255
      JMMLU: 0.657
      JHumanEval: 0.608
    ja_mtb:
      JMT Avg: 0.618
      coding: 0.633
      extraction: 0.823
      humanities: 0.601
      math: 0.521
      reasoning: 0.482
      roleplay: 0.622
      stem: 0.635
      writing: 0.63
    en_basic:
      En Avg: 0.716
      OpenBookQA: 0.446
      TriviaQA: 0.818
      HellaSwag: 0.676
      SQuAD2: 0.681
      XWINO: 0.923
      MMLU: 0.789
      GSM8K: 0.868
      MATH: 0.46
      BBH: 0.816
      HumanEval: 0.68
    other:
      GPQA: 0.357
- id: rinna/llama-3-youko-8b
  name: Llama 3 Youko 8B
  date: '2024-05-07'
  params: 8.0
  base_model: ''
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-8b
  results:
    ja_basic:
      Ja Avg: 0.442
      JComQA: 0.87
      JEMHopQA: 0.493
      NIILC: 0.513
      JSQuAD: 0.895
      XL-Sum: 0.213
      MGSM: 0.276
      WMT20-en-ja: 0.276
      WMT20-ja-en: 0.219
      JMMLU: 0.449
      JHumanEval: 0.222
    en_basic:
      En Avg: 0.486
      OpenBookQA: 0.348
      TriviaQA: 0.625
      HellaSwag: 0.589
      SQuAD2: 0.502
      XWINO: 0.896
      MMLU: 0.601
      GSM8K: 0.355
      MATH: 0.096
      BBH: 0.571
      HumanEval: 0.281
    other:
      GPQA: 0.063
- id: rinna/llama-3-youko-8b-instruct
  name: Llama 3 Youko 8B Instruct
  date: '2024-05-07'
  params: 8.0
  base_model: Llama 3 Youko 8B
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-8b-instruct
  results:
    ja_basic:
      Ja Avg: 0.468
      JComQA: 0.921
      JEMHopQA: 0.481
      NIILC: 0.517
      JSQuAD: 0.899
      XL-Sum: 0.209
      MGSM: 0.472
      WMT20-en-ja: 0.256
      WMT20-ja-en: 0.191
      JMMLU: 0.469
      JHumanEval: 0.262
    ja_mtb:
      JMT Avg: 0.616
      coding: 0.464
      extraction: 0.757
      humanities: 0.769
      math: 0.414
      reasoning: 0.487
      roleplay: 0.695
      stem: 0.583
      writing: 0.753
    en_basic:
      En Avg: 0.507
      OpenBookQA: 0.406
      TriviaQA: 0.613
      HellaSwag: 0.599
      SQuAD2: 0.559
      XWINO: 0.897
      MMLU: 0.596
      GSM8K: 0.563
      MATH: 0.152
      BBH: 0.401
      HumanEval: 0.287
    other:
      GPQA: 0.29
- id: rinna/llama-3-youko-70b
  name: Llama 3 Youko 70B
  date: '2024-07-25'
  params: 70
  base_model: ''
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-70b
  results:
    ja_basic:
      Ja Avg: 0.571
      JComQA: 0.946
      JEMHopQA: 0.602
      NIILC: 0.61
      JSQuAD: 0.923
      XL-Sum: 0.242
      MGSM: 0.684
      WMT20-en-ja: 0.292
      WMT20-ja-en: 0.25
      JMMLU: 0.704
      JHumanEval: 0.463
    en_basic:
      En Avg: 0.671
      OpenBookQA: 0.436
      TriviaQA: 0.829
      HellaSwag: 0.69
      SQuAD2: 0.61
      XWINO: 0.922
      MMLU: 0.785
      GSM8K: 0.797
      MATH: 0.408
      BBH: 0.826
      HumanEval: 0.412
    other:
      GPQA: 0.25
- id: rinna/llama-3-youko-70b-instruct
  name: Llama 3 Youko 70B Instruct
  date: '2024-07-25'
  params: 70
  base_model: Llama 3 Youko 70B
  sortkey: llama 3 youko
  url: https://huggingface.co/rinna/llama-3-youko-70b-instruct
  results:
    ja_basic:
      Ja Avg: 0.582
      JComQA: 0.952
      JEMHopQA: 0.625
      NIILC: 0.584
      JSQuAD: 0.921
      XL-Sum: 0.198
      MGSM: 0.72
      WMT20-en-ja: 0.263
      WMT20-ja-en: 0.226
      JMMLU: 0.718
      JHumanEval: 0.61
    ja_mtb:
      JMT Avg: 0.75
      coding: 0.607
      extraction: 0.894
      humanities: 0.834
      math: 0.609
      reasoning: 0.673
      roleplay: 0.79
      stem: 0.764
      writing: 0.829
    en_basic:
      En Avg: 0.708
      OpenBookQA: 0.454
      TriviaQA: 0.797
      HellaSwag: 0.686
      SQuAD2: 0.659
      XWINO: 0.915
      MMLU: 0.805
      GSM8K: 0.892
      MATH: 0.434
      BBH: 0.78
      HumanEval: 0.662
    other:
      GPQA: 0.304
- id: meta-llama/Meta-Llama-3.1-8B
  name: Llama 3.1 8B
  date: '2024-07-23'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
  results:
    ja_basic:
      Ja Avg: 0.437
      JComQA: 0.845
      JEMHopQA: 0.461
      NIILC: 0.405
      JSQuAD: 0.895
      XL-Sum: 0.179
      MGSM: 0.356
      WMT20-en-ja: 0.221
      WMT20-ja-en: 0.21
      JMMLU: 0.479
      JHumanEval: 0.32
    en_basic:
      En Avg: 0.545
      OpenBookQA: 0.38
      TriviaQA: 0.702
      HellaSwag: 0.609
      SQuAD2: 0.503
      XWINO: 0.907
      MMLU: 0.651
      GSM8K: 0.507
      MATH: 0.214
      BBH: 0.616
      HumanEval: 0.364
    other:
      GPQA: 0.17
- id: meta-llama/Meta-Llama-3.1-8B-Instruct
  name: Llama 3.1 8B Instruct
  date: '2024-07-23'
  params: 8.0
  base_model: Llama 3.1 8B
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.47
      JComQA: 0.88
      JEMHopQA: 0.447
      NIILC: 0.407
      JSQuAD: 0.886
      XL-Sum: 0.148
      MGSM: 0.516
      WMT20-en-ja: 0.218
      WMT20-ja-en: 0.2
      JMMLU: 0.509
      JHumanEval: 0.488
    ja_mtb:
      JMT Avg: 0.519
      coding: 0.42
      extraction: 0.83
      humanities: 0.55
      math: 0.514
      reasoning: 0.349
      roleplay: 0.502
      stem: 0.479
      writing: 0.504
    en_basic:
      En Avg: 0.627
      OpenBookQA: 0.366
      TriviaQA: 0.699
      HellaSwag: 0.592
      SQuAD2: 0.6
      XWINO: 0.904
      MMLU: 0.68
      GSM8K: 0.743
      MATH: 0.376
      BBH: 0.69
      HumanEval: 0.624
    other:
      GPQA: 0.283
- id: meta-llama/Meta-Llama-3.1-70B
  name: Llama 3.1 70B
  date: '2024-07-23'
  params: 70
  base_model: ''
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B
  results:
    ja_basic:
      Ja Avg: 0.566
      JComQA: 0.946
      JEMHopQA: 0.616
      NIILC: 0.603
      JSQuAD: 0.925
      XL-Sum: 0.228
      MGSM: 0.672
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.257
      JMMLU: 0.669
      JHumanEval: 0.462
    en_basic:
      En Avg: 0.671
      OpenBookQA: 0.45
      TriviaQA: 0.829
      HellaSwag: 0.69
      SQuAD2: 0.605
      XWINO: 0.92
      MMLU: 0.786
      GSM8K: 0.798
      MATH: 0.434
      BBH: 0.655
      HumanEval: 0.546
    other:
      GPQA: 0.188
- id: meta-llama/Meta-Llama-3.1-70B-Instruct
  name: Llama 3.1 70B Instruct
  date: '2024-07-23'
  params: 70
  base_model: Llama 3.1 70B
  sortkey: llama 3.1
  url: https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.595
      JComQA: 0.95
      JEMHopQA: 0.635
      NIILC: 0.579
      JSQuAD: 0.921
      XL-Sum: 0.178
      MGSM: 0.732
      WMT20-en-ja: 0.279
      WMT20-ja-en: 0.247
      JMMLU: 0.733
      JHumanEval: 0.696
    ja_mtb:
      JMT Avg: 0.706
      coding: 0.691
      extraction: 0.848
      humanities: 0.73
      math: 0.669
      reasoning: 0.618
      roleplay: 0.699
      stem: 0.699
      writing: 0.694
    en_basic:
      En Avg: 0.738
      OpenBookQA: 0.426
      TriviaQA: 0.821
      HellaSwag: 0.662
      SQuAD2: 0.66
      XWINO: 0.917
      MMLU: 0.822
      GSM8K: 0.876
      MATH: 0.56
      BBH: 0.842
      HumanEval: 0.794
    other:
      GPQA: 0.435
- id: cyberagent/Llama-3.1-70B-Japanese-Instruct-2407
  name: Llama-3.1-70B-Japanese-Instruct-2407
  date: '2024-07-23'
  params: 70
  base_model: Llama 3.1 70B Instruct
  sortkey: llama 3.1 japanese 2407
  url: https://huggingface.co/cyberagent/Llama-3.1-70B-Japanese-Instruct-2407
  results:
    ja_basic:
      Ja Avg: 0.597
      JComQA: 0.956
      JEMHopQA: 0.647
      NIILC: 0.66
      JSQuAD: 0.919
      XL-Sum: 0.156
      MGSM: 0.748
      WMT20-en-ja: 0.29
      WMT20-ja-en: 0.241
      JMMLU: 0.723
      JHumanEval: 0.627
    ja_mtb:
      JMT Avg: 0.751
      coding: 0.683
      extraction: 0.827
      humanities: 0.824
      math: 0.749
      reasoning: 0.643
      roleplay: 0.818
      stem: 0.715
      writing: 0.751
    en_basic:
      En Avg: 0.725
      OpenBookQA: 0.422
      TriviaQA: 0.81
      HellaSwag: 0.647
      SQuAD2: 0.663
      XWINO: 0.917
      MMLU: 0.807
      GSM8K: 0.889
      MATH: 0.528
      BBH: 0.823
      HumanEval: 0.746
    other:
      GPQA: 0.444
- id: tokyotech-llm/Llama-3.1-Swallow-8B-v0.1
  name: Llama 3.1 Swallow 8B v0.1
  date: '2024-10-08'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.49
      JComQA: 0.912
      JEMHopQA: 0.509
      NIILC: 0.601
      JSQuAD: 0.899
      XL-Sum: 0.202
      MGSM: 0.46
      WMT20-en-ja: 0.291
      WMT20-ja-en: 0.231
      JMMLU: 0.518
      JHumanEval: 0.276
    en_basic:
      En Avg: 0.538
      OpenBookQA: 0.378
      TriviaQA: 0.671
      HellaSwag: 0.605
      SQuAD2: 0.502
      XWINO: 0.905
      MMLU: 0.624
      GSM8K: 0.511
      MATH: 0.224
      BBH: 0.615
      HumanEval: 0.348
    other:
      GPQA: 0.234
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1
  name: Llama 3.1 Swallow 8B Instruct v0.1
  date: '2024-10-08'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.1
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.505
      JComQA: 0.924
      JEMHopQA: 0.587
      NIILC: 0.574
      JSQuAD: 0.917
      XL-Sum: 0.138
      MGSM: 0.508
      WMT20-en-ja: 0.282
      WMT20-ja-en: 0.228
      JMMLU: 0.53
      JHumanEval: 0.366
    ja_mtb:
      JMT Avg: 0.581
      coding: 0.427
      extraction: 0.738
      humanities: 0.675
      math: 0.527
      reasoning: 0.453
      roleplay: 0.615
      stem: 0.593
      writing: 0.624
    en_basic:
      En Avg: 0.563
      OpenBookQA: 0.388
      TriviaQA: 0.649
      HellaSwag: 0.615
      SQuAD2: 0.598
      XWINO: 0.891
      MMLU: 0.624
      GSM8K: 0.605
      MATH: 0.236
      BBH: 0.642
      HumanEval: 0.379
    other:
      GPQA: 0.281
- id: tokyotech-llm/Llama-3.1-Swallow-70B-v0.1
  name: Llama 3.1 Swallow 70B v0.1
  date: '2024-10-08'
  params: 70
  base_model: ''
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.593
      JComQA: 0.955
      JEMHopQA: 0.645
      NIILC: 0.678
      JSQuAD: 0.923
      XL-Sum: 0.272
      MGSM: 0.684
      WMT20-en-ja: 0.32
      WMT20-ja-en: 0.259
      JMMLU: 0.709
      JHumanEval: 0.487
    en_basic:
      En Avg: 0.679
      OpenBookQA: 0.428
      TriviaQA: 0.826
      HellaSwag: 0.69
      SQuAD2: 0.612
      XWINO: 0.927
      MMLU: 0.772
      GSM8K: 0.809
      MATH: 0.38
      BBH: 0.806
      HumanEval: 0.54
    other:
      GPQA: 0.288
- id: tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1
  name: Llama 3.1 Swallow 70B Instruct v0.1
  date: '2024-10-08'
  params: 70
  base_model: Llama 3.1 Swallow 70B v0.1
  sortkey: llama 3.1 swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.588
      JComQA: 0.962
      JEMHopQA: 0.621
      NIILC: 0.66
      JSQuAD: 0.924
      XL-Sum: 0.192
      MGSM: 0.776
      WMT20-en-ja: 0.312
      WMT20-ja-en: 0.259
      JMMLU: 0.711
      JHumanEval: 0.468
    ja_mtb:
      JMT Avg: 0.691
      coding: 0.654
      extraction: 0.792
      humanities: 0.768
      math: 0.704
      reasoning: 0.573
      roleplay: 0.682
      stem: 0.653
      writing: 0.704
    en_basic:
      En Avg: 0.71
      OpenBookQA: 0.446
      TriviaQA: 0.815
      HellaSwag: 0.683
      SQuAD2: 0.681
      XWINO: 0.917
      MMLU: 0.787
      GSM8K: 0.884
      MATH: 0.474
      BBH: 0.848
      HumanEval: 0.568
    other:
      GPQA: 0.395
- id: tokyotech-llm/Llama-3.1-Swallow-8B-v0.2
  name: Llama 3.1 Swallow 8B v0.2
  date: '2024-11-11'
  params: 8.0
  base_model: ''
  sortkey: llama 3.1 swallow v0.2
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2
  results:
    ja_basic:
      Ja Avg: 0.499
      JComQA: 0.911
      JEMHopQA: 0.51
      NIILC: 0.627
      JSQuAD: 0.892
      XL-Sum: 0.198
      MGSM: 0.464
      WMT20-en-ja: 0.296
      WMT20-ja-en: 0.233
      JMMLU: 0.525
      JHumanEval: 0.336
    en_basic:
      En Avg: 0.539
      OpenBookQA: 0.382
      TriviaQA: 0.651
      HellaSwag: 0.596
      SQuAD2: 0.513
      XWINO: 0.904
      MMLU: 0.622
      GSM8K: 0.521
      MATH: 0.228
      BBH: 0.605
      HumanEval: 0.366
    other:
      GPQA: 0.225
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2
  name: Llama 3.1 Swallow 8B Instruct v0.2
  date: '2024-11-11'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.2
  sortkey: llama 3.1 swallow v0.2
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2
  results:
    ja_basic:
      Ja Avg: 0.514
      JComQA: 0.929
      JEMHopQA: 0.56
      NIILC: 0.599
      JSQuAD: 0.915
      XL-Sum: 0.137
      MGSM: 0.528
      WMT20-en-ja: 0.288
      WMT20-ja-en: 0.227
      JMMLU: 0.55
      JHumanEval: 0.408
    ja_mtb:
      JMT Avg: 0.612
      coding: 0.534
      extraction: 0.748
      humanities: 0.705
      math: 0.565
      reasoning: 0.475
      roleplay: 0.646
      stem: 0.579
      writing: 0.646
    en_basic:
      En Avg: 0.574
      OpenBookQA: 0.38
      TriviaQA: 0.625
      HellaSwag: 0.603
      SQuAD2: 0.607
      XWINO: 0.887
      MMLU: 0.634
      GSM8K: 0.62
      MATH: 0.264
      BBH: 0.649
      HumanEval: 0.474
    other:
      GPQA: 0.301
- id: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  name: Llama 3.1 Swallow 8B Instruct v0.3
  date: '2024-12-23'
  params: 8.0
  base_model: Llama 3.1 Swallow 8B v0.2
  sortkey: llama 3.1 swallow v0.3
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.51
      JComQA: 0.924
      JEMHopQA: 0.528
      NIILC: 0.583
      JSQuAD: 0.896
      XL-Sum: 0.191
      MGSM: 0.532
      WMT20-en-ja: 0.281
      WMT20-ja-en: 0.229
      JMMLU: 0.544
      JHumanEval: 0.394
    ja_mtb:
      JMT Avg: 0.705
      coding: 0.562
      extraction: 0.756
      humanities: 0.869
      math: 0.61
      reasoning: 0.512
      roleplay: 0.783
      stem: 0.748
      writing: 0.803
    en_basic:
      En Avg: 0.566
      OpenBookQA: 0.396
      TriviaQA: 0.629
      HellaSwag: 0.593
      SQuAD2: 0.57
      XWINO: 0.884
      MMLU: 0.629
      GSM8K: 0.622
      MATH: 0.266
      BBH: 0.626
      HumanEval: 0.445
    other:
      GPQA: 0.201
- id: tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3
  name: Llama 3.1 Swallow 70B Instruct v0.3
  date: '2024-12-30'
  params: 70
  base_model: Llama 3.1 Swallow 70B v0.1
  sortkey: llama 3.1 swallow v0.3
  url: https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.598
      JComQA: 0.964
      JEMHopQA: 0.632
      NIILC: 0.654
      JSQuAD: 0.911
      XL-Sum: 0.196
      MGSM: 0.772
      WMT20-en-ja: 0.305
      WMT20-ja-en: 0.257
      JMMLU: 0.69
      JHumanEval: 0.596
    ja_mtb:
      JMT Avg: 0.769
      coding: 0.678
      extraction: 0.82
      humanities: 0.867
      math: 0.776
      reasoning: 0.57
      roleplay: 0.816
      stem: 0.769
      writing: 0.852
    en_basic:
      En Avg: 0.71
      OpenBookQA: 0.454
      TriviaQA: 0.825
      HellaSwag: 0.692
      SQuAD2: 0.647
      XWINO: 0.919
      MMLU: 0.777
      GSM8K: 0.872
      MATH: 0.458
      BBH: 0.816
      HumanEval: 0.643
    other:
      GPQA: 0.259
- id: meta-llama/Llama-3.2-1B
  name: Llama 3.2 1B
  date: '2024-09-25'
  params: 1.2
  base_model: ''
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-1B
  results:
    ja_basic:
      Ja Avg: 0.201
      JComQA: 0.208
      JEMHopQA: 0.404
      NIILC: 0.188
      JSQuAD: 0.525
      XL-Sum: 0.081
      MGSM: 0.024
      WMT20-en-ja: 0.079
      WMT20-ja-en: 0.092
      JMMLU: 0.26
      JHumanEval: 0.15
    en_basic:
      En Avg: 0.339
      OpenBookQA: 0.3
      TriviaQA: 0.388
      HellaSwag: 0.477
      SQuAD2: 0.501
      XWINO: 0.849
      MMLU: 0.313
      GSM8K: 0.049
      MATH: 0.02
      BBH: 0.303
      HumanEval: 0.193
    other:
      GPQA: 0.018
- id: meta-llama/Llama-3.2-1B-Instruct
  name: Llama 3.2 1B Instruct
  date: '2024-09-25'
  params: 1.2
  base_model: Llama 3.2 1B
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.239
      JComQA: 0.397
      JEMHopQA: 0.346
      NIILC: 0.179
      JSQuAD: 0.57
      XL-Sum: 0.075
      MGSM: 0.164
      WMT20-en-ja: 0.07
      WMT20-ja-en: 0.091
      JMMLU: 0.287
      JHumanEval: 0.207
    ja_mtb:
      JMT Avg: 0.273
      coding: 0.254
      extraction: 0.376
      humanities: 0.218
      math: 0.307
      reasoning: 0.267
      roleplay: 0.262
      stem: 0.246
      writing: 0.258
    en_basic:
      En Avg: 0.408
      OpenBookQA: 0.274
      TriviaQA: 0.375
      HellaSwag: 0.44
      SQuAD2: 0.501
      XWINO: 0.837
      MMLU: 0.454
      GSM8K: 0.318
      MATH: 0.172
      BBH: 0.362
      HumanEval: 0.347
    other:
      GPQA: 0.051
- id: meta-llama/Llama-3.2-3B
  name: Llama 3.2 3B
  date: '2024-09-25'
  params: 3.2
  base_model: ''
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-3B
  results:
    ja_basic:
      Ja Avg: 0.337
      JComQA: 0.605
      JEMHopQA: 0.443
      NIILC: 0.324
      JSQuAD: 0.816
      XL-Sum: 0.129
      MGSM: 0.136
      WMT20-en-ja: 0.161
      WMT20-ja-en: 0.167
      JMMLU: 0.352
      JHumanEval: 0.235
    en_basic:
      En Avg: 0.45
      OpenBookQA: 0.326
      TriviaQA: 0.586
      HellaSwag: 0.558
      SQuAD2: 0.502
      XWINO: 0.888
      MMLU: 0.558
      GSM8K: 0.262
      MATH: 0.07
      BBH: 0.466
      HumanEval: 0.285
    other:
      GPQA: 0.042
- id: meta-llama/Llama-3.2-3B-Instruct
  name: Llama 3.2 3B Instruct
  date: '2024-09-25'
  params: 3.2
  base_model: Llama 3.2 3B
  sortkey: llama 3.2
  url: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.38
      JComQA: 0.783
      JEMHopQA: 0.304
      NIILC: 0.268
      JSQuAD: 0.846
      XL-Sum: 0.112
      MGSM: 0.372
      WMT20-en-ja: 0.173
      WMT20-ja-en: 0.155
      JMMLU: 0.404
      JHumanEval: 0.387
    ja_mtb:
      JMT Avg: 0.405
      coding: 0.426
      extraction: 0.593
      humanities: 0.431
      math: 0.389
      reasoning: 0.292
      roleplay: 0.35
      stem: 0.38
      writing: 0.38
    en_basic:
      En Avg: 0.537
      OpenBookQA: 0.306
      TriviaQA: 0.556
      HellaSwag: 0.524
      SQuAD2: 0.54
      XWINO: 0.874
      MMLU: 0.597
      GSM8K: 0.629
      MATH: 0.324
      BBH: 0.512
      HumanEval: 0.511
    other:
      GPQA: 0.259
- id: meta-llama/Llama-3.3-70B-Instruct
  name: Llama 3.3 70B Instruct
  date: '2024-12-06'
  params: 70
  base_model: Llama 3.1 70B
  sortkey: llama 3.3
  url: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.601
      JComQA: 0.941
      JEMHopQA: 0.64
      NIILC: 0.57
      JSQuAD: 0.893
      XL-Sum: 0.179
      MGSM: 0.784
      WMT20-en-ja: 0.278
      WMT20-ja-en: 0.243
      JMMLU: 0.735
      JHumanEval: 0.744
    ja_mtb:
      JMT Avg: 0.737
      coding: 0.707
      extraction: 0.865
      humanities: 0.757
      math: 0.72
      reasoning: 0.635
      roleplay: 0.773
      stem: 0.706
      writing: 0.733
    en_basic:
      En Avg: 0.762
      OpenBookQA: 0.426
      TriviaQA: 0.817
      HellaSwag: 0.667
      SQuAD2: 0.684
      XWINO: 0.917
      MMLU: 0.824
      GSM8K: 0.89
      MATH: 0.706
      BBH: 0.853
      HumanEval: 0.834
    other:
      GPQA: 0.592
- id: tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  name: Llama 3.3 Swallow 70B v0.4
  date: '2025-03-14'
  params: 70
  base_model: ''
  sortkey: llama 3.3 swallow v0.4
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-v0.4
  results:
    ja_basic:
      Ja Avg: 0.629
      JComQA: 0.967
      JEMHopQA: 0.671
      NIILC: 0.732
      JSQuAD: 0.924
      XL-Sum: 0.283
      MGSM: 0.776
      WMT20-en-ja: 0.327
      WMT20-ja-en: 0.26
      JMMLU: 0.742
      JHumanEval: 0.604
    en_basic:
      En Avg: 0.711
      OpenBookQA: 0.424
      TriviaQA: 0.817
      HellaSwag: 0.683
      SQuAD2: 0.641
      XWINO: 0.92
      MMLU: 0.802
      GSM8K: 0.863
      MATH: 0.496
      BBH: 0.754
      HumanEval: 0.709
    other:
      GPQA: 0.163
- id: tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4
  name: Llama 3.3 Swallow 70B Instruct v0.4
  date: '2025-03-10'
  params: 70
  base_model: Llama 3.3 Swallow 70B v0.4
  sortkey: llama 3.3 swallow v0.4
  url: https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4
  results:
    ja_basic:
      Ja Avg: 0.613
      JComQA: 0.981
      JEMHopQA: 0.618
      NIILC: 0.662
      JSQuAD: 0.907
      XL-Sum: 0.162
      MGSM: 0.812
      WMT20-en-ja: 0.319
      WMT20-ja-en: 0.261
      JMMLU: 0.707
      JHumanEval: 0.7
    ja_mtb:
      JMT Avg: 0.772
      coding: 0.705
      extraction: 0.82
      humanities: 0.87
      math: 0.73
      reasoning: 0.623
      roleplay: 0.811
      stem: 0.781
      writing: 0.832
    en_basic:
      En Avg: 0.736
      OpenBookQA: 0.448
      TriviaQA: 0.817
      HellaSwag: 0.686
      SQuAD2: 0.654
      XWINO: 0.912
      MMLU: 0.803
      GSM8K: 0.908
      MATH: 0.566
      BBH: 0.812
      HumanEval: 0.75
    other:
      GPQA: 0.402
- id: llm-jp/llm-jp-3-1.8b
  name: llm-jp-3-1.8b
  date: '2024-09-25'
  params: 1.8
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-1.8b
  results:
    ja_basic:
      Ja Avg: 0.251
      JComQA: 0.209
      JEMHopQA: 0.463
      NIILC: 0.449
      JSQuAD: 0.703
      XL-Sum: 0.1
      MGSM: 0.012
      WMT20-en-ja: 0.198
      WMT20-ja-en: 0.134
      JMMLU: 0.242
      JHumanEval: 0.001
    en_basic:
      En Avg: 0.293
      OpenBookQA: 0.244
      TriviaQA: 0.301
      HellaSwag: 0.462
      SQuAD2: 0.501
      XWINO: 0.851
      MMLU: 0.248
      GSM8K: 0.017
      MATH: 0.018
      BBH: 0.276
      HumanEval: 0.008
    other:
      GPQA: 0.0
- id: llm-jp/llm-jp-3-1.8b-instruct
  name: llm-jp-3-1.8b-instruct
  date: '2024-09-25'
  params: 1.8
  base_model: llm-jp-3-1.8b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-1.8b-instruct
  results:
    ja_basic:
      Ja Avg: 0.293
      JComQA: 0.324
      JEMHopQA: 0.413
      NIILC: 0.466
      JSQuAD: 0.837
      XL-Sum: 0.105
      MGSM: 0.08
      WMT20-en-ja: 0.206
      WMT20-ja-en: 0.142
      JMMLU: 0.292
      JHumanEval: 0.061
    ja_mtb:
      JMT Avg: 0.451
      coding: 0.274
      extraction: 0.321
      humanities: 0.68
      math: 0.281
      reasoning: 0.301
      roleplay: 0.628
      stem: 0.504
      writing: 0.617
    en_basic:
      En Avg: 0.313
      OpenBookQA: 0.286
      TriviaQA: 0.296
      HellaSwag: 0.485
      SQuAD2: 0.502
      XWINO: 0.847
      MMLU: 0.277
      GSM8K: 0.043
      MATH: 0.016
      BBH: 0.29
      HumanEval: 0.087
    other:
      GPQA: 0.123
- id: llm-jp/llm-jp-3-3.7b
  name: llm-jp-3-3.7b
  date: '2024-09-25'
  params: 3.7
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-3.7b
  results:
    ja_basic:
      Ja Avg: 0.281
      JComQA: 0.203
      JEMHopQA: 0.431
      NIILC: 0.541
      JSQuAD: 0.804
      XL-Sum: 0.142
      MGSM: 0.06
      WMT20-en-ja: 0.223
      WMT20-ja-en: 0.159
      JMMLU: 0.249
      JHumanEval: 0.0
    en_basic:
      En Avg: 0.324
      OpenBookQA: 0.28
      TriviaQA: 0.421
      HellaSwag: 0.506
      SQuAD2: 0.502
      XWINO: 0.876
      MMLU: 0.253
      GSM8K: 0.055
      MATH: 0.016
      BBH: 0.309
      HumanEval: 0.019
    other:
      GPQA: 0.007
- id: llm-jp/llm-jp-3-3.7b-instruct
  name: llm-jp-3-3.7b-instruct
  date: '2024-09-25'
  params: 3.7
  base_model: llm-jp-3-3.7b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-3.7b-instruct
  results:
    ja_basic:
      Ja Avg: 0.35
      JComQA: 0.533
      JEMHopQA: 0.464
      NIILC: 0.529
      JSQuAD: 0.847
      XL-Sum: 0.139
      MGSM: 0.152
      WMT20-en-ja: 0.224
      WMT20-ja-en: 0.17
      JMMLU: 0.359
      JHumanEval: 0.085
    ja_mtb:
      JMT Avg: 0.485
      coding: 0.311
      extraction: 0.418
      humanities: 0.73
      math: 0.311
      reasoning: 0.339
      roleplay: 0.618
      stem: 0.551
      writing: 0.6
    en_basic:
      En Avg: 0.347
      OpenBookQA: 0.31
      TriviaQA: 0.398
      HellaSwag: 0.534
      SQuAD2: 0.503
      XWINO: 0.862
      MMLU: 0.349
      GSM8K: 0.071
      MATH: 0.022
      BBH: 0.324
      HumanEval: 0.099
    other:
      GPQA: 0.112
- id: llm-jp/llm-jp-3-13b
  name: llm-jp-3-13b
  date: '2024-09-25'
  params: 13
  base_model: ''
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-13b
  results:
    ja_basic:
      Ja Avg: 0.393
      JComQA: 0.65
      JEMHopQA: 0.525
      NIILC: 0.649
      JSQuAD: 0.882
      XL-Sum: 0.164
      MGSM: 0.16
      WMT20-en-ja: 0.273
      WMT20-ja-en: 0.21
      JMMLU: 0.399
      JHumanEval: 0.023
    en_basic:
      En Avg: 0.399
      OpenBookQA: 0.332
      TriviaQA: 0.602
      HellaSwag: 0.57
      SQuAD2: 0.501
      XWINO: 0.902
      MMLU: 0.462
      GSM8K: 0.158
      MATH: 0.026
      BBH: 0.402
      HumanEval: 0.032
    other:
      GPQA: 0.094
- id: llm-jp/llm-jp-3-13b-instruct
  name: llm-jp-3-13b-instruct
  date: '2024-09-25'
  params: 13
  base_model: llm-jp-3-13b
  sortkey: llm jp 3
  url: https://huggingface.co/llm-jp/llm-jp-3-13b-instruct
  results:
    ja_basic:
      Ja Avg: 0.436
      JComQA: 0.894
      JEMHopQA: 0.339
      NIILC: 0.638
      JSQuAD: 0.901
      XL-Sum: 0.151
      MGSM: 0.324
      WMT20-en-ja: 0.252
      WMT20-ja-en: 0.203
      JMMLU: 0.468
      JHumanEval: 0.188
    ja_mtb:
      JMT Avg: 0.588
      coding: 0.373
      extraction: 0.556
      humanities: 0.816
      math: 0.371
      reasoning: 0.526
      roleplay: 0.73
      stem: 0.614
      writing: 0.715
    en_basic:
      En Avg: 0.432
      OpenBookQA: 0.342
      TriviaQA: 0.534
      HellaSwag: 0.594
      SQuAD2: 0.516
      XWINO: 0.892
      MMLU: 0.506
      GSM8K: 0.243
      MATH: 0.046
      BBH: 0.438
      HumanEval: 0.205
    other:
      GPQA: 0.201
- id: mistralai/Mistral-Nemo-Base-2407
  name: Mistral-Nemo-Base-2407 (12B)
  date: '2024-07-18'
  params: 12
  base_model: ''
  sortkey: mistral nemo 2407 ()
  url: https://huggingface.co/mistralai/Mistral-Nemo-Base-2407
  results:
    ja_basic:
      Ja Avg: 0.46
      JComQA: 0.911
      JEMHopQA: 0.516
      NIILC: 0.475
      JSQuAD: 0.904
      XL-Sum: 0.192
      MGSM: 0.416
      WMT20-en-ja: 0.244
      WMT20-ja-en: 0.212
      JMMLU: 0.538
      JHumanEval: 0.194
    en_basic:
      En Avg: 0.559
      OpenBookQA: 0.422
      TriviaQA: 0.741
      HellaSwag: 0.647
      SQuAD2: 0.528
      XWINO: 0.914
      MMLU: 0.69
      GSM8K: 0.55
      MATH: 0.184
      BBH: 0.657
      HumanEval: 0.259
    other:
      GPQA: 0.007
- id: mistralai/Mistral-Nemo-Instruct-2407
  name: Mistral-NeMo-Instruct-2407 (12B)
  date: '2024-07-18'
  params: 12
  base_model: Mistral-Nemo-Base-2407 (12B)
  sortkey: mistral nemo 2407 ()
  url: https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407
  results:
    ja_basic:
      Ja Avg: 0.5
      JComQA: 0.927
      JEMHopQA: 0.497
      NIILC: 0.484
      JSQuAD: 0.905
      XL-Sum: 0.176
      MGSM: 0.552
      WMT20-en-ja: 0.24
      WMT20-ja-en: 0.205
      JMMLU: 0.548
      JHumanEval: 0.47
    ja_mtb:
      JMT Avg: 0.616
      coding: 0.515
      extraction: 0.698
      humanities: 0.702
      math: 0.512
      reasoning: 0.481
      roleplay: 0.669
      stem: 0.66
      writing: 0.691
    en_basic:
      En Avg: 0.608
      OpenBookQA: 0.406
      TriviaQA: 0.726
      HellaSwag: 0.645
      SQuAD2: 0.606
      XWINO: 0.911
      MMLU: 0.683
      GSM8K: 0.721
      MATH: 0.274
      BBH: 0.537
      HumanEval: 0.571
    other:
      GPQA: 0.208
- id: nvidia/Mistral-NeMo-Minitron-8B-Base
  name: Mistral-NeMo-Minitron 8B
  date: '2024-08-21'
  params: 8.4
  base_model: ''
  sortkey: mistral nemo minitron
  url: https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base
  results:
    ja_basic:
      Ja Avg: 0.444
      JComQA: 0.887
      JEMHopQA: 0.486
      NIILC: 0.374
      JSQuAD: 0.902
      XL-Sum: 0.157
      MGSM: 0.424
      WMT20-en-ja: 0.186
      WMT20-ja-en: 0.193
      JMMLU: 0.494
      JHumanEval: 0.332
    en_basic:
      En Avg: 0.572
      OpenBookQA: 0.406
      TriviaQA: 0.728
      HellaSwag: 0.621
      SQuAD2: 0.525
      XWINO: 0.915
      MMLU: 0.694
      GSM8K: 0.585
      MATH: 0.202
      BBH: 0.658
      HumanEval: 0.382
    other:
      GPQA: 0.179
- id: nvidia/Mistral-NeMo-Minitron-8B-Instruct
  name: Mistral-NeMo-Minitron 8B Instruct
  date: '2024-08-21'
  params: 8.4
  base_model: Mistral-NeMo-Minitron 8B
  sortkey: mistral nemo minitron
  url: https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.446
      JComQA: 0.892
      JEMHopQA: 0.498
      NIILC: 0.38
      JSQuAD: 0.578
      XL-Sum: 0.157
      MGSM: 0.556
      WMT20-en-ja: 0.199
      WMT20-ja-en: 0.193
      JMMLU: 0.51
      JHumanEval: 0.496
    ja_mtb:
      JMT Avg: 0.567
      coding: 0.547
      extraction: 0.684
      humanities: 0.649
      math: 0.545
      reasoning: 0.454
      roleplay: 0.564
      stem: 0.549
      writing: 0.541
    en_basic:
      En Avg: 0.634
      OpenBookQA: 0.452
      TriviaQA: 0.719
      HellaSwag: 0.639
      SQuAD2: 0.624
      XWINO: 0.909
      MMLU: 0.701
      GSM8K: 0.754
      MATH: 0.274
      BBH: 0.663
      HumanEval: 0.601
    other:
      GPQA: 0.283
- id: mistralai/Mistral-7B-v0.3
  name: Mistral-7B-v0.3
  date: '2024-05-22'
  params: 7.2
  base_model: ''
  sortkey: mistral v0.3
  url: https://huggingface.co/mistralai/Mistral-7B-v0.3
  results:
    ja_basic:
      Ja Avg: 0.361
      JComQA: 0.714
      JEMHopQA: 0.474
      NIILC: 0.245
      JSQuAD: 0.847
      XL-Sum: 0.212
      MGSM: 0.156
      WMT20-en-ja: 0.142
      WMT20-ja-en: 0.171
      JMMLU: 0.404
      JHumanEval: 0.242
    en_basic:
      En Avg: 0.507
      OpenBookQA: 0.374
      TriviaQA: 0.695
      HellaSwag: 0.622
      SQuAD2: 0.511
      XWINO: 0.909
      MMLU: 0.623
      GSM8K: 0.361
      MATH: 0.116
      BBH: 0.585
      HumanEval: 0.273
    other:
      GPQA: 0.129
- id: mistralai/Mistral-7B-Instruct-v0.3
  name: Mistral-7B-Instruct-v0.3
  date: '2024-05-22'
  params: 7.2
  base_model: Mistral-7B-v0.3
  sortkey: mistral v0.3
  url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
  results:
    ja_basic:
      Ja Avg: 0.378
      JComQA: 0.754
      JEMHopQA: 0.447
      NIILC: 0.268
      JSQuAD: 0.87
      XL-Sum: 0.205
      MGSM: 0.224
      WMT20-en-ja: 0.163
      WMT20-ja-en: 0.177
      JMMLU: 0.403
      JHumanEval: 0.266
    ja_mtb:
      JMT Avg: 0.428
      coding: 0.488
      extraction: 0.54
      humanities: 0.435
      math: 0.354
      reasoning: 0.392
      roleplay: 0.409
      stem: 0.405
      writing: 0.401
    en_basic:
      En Avg: 0.541
      OpenBookQA: 0.408
      TriviaQA: 0.677
      HellaSwag: 0.652
      SQuAD2: 0.576
      XWINO: 0.905
      MMLU: 0.621
      GSM8K: 0.5
      MATH: 0.16
      BBH: 0.563
      HumanEval: 0.346
    other:
      GPQA: 0.277
- id: mistralai/Mixtral-8x22B-v0.1
  name: Mixtral-8x22B-v0.1
  date: '2024-04-17'
  params: 141
  base_model: ''
  sortkey: mixtral v0.1
  url: https://huggingface.co/mistralai/Mixtral-8x22B-v0.1
  results:
    ja_basic:
      Ja Avg: 0.496
      JComQA: 0.895
      JEMHopQA: 0.512
      NIILC: 0.42
      JSQuAD: 0.914
      XL-Sum: 0.241
      MGSM: 0.544
      WMT20-en-ja: 0.229
      WMT20-ja-en: 0.229
      JMMLU: 0.604
      JHumanEval: 0.371
    en_basic:
      En Avg: 0.652
      OpenBookQA: 0.42
      TriviaQA: 0.833
      HellaSwag: 0.696
      SQuAD2: 0.593
      XWINO: 0.919
      MMLU: 0.772
      GSM8K: 0.754
      MATH: 0.414
      BBH: 0.811
      HumanEval: 0.309
    other:
      GPQA: 0.19
- id: mistralai/Mixtral-8x22B-Instruct-v0.1
  name: Mixtral-8x22B-Instruct-v0.1
  date: '2024-04-17'
  params: 141
  base_model: Mixtral-8x22B-v0.1
  sortkey: mixtral v0.1
  url: https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.532
      JComQA: 0.903
      JEMHopQA: 0.498
      NIILC: 0.446
      JSQuAD: 0.918
      XL-Sum: 0.207
      MGSM: 0.696
      WMT20-en-ja: 0.233
      WMT20-ja-en: 0.232
      JMMLU: 0.602
      JHumanEval: 0.588
    ja_mtb:
      JMT Avg: 0.622
      coding: 0.591
      extraction: 0.797
      humanities: 0.606
      math: 0.585
      reasoning: 0.557
      roleplay: 0.618
      stem: 0.565
      writing: 0.658
    en_basic:
      En Avg: 0.72
      OpenBookQA: 0.45
      TriviaQA: 0.827
      HellaSwag: 0.708
      SQuAD2: 0.676
      XWINO: 0.92
      MMLU: 0.774
      GSM8K: 0.832
      MATH: 0.456
      BBH: 0.83
      HumanEval: 0.723
    other:
      GPQA: 0.335
- id: o1-2024-12-17
  name: o1 (o1-2024-12-17)
  date: '2024-08-06'
  params: 0
  base_model: (private)
  sortkey: o1 (o1 2024 12 17)
  url: https://huggingface.co/o1-2024-12-17
  results:
    ja_basic:
      Ja Avg: -0.01
      JComQA: -0.01
      JEMHopQA: -0.01
      NIILC: -0.01
      JSQuAD: -0.01
      XL-Sum: -0.01
      MGSM: -0.01
      WMT20-en-ja: -0.01
      WMT20-ja-en: -0.01
      JMMLU: -0.01
      JHumanEval: -0.01
    ja_mtb:
      JMT Avg: 0.807
      coding: 0.784
      extraction: 0.782
      humanities: 0.853
      math: 0.973
      reasoning: 0.618
      roleplay: 0.828
      stem: 0.798
      writing: 0.817
    en_basic:
      En Avg: -0.01
      OpenBookQA: -0.01
      TriviaQA: -0.01
      HellaSwag: -0.01
      SQuAD2: -0.01
      XWINO: -0.01
      MMLU: -0.01
      GSM8K: -0.01
      MATH: -0.01
      BBH: -0.01
      HumanEval: -0.01
    other:
      GPQA: -0.001
- id: microsoft/Phi-3-mini-128k-instruct
  name: Phi-3-Mini-128K-Instruct
  date: '2024-04-23'
  params: 3.8
  base_model: (private)
  sortkey: phi 3 mini 128k
  url: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
  results:
    ja_basic:
      Ja Avg: 0.382
      JComQA: 0.72
      JEMHopQA: 0.394
      NIILC: 0.208
      JSQuAD: 0.832
      XL-Sum: 0.132
      MGSM: 0.408
      WMT20-en-ja: 0.15
      WMT20-ja-en: 0.136
      JMMLU: 0.409
      JHumanEval: 0.428
    ja_mtb:
      JMT Avg: 0.524
      coding: 0.535
      extraction: 0.68
      humanities: 0.553
      math: 0.514
      reasoning: 0.416
      roleplay: 0.505
      stem: 0.465
      writing: 0.525
    en_basic:
      En Avg: 0.614
      OpenBookQA: 0.422
      TriviaQA: 0.526
      HellaSwag: 0.605
      SQuAD2: 0.559
      XWINO: 0.871
      MMLU: 0.695
      GSM8K: 0.759
      MATH: 0.368
      BBH: 0.711
      HumanEval: 0.627
    other:
      GPQA: 0.275
- id: microsoft/phi-4
  name: Phi-4
  date: '2024-12-13'
  params: 14
  base_model: (private)
  sortkey: phi 4
  url: https://huggingface.co/microsoft/phi-4
  results:
    ja_basic:
      Ja Avg: 0.58
      JComQA: 0.945
      JEMHopQA: 0.608
      NIILC: 0.507
      JSQuAD: 0.923
      XL-Sum: 0.219
      MGSM: 0.796
      WMT20-en-ja: 0.283
      WMT20-ja-en: 0.231
      JMMLU: 0.689
      JHumanEval: 0.598
    ja_mtb:
      JMT Avg: 0.769
      coding: 0.692
      extraction: 0.929
      humanities: 0.795
      math: 0.914
      reasoning: 0.544
      roleplay: 0.754
      stem: 0.688
      writing: 0.84
    en_basic:
      En Avg: 0.677
      OpenBookQA: 0.378
      TriviaQA: 0.682
      HellaSwag: 0.647
      SQuAD2: 0.646
      XWINO: 0.903
      MMLU: 0.802
      GSM8K: 0.899
      MATH: 0.556
      BBH: 0.654
      HumanEval: 0.601
    other:
      GPQA: 0.0
- id: pfnet/plamo-2-1b
  name: PLaMo 2 1B
  date: '2025-02-21'
  params: 1.3
  base_model: ''
  sortkey: plamo 2
  url: https://huggingface.co/pfnet/plamo-2-1b
  results:
    ja_basic:
      Ja Avg: 0.25
      JComQA: 0.203
      JEMHopQA: 0.463
      NIILC: 0.434
      JSQuAD: 0.626
      XL-Sum: 0.055
      MGSM: 0.052
      WMT20-en-ja: 0.236
      WMT20-ja-en: 0.119
      JMMLU: 0.256
      JHumanEval: 0.057
    en_basic:
      En Avg: 0.274
      OpenBookQA: 0.28
      TriviaQA: 0.129
      HellaSwag: 0.425
      SQuAD2: 0.501
      XWINO: 0.807
      MMLU: 0.294
      GSM8K: 0.072
      MATH: 0.034
      BBH: 0.122
      HumanEval: 0.08
    other:
      GPQA: 0.007
- id: pfnet/plamo-2-8b
  name: PLaMo 2 8B
  date: '2025-02-21'
  params: 9.1
  base_model: ''
  sortkey: plamo 2
  url: https://huggingface.co/pfnet/plamo-2-8b
  results:
    ja_basic:
      Ja Avg: 0.481
      JComQA: 0.909
      JEMHopQA: 0.474
      NIILC: 0.655
      JSQuAD: 0.91
      XL-Sum: 0.12
      MGSM: 0.508
      WMT20-en-ja: 0.28
      WMT20-ja-en: 0.205
      JMMLU: 0.536
      JHumanEval: 0.213
    en_basic:
      En Avg: 0.474
      OpenBookQA: 0.346
      TriviaQA: 0.584
      HellaSwag: 0.56
      SQuAD2: 0.511
      XWINO: 0.89
      MMLU: 0.575
      GSM8K: 0.55
      MATH: 0.2
      BBH: 0.26
      HumanEval: 0.26
    other:
      GPQA: 0.022
- id: Qwen/Qwen2-7B
  name: Qwen2-7B
  date: '2024-06-07'
  params: 7.6
  base_model: ''
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-7B
  results:
    ja_basic:
      Ja Avg: 0.472
      JComQA: 0.875
      JEMHopQA: 0.463
      NIILC: 0.372
      JSQuAD: 0.899
      XL-Sum: 0.172
      MGSM: 0.524
      WMT20-en-ja: 0.209
      WMT20-ja-en: 0.195
      JMMLU: 0.587
      JHumanEval: 0.422
    en_basic:
      En Avg: 0.602
      OpenBookQA: 0.374
      TriviaQA: 0.61
      HellaSwag: 0.602
      SQuAD2: 0.574
      XWINO: 0.891
      MMLU: 0.705
      GSM8K: 0.781
      MATH: 0.492
      BBH: 0.53
      HumanEval: 0.46
    other:
      GPQA: 0.246
- id: Qwen/Qwen2-7B-Instruct
  name: Qwen2-7B-Instruct
  date: '2024-06-07'
  params: 7.6
  base_model: Qwen2-7B
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.478
      JComQA: 0.888
      JEMHopQA: 0.39
      NIILC: 0.379
      JSQuAD: 0.897
      XL-Sum: 0.126
      MGSM: 0.576
      WMT20-en-ja: 0.206
      WMT20-ja-en: 0.19
      JMMLU: 0.571
      JHumanEval: 0.555
    ja_mtb:
      JMT Avg: 0.646
      coding: 0.512
      extraction: 0.771
      humanities: 0.719
      math: 0.687
      reasoning: 0.514
      roleplay: 0.683
      stem: 0.563
      writing: 0.717
    en_basic:
      En Avg: 0.582
      OpenBookQA: 0.396
      TriviaQA: 0.547
      HellaSwag: 0.615
      SQuAD2: 0.593
      XWINO: 0.886
      MMLU: 0.707
      GSM8K: 0.626
      MATH: 0.504
      BBH: 0.304
      HumanEval: 0.643
    other:
      GPQA: 0.105
- id: Qwen/Qwen2-72B
  name: Qwen2-72B
  date: '2024-06-07'
  params: 72
  base_model: ''
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-72B
  results:
    ja_basic:
      Ja Avg: 0.593
      JComQA: 0.96
      JEMHopQA: 0.62
      NIILC: 0.561
      JSQuAD: 0.926
      XL-Sum: 0.238
      MGSM: 0.768
      WMT20-en-ja: 0.275
      WMT20-ja-en: 0.241
      JMMLU: 0.782
      JHumanEval: 0.561
    en_basic:
      En Avg: 0.702
      OpenBookQA: 0.418
      TriviaQA: 0.79
      HellaSwag: 0.677
      SQuAD2: 0.673
      XWINO: 0.915
      MMLU: 0.842
      GSM8K: 0.893
      MATH: 0.56
      BBH: 0.643
      HumanEval: 0.608
    other:
      GPQA: 0.299
- id: Qwen/Qwen2-72B-Instruct
  name: Qwen2-72B-Instruct
  date: '2024-06-07'
  params: 72
  base_model: Qwen2-72B
  sortkey: qwen2
  url: https://huggingface.co/Qwen/Qwen2-72B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.598
      JComQA: 0.963
      JEMHopQA: 0.628
      NIILC: 0.557
      JSQuAD: 0.92
      XL-Sum: 0.166
      MGSM: 0.78
      WMT20-en-ja: 0.26
      WMT20-ja-en: 0.232
      JMMLU: 0.771
      JHumanEval: 0.701
    ja_mtb:
      JMT Avg: 0.756
      coding: 0.632
      extraction: 0.8
      humanities: 0.842
      math: 0.688
      reasoning: 0.616
      roleplay: 0.824
      stem: 0.797
      writing: 0.846
    en_basic:
      En Avg: 0.669
      OpenBookQA: 0.444
      TriviaQA: 0.759
      HellaSwag: 0.685
      SQuAD2: 0.685
      XWINO: 0.911
      MMLU: 0.839
      GSM8K: 0.848
      MATH: 0.634
      BBH: 0.193
      HumanEval: 0.688
    other:
      GPQA: 0.402
- id: Qwen/Qwen2.5-0.5B
  name: Qwen2.5-0.5B
  date: '2024-09-19'
  params: 0.5
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-0.5B
  results:
    ja_basic:
      Ja Avg: 0.234
      JComQA: 0.369
      JEMHopQA: 0.389
      NIILC: 0.139
      JSQuAD: 0.635
      XL-Sum: 0.101
      MGSM: 0.076
      WMT20-en-ja: 0.058
      WMT20-ja-en: 0.064
      JMMLU: 0.304
      JHumanEval: 0.203
    en_basic:
      En Avg: 0.365
      OpenBookQA: 0.266
      TriviaQA: 0.19
      HellaSwag: 0.399
      SQuAD2: 0.501
      XWINO: 0.768
      MMLU: 0.479
      GSM8K: 0.341
      MATH: 0.148
      BBH: 0.277
      HumanEval: 0.277
    other:
      GPQA: 0.094
- id: Qwen/Qwen2.5-0.5B-Instruct
  name: Qwen2.5-0.5B-Instruct
  date: '2024-09-19'
  params: 0.5
  base_model: Qwen2.5-0.5B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.243
      JComQA: 0.382
      JEMHopQA: 0.401
      NIILC: 0.157
      JSQuAD: 0.687
      XL-Sum: 0.112
      MGSM: 0.08
      WMT20-en-ja: 0.095
      WMT20-ja-en: 0.067
      JMMLU: 0.318
      JHumanEval: 0.135
    ja_mtb:
      JMT Avg: 0.294
      coding: 0.335
      extraction: 0.284
      humanities: 0.285
      math: 0.317
      reasoning: 0.248
      roleplay: 0.294
      stem: 0.279
      writing: 0.313
    en_basic:
      En Avg: 0.336
      OpenBookQA: 0.272
      TriviaQA: 0.184
      HellaSwag: 0.398
      SQuAD2: 0.501
      XWINO: 0.767
      MMLU: 0.471
      GSM8K: 0.19
      MATH: 0.236
      BBH: 0.106
      HumanEval: 0.24
    other:
      GPQA: 0.045
- id: Qwen/Qwen2.5-1.5B
  name: Qwen2.5-1.5B
  date: '2024-09-19'
  params: 1.5
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-1.5B
  results:
    ja_basic:
      Ja Avg: 0.372
      JComQA: 0.8
      JEMHopQA: 0.383
      NIILC: 0.241
      JSQuAD: 0.849
      XL-Sum: 0.143
      MGSM: 0.292
      WMT20-en-ja: 0.132
      WMT20-ja-en: 0.134
      JMMLU: 0.438
      JHumanEval: 0.308
    en_basic:
      En Avg: 0.49
      OpenBookQA: 0.342
      TriviaQA: 0.397
      HellaSwag: 0.499
      SQuAD2: 0.506
      XWINO: 0.851
      MMLU: 0.61
      GSM8K: 0.611
      MATH: 0.314
      BBH: 0.413
      HumanEval: 0.356
    other:
      GPQA: 0.045
- id: Qwen/Qwen2.5-1.5B-Instruct
  name: Qwen2.5-1.5B-Instruct
  date: '2024-09-19'
  params: 1.5
  base_model: Qwen2.5-1.5B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.355
      JComQA: 0.812
      JEMHopQA: 0.276
      NIILC: 0.241
      JSQuAD: 0.847
      XL-Sum: 0.128
      MGSM: 0.292
      WMT20-en-ja: 0.147
      WMT20-ja-en: 0.119
      JMMLU: 0.447
      JHumanEval: 0.242
    ja_mtb:
      JMT Avg: 0.45
      coding: 0.408
      extraction: 0.513
      humanities: 0.456
      math: 0.527
      reasoning: 0.352
      roleplay: 0.473
      stem: 0.406
      writing: 0.469
    en_basic:
      En Avg: 0.424
      OpenBookQA: 0.334
      TriviaQA: 0.378
      HellaSwag: 0.503
      SQuAD2: 0.501
      XWINO: 0.844
      MMLU: 0.604
      GSM8K: 0.257
      MATH: 0.272
      BBH: 0.272
      HumanEval: 0.277
    other:
      GPQA: 0.141
- id: Qwen/Qwen2.5-3B
  name: Qwen2.5-3B
  date: '2024-09-19'
  params: 3.1
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-3B
  results:
    ja_basic:
      Ja Avg: 0.442
      JComQA: 0.847
      JEMHopQA: 0.475
      NIILC: 0.306
      JSQuAD: 0.878
      XL-Sum: 0.176
      MGSM: 0.46
      WMT20-en-ja: 0.18
      WMT20-ja-en: 0.167
      JMMLU: 0.529
      JHumanEval: 0.404
    en_basic:
      En Avg: 0.534
      OpenBookQA: 0.36
      TriviaQA: 0.504
      HellaSwag: 0.553
      SQuAD2: 0.541
      XWINO: 0.872
      MMLU: 0.657
      GSM8K: 0.58
      MATH: 0.44
      BBH: 0.442
      HumanEval: 0.387
    other:
      GPQA: 0.0
- id: Qwen/Qwen2.5-3B-Instruct
  name: Qwen2.5-3B-Instruct
  date: '2024-09-19'
  params: 3.1
  base_model: Qwen2.5-3B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.409
      JComQA: 0.876
      JEMHopQA: 0.304
      NIILC: 0.293
      JSQuAD: 0.866
      XL-Sum: 0.144
      MGSM: 0.228
      WMT20-en-ja: 0.198
      WMT20-ja-en: 0.168
      JMMLU: 0.536
      JHumanEval: 0.474
    ja_mtb:
      JMT Avg: 0.593
      coding: 0.567
      extraction: 0.647
      humanities: 0.597
      math: 0.665
      reasoning: 0.457
      roleplay: 0.649
      stem: 0.526
      writing: 0.637
    en_basic:
      En Avg: 0.472
      OpenBookQA: 0.364
      TriviaQA: 0.446
      HellaSwag: 0.562
      SQuAD2: 0.504
      XWINO: 0.869
      MMLU: 0.664
      GSM8K: 0.096
      MATH: 0.612
      BBH: 0.128
      HumanEval: 0.471
    other:
      GPQA: 0.304
- id: Qwen/Qwen2.5-7B
  name: Qwen2.5-7B
  date: '2024-09-19'
  params: 7.6
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-7B
  results:
    ja_basic:
      Ja Avg: 0.512
      JComQA: 0.924
      JEMHopQA: 0.459
      NIILC: 0.426
      JSQuAD: 0.907
      XL-Sum: 0.216
      MGSM: 0.616
      WMT20-en-ja: 0.229
      WMT20-ja-en: 0.199
      JMMLU: 0.634
      JHumanEval: 0.507
    en_basic:
      En Avg: 0.63
      OpenBookQA: 0.392
      TriviaQA: 0.601
      HellaSwag: 0.6
      SQuAD2: 0.618
      XWINO: 0.888
      MMLU: 0.742
      GSM8K: 0.832
      MATH: 0.51
      BBH: 0.562
      HumanEval: 0.554
    other:
      GPQA: 0.295
- id: Qwen/Qwen2.5-7B-Instruct
  name: Qwen2.5-7B-Instruct
  date: '2024-09-19'
  params: 7.6
  base_model: Qwen2.5-7B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.498
      JComQA: 0.915
      JEMHopQA: 0.429
      NIILC: 0.391
      JSQuAD: 0.891
      XL-Sum: 0.168
      MGSM: 0.632
      WMT20-en-ja: 0.211
      WMT20-ja-en: 0.192
      JMMLU: 0.623
      JHumanEval: 0.532
    ja_mtb:
      JMT Avg: 0.665
      coding: 0.599
      extraction: 0.741
      humanities: 0.719
      math: 0.637
      reasoning: 0.541
      roleplay: 0.744
      stem: 0.624
      writing: 0.713
    en_basic:
      En Avg: 0.604
      OpenBookQA: 0.428
      TriviaQA: 0.519
      HellaSwag: 0.624
      SQuAD2: 0.569
      XWINO: 0.877
      MMLU: 0.742
      GSM8K: 0.739
      MATH: 0.688
      BBH: 0.217
      HumanEval: 0.636
    other:
      GPQA: 0.31
- id: Qwen/Qwen2.5-14B
  name: Qwen2.5-14B
  date: '2024-09-19'
  params: 14
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-14B
  results:
    ja_basic:
      Ja Avg: 0.568
      JComQA: 0.958
      JEMHopQA: 0.567
      NIILC: 0.537
      JSQuAD: 0.923
      XL-Sum: 0.225
      MGSM: 0.74
      WMT20-en-ja: 0.26
      WMT20-ja-en: 0.23
      JMMLU: 0.69
      JHumanEval: 0.55
    en_basic:
      En Avg: 0.66
      OpenBookQA: 0.412
      TriviaQA: 0.666
      HellaSwag: 0.642
      SQuAD2: 0.63
      XWINO: 0.899
      MMLU: 0.797
      GSM8K: 0.793
      MATH: 0.53
      BBH: 0.686
      HumanEval: 0.544
    other:
      GPQA: 0.214
- id: Qwen/Qwen2.5-32B
  name: Qwen2.5-32B
  date: '2024-09-19'
  params: 14
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-32B
  results:
    ja_basic:
      Ja Avg: 0.591
      JComQA: 0.961
      JEMHopQA: 0.561
      NIILC: 0.538
      JSQuAD: 0.925
      XL-Sum: 0.228
      MGSM: 0.808
      WMT20-en-ja: 0.271
      WMT20-ja-en: 0.233
      JMMLU: 0.751
      JHumanEval: 0.637
    en_basic:
      En Avg: 0.67
      OpenBookQA: 0.406
      TriviaQA: 0.664
      HellaSwag: 0.656
      SQuAD2: 0.668
      XWINO: 0.913
      MMLU: 0.832
      GSM8K: 0.718
      MATH: 0.6
      BBH: 0.717
      HumanEval: 0.523
    other:
      GPQA: 0.317
- id: Qwen/Qwen2.5-14B-Instruct
  name: Qwen2.5-14B-Instruct
  date: '2024-09-25'
  params: 14
  base_model: Qwen2.5-14B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.553
      JComQA: 0.953
      JEMHopQA: 0.588
      NIILC: 0.519
      JSQuAD: 0.902
      XL-Sum: 0.14
      MGSM: 0.68
      WMT20-en-ja: 0.192
      WMT20-ja-en: 0.16
      JMMLU: 0.708
      JHumanEval: 0.691
    ja_mtb:
      JMT Avg: 0.762
      coding: 0.673
      extraction: 0.829
      humanities: 0.798
      math: 0.828
      reasoning: 0.571
      roleplay: 0.815
      stem: 0.743
      writing: 0.841
    en_basic:
      En Avg: 0.614
      OpenBookQA: 0.438
      TriviaQA: 0.592
      HellaSwag: 0.656
      SQuAD2: 0.68
      XWINO: 0.89
      MMLU: 0.8
      GSM8K: 0.761
      MATH: 0.666
      BBH: 0.029
      HumanEval: 0.632
    other:
      GPQA: 0.379
- id: Qwen/Qwen2.5-32B-Instruct
  name: Qwen2.5-32B-Instruct
  date: '2024-09-25'
  params: 32
  base_model: Qwen2.5-32B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.571
      JComQA: 0.959
      JEMHopQA: 0.567
      NIILC: 0.497
      JSQuAD: 0.903
      XL-Sum: 0.169
      MGSM: 0.78
      WMT20-en-ja: 0.228
      WMT20-ja-en: 0.195
      JMMLU: 0.757
      JHumanEval: 0.651
    ja_mtb:
      JMT Avg: 0.809
      coding: 0.724
      extraction: 0.885
      humanities: 0.816
      math: 0.918
      reasoning: 0.726
      roleplay: 0.834
      stem: 0.763
      writing: 0.808
    en_basic:
      En Avg: 0.588
      OpenBookQA: 0.424
      TriviaQA: 0.534
      HellaSwag: 0.671
      SQuAD2: 0.536
      XWINO: 0.893
      MMLU: 0.834
      GSM8K: 0.581
      MATH: 0.802
      BBH: 0.017
      HumanEval: 0.589
    other:
      GPQA: 0.402
- id: Qwen/Qwen2.5-72B
  name: Qwen2.5-72B
  date: '2024-09-19'
  params: 72
  base_model: ''
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-72B
  results:
    ja_basic:
      Ja Avg: 0.623
      JComQA: 0.972
      JEMHopQA: 0.611
      NIILC: 0.619
      JSQuAD: 0.93
      XL-Sum: 0.279
      MGSM: 0.828
      WMT20-en-ja: 0.287
      WMT20-ja-en: 0.252
      JMMLU: 0.804
      JHumanEval: 0.648
    en_basic:
      En Avg: 0.709
      OpenBookQA: 0.416
      TriviaQA: 0.76
      HellaSwag: 0.685
      SQuAD2: 0.693
      XWINO: 0.901
      MMLU: 0.861
      GSM8K: 0.87
      MATH: 0.626
      BBH: 0.727
      HumanEval: 0.554
    other:
      GPQA: 0.248
- id: Qwen/Qwen2.5-72B-Instruct
  name: Qwen2.5-72B-Instruct
  date: '2024-09-19'
  params: 72
  base_model: Qwen2.5-72B
  sortkey: qwen2.5
  url: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.574
      JComQA: 0.97
      JEMHopQA: 0.569
      NIILC: 0.582
      JSQuAD: 0.738
      XL-Sum: 0.17
      MGSM: 0.84
      WMT20-en-ja: 0.227
      WMT20-ja-en: 0.218
      JMMLU: 0.789
      JHumanEval: 0.634
    ja_mtb:
      JMT Avg: 0.835
      coding: 0.795
      extraction: 0.86
      humanities: 0.865
      math: 0.857
      reasoning: 0.784
      roleplay: 0.863
      stem: 0.804
      writing: 0.854
    en_basic:
      En Avg: 0.691
      OpenBookQA: 0.454
      TriviaQA: 0.676
      HellaSwag: 0.706
      SQuAD2: 0.677
      XWINO: 0.889
      MMLU: 0.848
      GSM8K: 0.904
      MATH: 0.77
      BBH: 0.375
      HumanEval: 0.614
    other:
      GPQA: 0.464
- id: sbintuitions/sarashina2-7b
  name: Sarashina2-7B
  date: '2024-06-14'
  params: 7.3
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-7b
  results:
    ja_basic:
      Ja Avg: 0.395
      JComQA: 0.742
      JEMHopQA: 0.509
      NIILC: 0.634
      JSQuAD: 0.868
      XL-Sum: 0.141
      MGSM: 0.08
      WMT20-en-ja: 0.273
      WMT20-ja-en: 0.201
      JMMLU: 0.384
      JHumanEval: 0.121
    en_basic:
      En Avg: 0.383
      OpenBookQA: 0.346
      TriviaQA: 0.479
      HellaSwag: 0.532
      SQuAD2: 0.501
      XWINO: 0.892
      MMLU: 0.425
      GSM8K: 0.101
      MATH: 0.034
      BBH: 0.373
      HumanEval: 0.146
    other:
      GPQA: 0.078
- id: sbintuitions/sarashina2-13b
  name: Sarashina2-13B
  date: '2024-06-14'
  params: 13
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-13b
  results:
    ja_basic:
      Ja Avg: 0.445
      JComQA: 0.85
      JEMHopQA: 0.557
      NIILC: 0.661
      JSQuAD: 0.898
      XL-Sum: 0.158
      MGSM: 0.188
      WMT20-en-ja: 0.284
      WMT20-ja-en: 0.221
      JMMLU: 0.473
      JHumanEval: 0.161
    en_basic:
      En Avg: 0.418
      OpenBookQA: 0.34
      TriviaQA: 0.548
      HellaSwag: 0.562
      SQuAD2: 0.501
      XWINO: 0.896
      MMLU: 0.496
      GSM8K: 0.158
      MATH: 0.036
      BBH: 0.442
      HumanEval: 0.198
    other:
      GPQA: 0.02
- id: sbintuitions/sarashina2-70b
  name: Sarashina2-70B
  date: '2024-06-14'
  params: 70
  base_model: ''
  sortkey: sarashina2
  url: https://huggingface.co/sbintuitions/sarashina2-70b
  results:
    ja_basic:
      Ja Avg: 0.53
      JComQA: 0.929
      JEMHopQA: 0.717
      NIILC: 0.668
      JSQuAD: 0.929
      XL-Sum: 0.19
      MGSM: 0.488
      WMT20-en-ja: 0.313
      WMT20-ja-en: 0.243
      JMMLU: 0.592
      JHumanEval: 0.235
    en_basic:
      En Avg: 0.491
      OpenBookQA: 0.388
      TriviaQA: 0.537
      HellaSwag: 0.628
      SQuAD2: 0.675
      XWINO: 0.917
      MMLU: 0.63
      GSM8K: 0.011
      MATH: 0.206
      BBH: 0.639
      HumanEval: 0.281
    other:
      GPQA: 0.163
- id: stockmark/stockmark-100b
  name: Stockmark-100b
  date: '2024-05-16'
  params: 100
  base_model: ''
  sortkey: stockmark
  url: https://huggingface.co/stockmark/stockmark-100b
  results:
    ja_basic:
      Ja Avg: 0.238
      JComQA: 0.205
      JEMHopQA: 0.408
      NIILC: 0.557
      JSQuAD: 0.558
      XL-Sum: 0.062
      MGSM: 0.008
      WMT20-en-ja: 0.203
      WMT20-ja-en: 0.118
      JMMLU: 0.235
      JHumanEval: 0.032
    en_basic:
      En Avg: 0.302
      OpenBookQA: 0.278
      TriviaQA: 0.366
      HellaSwag: 0.458
      SQuAD2: 0.501
      XWINO: 0.82
      MMLU: 0.258
      GSM8K: 0.017
      MATH: 0.014
      BBH: 0.259
      HumanEval: 0.046
    other:
      GPQA: 0.0
- id: tokyotech-llm/Swallow-7b-hf
  name: Swallow 7B
  date: '2023-12-19'
  params: 6.7
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-7b-hf
  results:
    ja_basic:
      Ja Avg: 0.346
      JComQA: 0.483
      JEMHopQA: 0.511
      NIILC: 0.585
      JSQuAD: 0.847
      XL-Sum: 0.182
      MGSM: 0.108
      WMT20-en-ja: 0.25
      WMT20-ja-en: 0.149
      JMMLU: 0.324
      JHumanEval: 0.018
    en_basic:
      En Avg: 0.363
      OpenBookQA: 0.312
      TriviaQA: 0.491
      HellaSwag: 0.527
      SQuAD2: 0.501
      XWINO: 0.885
      MMLU: 0.391
      GSM8K: 0.103
      MATH: 0.02
      BBH: 0.354
      HumanEval: 0.041
    other:
      GPQA: 0.013
- id: tokyotech-llm/Swallow-13b-hf
  name: Swallow 13B
  date: '2023-12-19'
  params: 13
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-13b-hf
  results:
    ja_basic:
      Ja Avg: 0.415
      JComQA: 0.764
      JEMHopQA: 0.507
      NIILC: 0.643
      JSQuAD: 0.893
      XL-Sum: 0.215
      MGSM: 0.208
      WMT20-en-ja: 0.272
      WMT20-ja-en: 0.178
      JMMLU: 0.439
      JHumanEval: 0.027
    en_basic:
      En Avg: 0.412
      OpenBookQA: 0.344
      TriviaQA: 0.58
      HellaSwag: 0.56
      SQuAD2: 0.502
      XWINO: 0.902
      MMLU: 0.501
      GSM8K: 0.197
      MATH: 0.024
      BBH: 0.43
      HumanEval: 0.08
    other:
      GPQA: 0.116
- id: tokyotech-llm/Swallow-70b-hf
  name: Swallow 70B
  date: '2023-12-19'
  params: 70
  base_model: ''
  sortkey: swallow
  url: https://huggingface.co/tokyotech-llm/Swallow-70b-hf
  results:
    ja_basic:
      Ja Avg: 0.519
      JComQA: 0.92
      JEMHopQA: 0.626
      NIILC: 0.689
      JSQuAD: 0.92
      XL-Sum: 0.225
      MGSM: 0.48
      WMT20-en-ja: 0.304
      WMT20-ja-en: 0.231
      JMMLU: 0.579
      JHumanEval: 0.22
    en_basic:
      En Avg: 0.543
      OpenBookQA: 0.416
      TriviaQA: 0.761
      HellaSwag: 0.643
      SQuAD2: 0.522
      XWINO: 0.92
      MMLU: 0.659
      GSM8K: 0.503
      MATH: 0.108
      BBH: 0.655
      HumanEval: 0.24
    other:
      GPQA: 0.098
- id: tokyotech-llm/Swallow-MS-7b-v0.1
  name: Swallow-MS 7B v0.1
  date: '2024-03-11'
  params: 7.2
  base_model: ''
  sortkey: swallow ms v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MS-7b-v0.1
  results:
    ja_basic:
      Ja Avg: 0.439
      JComQA: 0.873
      JEMHopQA: 0.517
      NIILC: 0.572
      JSQuAD: 0.879
      XL-Sum: 0.197
      MGSM: 0.244
      WMT20-en-ja: 0.251
      WMT20-ja-en: 0.167
      JMMLU: 0.459
      JHumanEval: 0.232
    en_basic:
      En Avg: 0.461
      OpenBookQA: 0.352
      TriviaQA: 0.599
      HellaSwag: 0.579
      SQuAD2: 0.501
      XWINO: 0.901
      MMLU: 0.548
      GSM8K: 0.268
      MATH: 0.096
      BBH: 0.491
      HumanEval: 0.27
    other:
      GPQA: 0.08
- id: tokyotech-llm/Swallow-MS-7b-instruct-v0.1
  name: Swallow-MS-7b-instruct-v0.1
  date: '2024-03-11'
  params: 7.2
  base_model: Swallow-MS 7B v0.1
  sortkey: swallow ms v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MS-7b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.394
      JComQA: 0.758
      JEMHopQA: 0.49
      NIILC: 0.446
      JSQuAD: 0.864
      XL-Sum: 0.158
      MGSM: 0.172
      WMT20-en-ja: 0.227
      WMT20-ja-en: 0.187
      JMMLU: 0.419
      JHumanEval: 0.215
    ja_mtb:
      JMT Avg: 0.4
      coding: 0.358
      extraction: 0.421
      humanities: 0.501
      math: 0.222
      reasoning: 0.349
      roleplay: 0.458
      stem: 0.444
      writing: 0.449
    en_basic:
      En Avg: 0.436
      OpenBookQA: 0.36
      TriviaQA: 0.5
      HellaSwag: 0.587
      SQuAD2: 0.51
      XWINO: 0.886
      MMLU: 0.526
      GSM8K: 0.215
      MATH: 0.082
      BBH: 0.441
      HumanEval: 0.255
    other:
      GPQA: 0.201
- id: tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1
  name: Swallow-MX 8x7B v0.1
  date: '2024-03-11'
  params: 47
  base_model: ''
  sortkey: swallow mx v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1
  results:
    ja_basic:
      Ja Avg: 0.506
      JComQA: 0.922
      JEMHopQA: 0.533
      NIILC: 0.577
      JSQuAD: 0.917
      XL-Sum: 0.263
      MGSM: 0.444
      WMT20-en-ja: 0.272
      WMT20-ja-en: 0.209
      JMMLU: 0.565
      JHumanEval: 0.358
    en_basic:
      En Avg: 0.589
      OpenBookQA: 0.348
      TriviaQA: 0.773
      HellaSwag: 0.651
      SQuAD2: 0.538
      XWINO: 0.919
      MMLU: 0.692
      GSM8K: 0.574
      MATH: 0.298
      BBH: 0.686
      HumanEval: 0.41
    other:
      GPQA: 0.103
- id: tokyotech-llm/Swallow-7b-instruct-v0.1
  name: Swallow-7b-instruct-v0.1
  date: '2023-12-19'
  params: 6.7
  base_model: Swallow 7B
  sortkey: swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.353
      JComQA: 0.599
      JEMHopQA: 0.491
      NIILC: 0.531
      JSQuAD: 0.837
      XL-Sum: 0.153
      MGSM: 0.128
      WMT20-en-ja: 0.228
      WMT20-ja-en: 0.179
      JMMLU: 0.352
      JHumanEval: 0.027
    ja_mtb:
      JMT Avg: 0.419
      coding: 0.324
      extraction: 0.401
      humanities: 0.519
      math: 0.275
      reasoning: 0.344
      roleplay: 0.535
      stem: 0.494
      writing: 0.462
    en_basic:
      En Avg: 0.376
      OpenBookQA: 0.33
      TriviaQA: 0.481
      HellaSwag: 0.55
      SQuAD2: 0.501
      XWINO: 0.88
      MMLU: 0.407
      GSM8K: 0.124
      MATH: 0.034
      BBH: 0.359
      HumanEval: 0.094
    other:
      GPQA: 0.118
- id: tokyotech-llm/Swallow-70b-instruct-v0.1
  name: Swallow-70b-instruct-v0.1
  date: '2023-12-19'
  params: 70
  base_model: Swallow 70B
  sortkey: swallow v0.1
  url: https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1
  results:
    ja_basic:
      Ja Avg: 0.492
      JComQA: 0.923
      JEMHopQA: 0.566
      NIILC: 0.565
      JSQuAD: 0.903
      XL-Sum: 0.186
      MGSM: 0.42
      WMT20-en-ja: 0.263
      WMT20-ja-en: 0.232
      JMMLU: 0.571
      JHumanEval: 0.293
    ja_mtb:
      JMT Avg: 0.509
      coding: 0.381
      extraction: 0.604
      humanities: 0.568
      math: 0.464
      reasoning: 0.402
      roleplay: 0.583
      stem: 0.557
      writing: 0.51
    en_basic:
      En Avg: 0.556
      OpenBookQA: 0.446
      TriviaQA: 0.742
      HellaSwag: 0.656
      SQuAD2: 0.571
      XWINO: 0.917
      MMLU: 0.668
      GSM8K: 0.509
      MATH: 0.108
      BBH: 0.664
      HumanEval: 0.28
    other:
      GPQA: 0.268
- id: weblab-GENIAC/Tanuki-8B-dpo-v1.0
  name: Tanuki-8B-dpo-v1.0
  date: '2024-08-30'
  params: 7.5
  base_model: (private)
  sortkey: tanuki dpo v1.0
  url: https://huggingface.co/weblab-GENIAC/Tanuki-8B-dpo-v1.0
  results:
    ja_basic:
      Ja Avg: 0.311
      JComQA: 0.278
      JEMHopQA: 0.284
      NIILC: 0.37
      JSQuAD: 0.67
      XL-Sum: 0.102
      MGSM: 0.428
      WMT20-en-ja: 0.238
      WMT20-ja-en: 0.183
      JMMLU: 0.306
      JHumanEval: 0.251
    ja_mtb:
      JMT Avg: 0.529
      coding: 0.461
      extraction: 0.597
      humanities: 0.562
      math: 0.495
      reasoning: 0.377
      roleplay: 0.589
      stem: 0.509
      writing: 0.643
    en_basic:
      En Avg: 0.406
      OpenBookQA: 0.334
      TriviaQA: 0.283
      HellaSwag: 0.469
      SQuAD2: 0.501
      XWINO: 0.816
      MMLU: 0.377
      GSM8K: 0.487
      MATH: 0.178
      BBH: 0.333
      HumanEval: 0.288
    other:
      GPQA: 0.136
- id: weblab-GENIAC/Tanuki-8x8B-dpo-v1.0
  name: Tanuki-8x8B-dpo-v1.0
  date: '2024-08-30'
  params: 47
  base_model: (private)
  sortkey: tanuki dpo v1.0
  url: https://huggingface.co/weblab-GENIAC/Tanuki-8x8B-dpo-v1.0
  results:
    ja_basic:
      Ja Avg: 0.454
      JComQA: 0.708
      JEMHopQA: 0.551
      NIILC: 0.612
      JSQuAD: 0.867
      XL-Sum: 0.142
      MGSM: 0.456
      WMT20-en-ja: 0.269
      WMT20-ja-en: 0.208
      JMMLU: 0.439
      JHumanEval: 0.284
    ja_mtb:
      JMT Avg: 0.546
      coding: 0.513
      extraction: 0.489
      humanities: 0.624
      math: 0.557
      reasoning: 0.445
      roleplay: 0.604
      stem: 0.547
      writing: 0.594
    en_basic:
      En Avg: 0.464
      OpenBookQA: 0.348
      TriviaQA: 0.481
      HellaSwag: 0.555
      SQuAD2: 0.521
      XWINO: 0.85
      MMLU: 0.493
      GSM8K: 0.544
      MATH: 0.236
      BBH: 0.419
      HumanEval: 0.193
    other:
      GPQA: 0.158
- id: SakanaAI/TinySwallow-1.5B
  name: TinySwallow-1.5B
  date: '2025-01-30'
  params: 1.5
  base_model: ''
  sortkey: tinyswallow
  url: https://huggingface.co/SakanaAI/TinySwallow-1.5B
  results:
    ja_basic:
      Ja Avg: 0.402
      JComQA: 0.84
      JEMHopQA: 0.437
      NIILC: 0.474
      JSQuAD: 0.839
      XL-Sum: 0.173
      MGSM: 0.256
      WMT20-en-ja: 0.201
      WMT20-ja-en: 0.125
      JMMLU: 0.446
      JHumanEval: 0.231
    en_basic:
      En Avg: 0.413
      OpenBookQA: 0.308
      TriviaQA: 0.332
      HellaSwag: 0.468
      SQuAD2: 0.501
      XWINO: 0.85
      MMLU: 0.546
      GSM8K: 0.379
      MATH: 0.162
      BBH: 0.328
      HumanEval: 0.254
    other:
      GPQA: 0.011
- id: SakanaAI/TinySwallow-1.5B-Instruct
  name: TinySwallow-1.5B-Instruct
  date: '2025-01-30'
  params: 1.5
  base_model: TinySwallow-1.5B
  sortkey: tinyswallow
  url: https://huggingface.co/SakanaAI/TinySwallow-1.5B-Instruct
  results:
    ja_basic:
      Ja Avg: 0.398
      JComQA: 0.803
      JEMHopQA: 0.345
      NIILC: 0.447
      JSQuAD: 0.856
      XL-Sum: 0.159
      MGSM: 0.308
      WMT20-en-ja: 0.203
      WMT20-ja-en: 0.143
      JMMLU: 0.461
      JHumanEval: 0.251
    ja_mtb:
      JMT Avg: 0.565
      coding: 0.434
      extraction: 0.572
      humanities: 0.772
      math: 0.453
      reasoning: 0.392
      roleplay: 0.645
      stem: 0.61
      writing: 0.643
    en_basic:
      En Avg: 0.411
      OpenBookQA: 0.31
      TriviaQA: 0.309
      HellaSwag: 0.487
      SQuAD2: 0.501
      XWINO: 0.843
      MMLU: 0.56
      GSM8K: 0.398
      MATH: 0.162
      BBH: 0.25
      HumanEval: 0.294
    other:
      GPQA: 0.013
- id: 01-ai/Yi-1.5-6B
  name: Yi-1.5 6B
  date: '2024-05-13'
  params: 6.1
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-6B
  results:
    ja_basic:
      Ja Avg: 0.354
      JComQA: 0.658
      JEMHopQA: 0.38
      NIILC: 0.226
      JSQuAD: 0.829
      XL-Sum: 0.198
      MGSM: 0.24
      WMT20-en-ja: 0.13
      WMT20-ja-en: 0.147
      JMMLU: 0.423
      JHumanEval: 0.313
    en_basic:
      En Avg: 0.54
      OpenBookQA: 0.344
      TriviaQA: 0.593
      HellaSwag: 0.575
      SQuAD2: 0.651
      XWINO: 0.898
      MMLU: 0.636
      GSM8K: 0.522
      MATH: 0.244
      BBH: 0.583
      HumanEval: 0.352
    other:
      GPQA: 0.266
- id: 01-ai/Yi-1.5-9B
  name: Yi-1.5 9B
  date: '2024-05-13'
  params: 8.8
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-9B
  results:
    ja_basic:
      Ja Avg: 0.432
      JComQA: 0.834
      JEMHopQA: 0.417
      NIILC: 0.265
      JSQuAD: 0.894
      XL-Sum: 0.224
      MGSM: 0.42
      WMT20-en-ja: 0.174
      WMT20-ja-en: 0.187
      JMMLU: 0.516
      JHumanEval: 0.391
    en_basic:
      En Avg: 0.592
      OpenBookQA: 0.39
      TriviaQA: 0.619
      HellaSwag: 0.601
      SQuAD2: 0.693
      XWINO: 0.902
      MMLU: 0.696
      GSM8K: 0.62
      MATH: 0.3
      BBH: 0.71
      HumanEval: 0.384
    other:
      GPQA: 0.275
- id: 01-ai/Yi-1.5-34B
  name: Yi-1.5 34B
  date: '2024-05-13'
  params: 34
  base_model: ''
  sortkey: yi 1.5
  url: https://huggingface.co/01-ai/Yi-1.5-34B
  results:
    ja_basic:
      Ja Avg: 0.468
      JComQA: 0.869
      JEMHopQA: 0.461
      NIILC: 0.332
      JSQuAD: 0.899
      XL-Sum: 0.238
      MGSM: 0.52
      WMT20-en-ja: 0.219
      WMT20-ja-en: 0.208
      JMMLU: 0.591
      JHumanEval: 0.346
    en_basic:
      En Avg: 0.65
      OpenBookQA: 0.402
      TriviaQA: 0.708
      HellaSwag: 0.662
      SQuAD2: 0.754
      XWINO: 0.91
      MMLU: 0.774
      GSM8K: 0.743
      MATH: 0.394
      BBH: 0.763
      HumanEval: 0.385
    other:
      GPQA: 0.342

