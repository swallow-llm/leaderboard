const g_models = {"Qwen/Qwen2.5-14B-Instruct": {"active_params": 15, "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-14B-Instruct", "is_post": true, "model_id": "Qwen/Qwen2.5-14B-Instruct", "name": "Qwen2.5-14B-Instruct", "params": 15, "pre_training": "Qwen2.5-14B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.865, "coding": 0.752, "extraction": 0.873, "humanities": 0.899, "math": 0.932, "reasoning": 0.861, "roleplay": 0.87, "stem": 0.894, "writing": 0.839}, "en_post": {"AIME": 0.133, "GPQA": 0.404, "HellaSwag": 0.886, "LCB": 0.215, "MATH500": 0.794, "MMLU-Pro": 0.652, "avg": 0.514}, "ja_mtb": {"avg": 0.799, "coding": 0.773, "extraction": 0.882, "humanities": 0.85, "math": 0.796, "reasoning": 0.646, "roleplay": 0.829, "stem": 0.795, "writing": 0.822}, "ja_post": {"GPQA": 0.348, "JEMHopQA": 0.553, "JHumanEval": 0.754, "MATH100": 0.768, "MMLU-ProX": 0.556, "__MIFEvalJa": 0.606, "avg": 0.596}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"}, "Qwen/Qwen2.5-32B-Instruct": {"active_params": 33, "date": "2024-09-19", "family": "Qwen2.5", "id": "Qwen/Qwen2.5-32B-Instruct", "is_post": true, "model_id": "Qwen/Qwen2.5-32B-Instruct", "name": "Qwen2.5-32B-Instruct", "params": 33, "pre_training": "Qwen2.5-32B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.869, "coding": 0.806, "extraction": 0.862, "humanities": 0.895, "math": 0.954, "reasoning": 0.817, "roleplay": 0.876, "stem": 0.89, "writing": 0.851}, "en_post": {"AIME": 0.15, "GPQA": 0.48, "HellaSwag": 0.908, "LCB": 0.27, "MATH500": 0.812, "MMLU-Pro": 0.64, "avg": 0.543}, "ja_mtb": {"avg": 0.819, "coding": 0.776, "extraction": 0.913, "humanities": 0.845, "math": 0.863, "reasoning": 0.706, "roleplay": 0.839, "stem": 0.802, "writing": 0.811}, "ja_post": {"GPQA": 0.411, "JEMHopQA": 0.604, "JHumanEval": 0.803, "MATH100": 0.768, "MMLU-ProX": 0.623, "__MIFEvalJa": 0.673, "avg": 0.642}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"}, "Qwen/Qwen2.5-7B-Instruct": {"active_params": 7.6, "date": "2024-09-19", "family": "Qwen2", "id": "Qwen/Qwen2.5-7B-Instruct", "is_post": true, "model_id": "Qwen/Qwen2.5-7B-Instruct", "name": "Qwen2.5-7B-Instruct", "params": 7.6, "pre_training": "Qwen2.5-7B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.797, "coding": 0.656, "extraction": 0.769, "humanities": 0.893, "math": 0.843, "reasoning": 0.662, "roleplay": 0.832, "stem": 0.886, "writing": 0.833}, "en_post": {"AIME": 0.1, "GPQA": 0.348, "HellaSwag": 0.82, "LCB": 0.158, "MATH500": 0.742, "MMLU-Pro": 0.554, "avg": 0.454}, "ja_mtb": {"avg": 0.688, "coding": 0.638, "extraction": 0.711, "humanities": 0.782, "math": 0.685, "reasoning": 0.494, "roleplay": 0.736, "stem": 0.73, "writing": 0.729}, "ja_post": {"GPQA": 0.315, "JEMHopQA": 0.372, "JHumanEval": 0.737, "MATH100": 0.636, "MMLU-ProX": 0.452, "__MIFEvalJa": 0.504, "avg": 0.502}}, "sortkey": "qwen2.5", "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"}, "Qwen/Qwen3-0.6B": {"active_params": 0.5, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-0.6B", "is_post": true, "model_id": "Qwen/Qwen3-0.6B", "name": "Qwen3-0.6B", "params": 0.5, "pre_training": "Qwen3-0.6B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.595, "coding": 0.376, "extraction": 0.678, "humanities": 0.673, "math": 0.803, "reasoning": 0.408, "roleplay": 0.551, "stem": 0.633, "writing": 0.637}, "en_post": {"AIME": 0.133, "GPQA": 0.283, "HellaSwag": 0.425, "LCB": 0.135, "MATH500": 0.694, "MMLU-Pro": 0.338, "avg": 0.335}, "ja_mtb": {"avg": 0.431, "coding": 0.332, "extraction": 0.423, "humanities": 0.46, "math": 0.626, "reasoning": 0.346, "roleplay": 0.418, "stem": 0.445, "writing": 0.402}, "ja_post": {"GPQA": 0.237, "JEMHopQA": 0.221, "JHumanEval": 0.408, "MATH100": 0.606, "MMLU-ProX": 0.295, "__MIFEvalJa": 0.438, "avg": 0.353}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-0.6B"}, "Qwen/Qwen3-1.7B": {"active_params": 1.5, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-1.7B", "is_post": true, "model_id": "Qwen/Qwen3-1.7B", "name": "Qwen3-1.7B", "params": 1.5, "pre_training": "Qwen3-1.7B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.779, "coding": 0.642, "extraction": 0.754, "humanities": 0.83, "math": 0.968, "reasoning": 0.686, "roleplay": 0.764, "stem": 0.785, "writing": 0.805}, "en_post": {"AIME": 0.383, "GPQA": 0.394, "HellaSwag": 0.626, "LCB": 0.315, "MATH500": 0.904, "MMLU-Pro": 0.56, "avg": 0.531}, "ja_mtb": {"avg": 0.662, "coding": 0.574, "extraction": 0.591, "humanities": 0.715, "math": 0.841, "reasoning": 0.567, "roleplay": 0.631, "stem": 0.765, "writing": 0.613}, "ja_post": {"GPQA": 0.315, "JEMHopQA": 0.23, "JHumanEval": 0.747, "MATH100": 0.859, "MMLU-ProX": 0.514, "__MIFEvalJa": 0.46, "avg": 0.533}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-1.7B"}, "Qwen/Qwen3-14B": {"active_params": 15, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-14B", "is_post": true, "model_id": "Qwen/Qwen3-14B", "name": "Qwen3-14B", "params": 15, "pre_training": "Qwen3-14B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.882, "coding": 0.843, "extraction": 0.849, "humanities": 0.904, "math": 0.971, "reasoning": 0.805, "roleplay": 0.878, "stem": 0.919, "writing": 0.89}, "en_post": {"AIME": 0.75, "GPQA": 0.611, "HellaSwag": 0.89, "LCB": 0.587, "MATH500": 0.972, "MMLU-Pro": 0.77, "avg": 0.763}, "ja_mtb": {"avg": 0.874, "coding": 0.85, "extraction": 0.839, "humanities": 0.903, "math": 0.994, "reasoning": 0.824, "roleplay": 0.839, "stem": 0.919, "writing": 0.827}, "ja_post": {"GPQA": 0.556, "JEMHopQA": 0.609, "JHumanEval": 0.91, "MATH100": 0.939, "MMLU-ProX": 0.737, "__MIFEvalJa": 0.624, "avg": 0.75}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-14B"}, "Qwen/Qwen3-235B-A22B-Instruct-2507": {"active_params": 22, "date": "2025-07-23", "family": "Qwen3", "id": "Qwen/Qwen3-235B-A22B-Instruct-2507", "is_post": true, "model_id": "Qwen/Qwen3-235B-A22B-Instruct-2507", "name": "Qwen3-235B-A22B-Instruct-2507", "params": 235, "pre_training": "(private)", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.911, "coding": 0.888, "extraction": 0.859, "humanities": 0.925, "math": 0.99, "reasoning": 0.873, "roleplay": 0.911, "stem": 0.94, "writing": 0.905}, "en_post": {"AIME": 0.767, "GPQA": 0.586, "HellaSwag": 0.94, "LCB": 0.529, "MATH500": 0.982, "MMLU-Pro": 0.824, "avg": 0.771}, "ja_mtb": {"avg": 0.915, "coding": 0.943, "extraction": 0.938, "humanities": 0.907, "math": 0.987, "reasoning": 0.826, "roleplay": 0.893, "stem": 0.933, "writing": 0.891}, "ja_post": {"GPQA": 0.701, "JEMHopQA": 0.735, "JHumanEval": 0.9, "MATH100": 0.97, "MMLU-ProX": 0.799, "__MIFEvalJa": 0.73, "avg": 0.821}}, "sortkey": "qwen3 a 2507", "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"}, "Qwen/Qwen3-235B-A22B-Thinking-2507": {"active_params": 22, "date": "2025-07-23", "family": "Qwen3", "id": "Qwen/Qwen3-235B-A22B-Thinking-2507", "is_post": true, "model_id": "Qwen/Qwen3-235B-A22B-Thinking-2507", "name": "Qwen3-235B-A22B-Thinking-2507", "params": 235, "pre_training": "(private)", "reasoning": "on", "results": {"en_mtb": {"avg": 0.922, "coding": 0.877, "extraction": 0.899, "humanities": 0.945, "math": 0.998, "reasoning": 0.886, "roleplay": 0.908, "stem": 0.952, "writing": 0.908}, "en_post": {"AIME": 0.883, "GPQA": 0.803, "HellaSwag": 0.931, "LCB": 0.692, "MATH500": 0.98, "MMLU-Pro": 0.845, "avg": 0.856}, "ja_mtb": {"avg": 0.904, "coding": 0.896, "extraction": 0.878, "humanities": 0.933, "math": 0.985, "reasoning": 0.851, "roleplay": 0.876, "stem": 0.955, "writing": 0.861}, "ja_post": {"GPQA": 0.739, "JEMHopQA": 0.651, "JHumanEval": 0.938, "MATH100": 0.97, "MMLU-ProX": 0.819, "__MIFEvalJa": 0.783, "avg": 0.823}}, "sortkey": "qwen3 a thinking 2507", "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"}, "Qwen/Qwen3-32B": {"active_params": 33, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-32B", "is_post": true, "model_id": "Qwen/Qwen3-32B", "name": "Qwen3-32B", "params": 33, "pre_training": "Qwen3-32B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.892, "coding": 0.86, "extraction": 0.91, "humanities": 0.905, "math": 0.979, "reasoning": 0.796, "roleplay": 0.899, "stem": 0.919, "writing": 0.869}, "en_post": {"AIME": 0.717, "GPQA": 0.646, "HellaSwag": 0.901, "LCB": 0.602, "MATH500": 0.964, "MMLU-Pro": 0.779, "avg": 0.768}, "ja_mtb": {"avg": 0.875, "coding": 0.794, "extraction": 0.871, "humanities": 0.871, "math": 0.997, "reasoning": 0.836, "roleplay": 0.881, "stem": 0.917, "writing": 0.83}, "ja_post": {"GPQA": 0.571, "JEMHopQA": 0.588, "JHumanEval": 0.923, "MATH100": 0.949, "MMLU-ProX": 0.746, "__MIFEvalJa": 0.681, "avg": 0.756}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-32B"}, "Qwen/Qwen3-4B": {"active_params": 3.1, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-4B", "is_post": true, "model_id": "Qwen/Qwen3-4B", "name": "Qwen3-4B", "params": 3.1, "pre_training": "Qwen3-4B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.839, "coding": 0.737, "extraction": 0.831, "humanities": 0.884, "math": 0.947, "reasoning": 0.735, "roleplay": 0.87, "stem": 0.861, "writing": 0.845}, "en_post": {"AIME": 0.6, "GPQA": 0.515, "HellaSwag": 0.79, "LCB": 0.499, "MATH500": 0.938, "MMLU-Pro": 0.69, "avg": 0.672}, "ja_mtb": {"avg": 0.797, "coding": 0.696, "extraction": 0.818, "humanities": 0.855, "math": 0.947, "reasoning": 0.729, "roleplay": 0.747, "stem": 0.826, "writing": 0.76}, "ja_post": {"GPQA": 0.44, "JEMHopQA": 0.389, "JHumanEval": 0.838, "MATH100": 0.919, "MMLU-ProX": 0.643, "__MIFEvalJa": 0.562, "avg": 0.646}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-4B"}, "Qwen/Qwen3-8B": {"active_params": 8.2, "date": "2025-04-29", "family": "Qwen3", "id": "Qwen/Qwen3-8B", "is_post": true, "model_id": "Qwen/Qwen3-8B", "name": "Qwen3-8B", "params": 8.2, "pre_training": "Qwen3-8B-Base", "reasoning": "on", "results": {"en_mtb": {"avg": 0.851, "coding": 0.804, "extraction": 0.831, "humanities": 0.892, "math": 0.98, "reasoning": 0.713, "roleplay": 0.858, "stem": 0.876, "writing": 0.854}, "en_post": {"AIME": 0.7, "GPQA": 0.561, "HellaSwag": 0.851, "LCB": 0.525, "MATH500": 0.942, "MMLU-Pro": 0.713, "avg": 0.715}, "ja_mtb": {"avg": 0.845, "coding": 0.757, "extraction": 0.834, "humanities": 0.89, "math": 0.996, "reasoning": 0.823, "roleplay": 0.829, "stem": 0.822, "writing": 0.806}, "ja_post": {"GPQA": 0.491, "JEMHopQA": 0.468, "JHumanEval": 0.869, "MATH100": 0.929, "MMLU-ProX": 0.696, "__MIFEvalJa": 0.575, "avg": 0.691}}, "sortkey": "qwen3", "url": "https://huggingface.co/Qwen/Qwen3-8B"}, "abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0": {"active_params": 33, "date": "2025-04-25", "family": "Qwen2.5", "id": "abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0", "is_post": true, "model_id": "abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0", "name": "ABEJA-QwQ32b-Reasoning-Japanese-v1.0", "params": 33, "pre_training": "ABEJA-Qwen2.5-32b-Japanese-v0.1", "reasoning": "on", "results": {"en_mtb": {"avg": 0.866, "coding": 0.808, "extraction": 0.878, "humanities": 0.899, "math": 0.951, "reasoning": 0.757, "roleplay": 0.872, "stem": 0.882, "writing": 0.881}, "en_post": {"AIME": 0.617, "GPQA": 0.606, "HellaSwag": 0.906, "LCB": 0.563, "MATH500": 0.964, "MMLU-Pro": 0.78, "avg": 0.739}, "ja_mtb": {"avg": 0.843, "coding": 0.868, "extraction": 0.893, "humanities": 0.885, "math": 0.889, "reasoning": 0.694, "roleplay": 0.848, "stem": 0.85, "writing": 0.821}, "ja_post": {"GPQA": 0.527, "JEMHopQA": 0.644, "JHumanEval": 0.866, "MATH100": 0.899, "MMLU-ProX": 0.712, "__MIFEvalJa": 0.619, "avg": 0.73}}, "sortkey": "abeja qwq reasoning japanese v1.0", "url": "https://huggingface.co/abeja/ABEJA-QwQ32b-Reasoning-Japanese-v1.0"}, "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese": {"active_params": 15, "date": "2025-01-27", "family": "Qwen2.5", "id": "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese", "is_post": true, "model_id": "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese", "name": "DeepSeek-R1-Distill-Qwen-14B-Japanese", "params": 15, "pre_training": "DeepSeek-R1-Distill-Qwen-14B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.835, "coding": 0.724, "extraction": 0.884, "humanities": 0.87, "math": 0.907, "reasoning": 0.771, "roleplay": 0.867, "stem": 0.817, "writing": 0.838}, "en_post": {"AIME": 0.433, "GPQA": 0.47, "HellaSwag": 0.823, "LCB": 0.451, "MATH500": 0.916, "MMLU-Pro": 0.679, "avg": 0.629}, "ja_mtb": {"avg": 0.771, "coding": 0.557, "extraction": 0.777, "humanities": 0.88, "math": 0.871, "reasoning": 0.664, "roleplay": 0.801, "stem": 0.859, "writing": 0.758}, "ja_post": {"GPQA": 0.4, "JEMHopQA": 0.545, "JHumanEval": 0.62, "MATH100": 0.788, "MMLU-ProX": 0.525, "__MIFEvalJa": 0.513, "avg": 0.575}}, "sortkey": "deepseek r1 distill qwen japanese", "url": "https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese"}, "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese": {"active_params": 33, "date": "2025-01-27", "family": "Qwen2.5", "id": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese", "is_post": true, "model_id": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese", "name": "DeepSeek-R1-Distill-Qwen-32B-Japanese", "params": 33, "pre_training": "DeepSeek-R1-Distill-Qwen-32B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.857, "coding": 0.73, "extraction": 0.893, "humanities": 0.894, "math": 0.964, "reasoning": 0.77, "roleplay": 0.871, "stem": 0.872, "writing": 0.861}, "en_post": {"AIME": 0.55, "GPQA": 0.576, "HellaSwag": 0.872, "LCB": 0.508, "MATH500": 0.94, "MMLU-Pro": 0.737, "avg": 0.697}, "ja_mtb": {"avg": 0.808, "coding": 0.639, "extraction": 0.813, "humanities": 0.917, "math": 0.924, "reasoning": 0.652, "roleplay": 0.842, "stem": 0.872, "writing": 0.802}, "ja_post": {"GPQA": 0.464, "JEMHopQA": 0.654, "JHumanEval": 0.68, "MATH100": 0.838, "MMLU-ProX": 0.606, "__MIFEvalJa": 0.544, "avg": 0.649}}, "sortkey": "deepseek r1 distill qwen japanese", "url": "https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese"}, "cyberagent/calm3-22b-chat": {"active_params": 22, "date": "2024-07-09", "family": "CyberAgentLM3", "id": "cyberagent/calm3-22b-chat", "is_post": true, "model_id": "cyberagent/calm3-22b-chat", "name": "CyberAgentLM3-22B-chat", "params": 22, "pre_training": "(private)", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.621, "coding": 0.467, "extraction": 0.695, "humanities": 0.828, "math": 0.479, "reasoning": 0.429, "roleplay": 0.678, "stem": 0.647, "writing": 0.747}, "en_post": {"AIME": 0.017, "GPQA": 0.288, "HellaSwag": 0.77, "LCB": 0.045, "MATH500": 0.298, "MMLU-Pro": 0.26, "avg": 0.28}, "ja_mtb": {"avg": 0.697, "coding": 0.5, "extraction": 0.733, "humanities": 0.859, "math": 0.591, "reasoning": 0.611, "roleplay": 0.791, "stem": 0.721, "writing": 0.769}, "ja_post": {"GPQA": 0.266, "JEMHopQA": 0.612, "JHumanEval": 0.443, "MATH100": 0.354, "MMLU-ProX": 0.31, "__MIFEvalJa": 0.429, "avg": 0.397}}, "sortkey": "cyberagentlm3", "url": "https://huggingface.co/cyberagent/calm3-22b-chat"}, "deepseek-ai/DeepSeek-R1-Distill-Llama-70B": {"active_params": 70, "date": "2025-01-20", "family": "Llama 3", "id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "is_post": true, "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "name": "DeepSeek-R1-Distill-Llama-70B", "params": 70, "pre_training": "Llama 3.3 70B Instruct", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.842, "coding": 0.787, "extraction": 0.931, "humanities": 0.862, "math": 0.919, "reasoning": 0.723, "roleplay": 0.85, "stem": 0.806, "writing": 0.854}, "en_post": {"AIME": 0.617, "GPQA": 0.626, "HellaSwag": 0.891, "LCB": 0.534, "MATH500": 0.936, "MMLU-Pro": 0.776, "avg": 0.73}, "ja_mtb": {"avg": 0.707, "coding": 0.551, "extraction": 0.778, "humanities": 0.838, "math": 0.78, "reasoning": 0.525, "roleplay": 0.768, "stem": 0.733, "writing": 0.681}, "ja_post": {"GPQA": 0.538, "JEMHopQA": 0.567, "JHumanEval": 0.812, "MATH100": 0.859, "MMLU-ProX": 0.642, "__MIFEvalJa": 0.558, "avg": 0.683}}, "sortkey": "deepseek r1 distill llama", "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"}, "deepseek-ai/DeepSeek-R1-Distill-Llama-8B": {"active_params": 8.0, "date": "2025-01-20", "family": "Llama 3", "id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "is_post": true, "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "name": "DeepSeek-R1-Distill-Llama-8B", "params": 8.0, "pre_training": "Llama 3.1 8B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.704, "coding": 0.398, "extraction": 0.745, "humanities": 0.825, "math": 0.827, "reasoning": 0.562, "roleplay": 0.768, "stem": 0.731, "writing": 0.775}, "en_post": {"AIME": 0.367, "GPQA": 0.46, "HellaSwag": 0.688, "LCB": 0.364, "MATH500": 0.866, "MMLU-Pro": 0.549, "avg": 0.549}, "ja_mtb": {"avg": 0.526, "coding": 0.376, "extraction": 0.625, "humanities": 0.681, "math": 0.595, "reasoning": 0.496, "roleplay": 0.483, "stem": 0.51, "writing": 0.442}, "ja_post": {"GPQA": 0.31, "JEMHopQA": 0.348, "JHumanEval": 0.721, "MATH100": 0.556, "MMLU-ProX": 0.319, "__MIFEvalJa": 0.319, "avg": 0.451}}, "sortkey": "deepseek r1 distill llama", "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"}, "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": {"active_params": 15, "date": "2025-01-20", "family": "Qwen2.5", "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "is_post": true, "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "name": "DeepSeek-R1-Distill-Qwen-14B", "params": 15, "pre_training": "Qwen2.5-14B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.775, "coding": 0.512, "extraction": 0.851, "humanities": 0.815, "math": 0.886, "reasoning": 0.745, "roleplay": 0.841, "stem": 0.75, "writing": 0.803}, "en_post": {"AIME": 0.567, "GPQA": 0.525, "HellaSwag": 0.841, "LCB": 0.486, "MATH500": 0.908, "MMLU-Pro": 0.707, "avg": 0.672}, "ja_mtb": {"avg": 0.7, "coding": 0.632, "extraction": 0.803, "humanities": 0.739, "math": 0.857, "reasoning": 0.563, "roleplay": 0.72, "stem": 0.631, "writing": 0.658}, "ja_post": {"GPQA": 0.496, "JEMHopQA": 0.508, "JHumanEval": 0.859, "MATH100": 0.737, "MMLU-ProX": 0.591, "__MIFEvalJa": 0.496, "avg": 0.638}}, "sortkey": "deepseek r1 distill qwen", "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"}, "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": {"active_params": 33, "date": "2025-01-20", "family": "Qwen2.5", "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "is_post": true, "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "name": "DeepSeek-R1-Distill-Qwen-32B", "params": 33, "pre_training": "Qwen2.5-32B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.822, "coding": 0.619, "extraction": 0.901, "humanities": 0.869, "math": 0.918, "reasoning": 0.793, "roleplay": 0.861, "stem": 0.768, "writing": 0.85}, "en_post": {"AIME": 0.567, "GPQA": 0.571, "HellaSwag": 0.885, "LCB": 0.523, "MATH500": 0.926, "MMLU-Pro": 0.737, "avg": 0.701}, "ja_mtb": {"avg": 0.753, "coding": 0.669, "extraction": 0.874, "humanities": 0.764, "math": 0.867, "reasoning": 0.606, "roleplay": 0.79, "stem": 0.738, "writing": 0.716}, "ja_post": {"GPQA": 0.536, "JEMHopQA": 0.572, "JHumanEval": 0.855, "MATH100": 0.838, "MMLU-ProX": 0.66, "__MIFEvalJa": 0.509, "avg": 0.692}}, "sortkey": "deepseek r1 distill qwen", "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"}, "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": {"active_params": 7.6, "date": "2025-01-20", "family": "Qwen2", "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", "is_post": true, "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", "name": "DeepSeek-R1-Distill-Qwen-7B", "params": 7.6, "pre_training": "Qwen2.5-Math-7B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.649, "coding": 0.481, "extraction": 0.656, "humanities": 0.708, "math": 0.762, "reasoning": 0.52, "roleplay": 0.686, "stem": 0.7, "writing": 0.677}, "en_post": {"AIME": 0.417, "GPQA": 0.495, "HellaSwag": 0.564, "LCB": 0.351, "MATH500": 0.902, "MMLU-Pro": 0.547, "avg": 0.546}, "ja_mtb": {"avg": 0.411, "coding": 0.371, "extraction": 0.572, "humanities": 0.347, "math": 0.804, "reasoning": 0.346, "roleplay": 0.275, "stem": 0.341, "writing": 0.228}, "ja_post": {"GPQA": 0.4, "JEMHopQA": 0.279, "JHumanEval": 0.674, "MATH100": 0.778, "MMLU-ProX": 0.438, "__MIFEvalJa": 0.341, "avg": 0.514}}, "sortkey": "deepseek r1 distill qwen", "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"}, "elyza/ELYZA-Thinking-1.0-Qwen-32B": {"active_params": 33, "date": "2025-05-01", "family": "Qwen2.5", "id": "elyza/ELYZA-Thinking-1.0-Qwen-32B", "is_post": true, "model_id": "elyza/ELYZA-Thinking-1.0-Qwen-32B", "name": "ELYZA-Thinking-1.0-Qwen-32B", "params": 33, "pre_training": "Qwen2.5-32B-Instruct", "reasoning": "on", "results": {"en_mtb": {"avg": 0.748, "coding": 0.77, "extraction": 0.913, "humanities": 0.754, "math": 0.912, "reasoning": 0.775, "roleplay": 0.617, "stem": 0.639, "writing": 0.606}, "en_post": {"AIME": 0.3, "GPQA": 0.576, "HellaSwag": 0.888, "LCB": 0.093, "MATH500": 0.86, "MMLU-Pro": 0.708, "avg": 0.571}, "ja_mtb": {"avg": 0.694, "coding": 0.687, "extraction": 0.824, "humanities": 0.688, "math": 0.927, "reasoning": 0.641, "roleplay": 0.583, "stem": 0.656, "writing": 0.542}, "ja_post": {"GPQA": 0.455, "JEMHopQA": 0.601, "JHumanEval": 0.162, "MATH100": 0.788, "MMLU-ProX": 0.623, "__MIFEvalJa": 0.566, "avg": 0.526}}, "sortkey": "elyza thinking 1.0 qwen", "url": "https://huggingface.co/elyza/ELYZA-Thinking-1.0-Qwen-32B"}, "google/gemma-2-27b-it": {"active_params": 27, "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-27b-it", "is_post": true, "model_id": "google/gemma-2-27b-it", "name": "Gemma 2 27B IT", "params": 27, "pre_training": "Gemma 2 27B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.8, "coding": 0.701, "extraction": 0.855, "humanities": 0.891, "math": 0.724, "reasoning": 0.702, "roleplay": 0.843, "stem": 0.827, "writing": 0.858}, "en_post": {"AIME": 0.033, "GPQA": 0.404, "HellaSwag": 0.846, "LCB": 0.23, "MATH500": 0.56, "MMLU-Pro": 0.572, "avg": 0.441}, "ja_mtb": {"avg": 0.762, "coding": 0.76, "extraction": 0.825, "humanities": 0.874, "math": 0.697, "reasoning": 0.578, "roleplay": 0.818, "stem": 0.745, "writing": 0.796}, "ja_post": {"GPQA": 0.304, "JEMHopQA": 0.561, "JHumanEval": 0.7, "MATH100": 0.505, "MMLU-ProX": 0.462, "__MIFEvalJa": 0.588, "avg": 0.506}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-27b-it"}, "google/gemma-2-2b-it": {"active_params": 2.6, "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-2b-it", "is_post": true, "model_id": "google/gemma-2-2b-it", "name": "Gemma 2 2B IT", "params": 2.6, "pre_training": "Gemma 2 2B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.718, "coding": 0.543, "extraction": 0.687, "humanities": 0.868, "math": 0.659, "reasoning": 0.609, "roleplay": 0.78, "stem": 0.78, "writing": 0.816}, "en_post": {"AIME": 0.0, "GPQA": 0.359, "HellaSwag": 0.596, "LCB": 0.034, "MATH500": 0.262, "MMLU-Pro": 0.287, "avg": 0.256}, "ja_mtb": {"avg": 0.555, "coding": 0.46, "extraction": 0.585, "humanities": 0.673, "math": 0.448, "reasoning": 0.422, "roleplay": 0.641, "stem": 0.571, "writing": 0.639}, "ja_post": {"GPQA": 0.248, "JEMHopQA": 0.321, "JHumanEval": 0.359, "MATH100": 0.202, "MMLU-ProX": 0.214, "__MIFEvalJa": 0.416, "avg": 0.269}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-2b-it"}, "google/gemma-2-9b-it": {"active_params": 9.2, "date": "2024-06-27", "family": "Gemma 2", "id": "google/gemma-2-9b-it", "is_post": true, "model_id": "google/gemma-2-9b-it", "name": "Gemma 2 9B IT", "params": 9.2, "pre_training": "Gemma 2 9B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.761, "coding": 0.624, "extraction": 0.799, "humanities": 0.893, "math": 0.682, "reasoning": 0.61, "roleplay": 0.832, "stem": 0.808, "writing": 0.841}, "en_post": {"AIME": 0.017, "GPQA": 0.369, "HellaSwag": 0.829, "LCB": 0.146, "MATH500": 0.488, "MMLU-Pro": 0.503, "avg": 0.392}, "ja_mtb": {"avg": 0.743, "coding": 0.635, "extraction": 0.816, "humanities": 0.865, "math": 0.686, "reasoning": 0.649, "roleplay": 0.784, "stem": 0.734, "writing": 0.773}, "ja_post": {"GPQA": 0.277, "JEMHopQA": 0.506, "JHumanEval": 0.583, "MATH100": 0.444, "MMLU-ProX": 0.423, "__MIFEvalJa": 0.558, "avg": 0.447}}, "sortkey": "gemma 2", "url": "https://huggingface.co/google/gemma-2-9b-it"}, "google/gemma-3-12b-it": {"active_params": 12, "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-12b-it", "is_post": true, "model_id": "google/gemma-3-12b-it", "name": "Gemma 3 12B IT", "params": 12, "pre_training": "Gemma 3 12B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.86, "coding": 0.741, "extraction": 0.862, "humanities": 0.917, "math": 0.932, "reasoning": 0.758, "roleplay": 0.891, "stem": 0.899, "writing": 0.879}, "en_post": {"AIME": 0.217, "GPQA": 0.389, "HellaSwag": 0.816, "LCB": 0.247, "MATH500": 0.862, "MMLU-Pro": 0.617, "avg": 0.524}, "ja_mtb": {"avg": 0.811, "coding": 0.784, "extraction": 0.807, "humanities": 0.88, "math": 0.858, "reasoning": 0.582, "roleplay": 0.856, "stem": 0.878, "writing": 0.844}, "ja_post": {"GPQA": 0.373, "JEMHopQA": 0.525, "JHumanEval": 0.763, "MATH100": 0.798, "MMLU-ProX": 0.527, "__MIFEvalJa": 0.619, "avg": 0.597}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-12b-it"}, "google/gemma-3-1b-it": {"active_params": 1.0, "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-1b-it", "is_post": true, "model_id": "google/gemma-3-1b-it", "name": "Gemma 3 1B IT", "params": 1.0, "pre_training": "Gemma 3 1B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.578, "coding": 0.503, "extraction": 0.453, "humanities": 0.719, "math": 0.709, "reasoning": 0.366, "roleplay": 0.634, "stem": 0.686, "writing": 0.553}, "en_post": {"AIME": 0.0, "GPQA": 0.237, "HellaSwag": 0.357, "LCB": 0.002, "MATH500": 0.438, "MMLU-Pro": 0.171, "avg": 0.201}, "ja_mtb": {"avg": 0.434, "coding": 0.396, "extraction": 0.484, "humanities": 0.519, "math": 0.343, "reasoning": 0.337, "roleplay": 0.519, "stem": 0.434, "writing": 0.436}, "ja_post": {"GPQA": 0.248, "JEMHopQA": 0.168, "JHumanEval": 0.112, "MATH100": 0.172, "MMLU-ProX": 0.148, "__MIFEvalJa": 0.323, "avg": 0.17}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-1b-it"}, "google/gemma-3-27b-it": {"active_params": 27, "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-27b-it", "is_post": true, "model_id": "google/gemma-3-27b-it", "name": "Gemma 3 27B IT", "params": 27, "pre_training": "Gemma 3 27B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.88, "coding": 0.767, "extraction": 0.919, "humanities": 0.92, "math": 0.924, "reasoning": 0.797, "roleplay": 0.908, "stem": 0.917, "writing": 0.888}, "en_post": {"AIME": 0.233, "GPQA": 0.475, "HellaSwag": 0.861, "LCB": 0.298, "MATH500": 0.88, "MMLU-Pro": 0.681, "avg": 0.571}, "ja_mtb": {"avg": 0.83, "coding": 0.747, "extraction": 0.942, "humanities": 0.878, "math": 0.808, "reasoning": 0.733, "roleplay": 0.849, "stem": 0.853, "writing": 0.831}, "ja_post": {"GPQA": 0.417, "JEMHopQA": 0.607, "JHumanEval": 0.796, "MATH100": 0.859, "MMLU-ProX": 0.609, "__MIFEvalJa": 0.597, "avg": 0.658}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-27b-it"}, "google/gemma-3-4b-it": {"active_params": 4.3, "date": "2025-03-12", "family": "Gemma 3", "id": "google/gemma-3-4b-it", "is_post": true, "model_id": "google/gemma-3-4b-it", "name": "Gemma 3 4B IT", "params": 4.3, "pre_training": "Gemma 3 4B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.793, "coding": 0.704, "extraction": 0.762, "humanities": 0.901, "math": 0.855, "reasoning": 0.553, "roleplay": 0.869, "stem": 0.845, "writing": 0.857}, "en_post": {"AIME": 0.117, "GPQA": 0.354, "HellaSwag": 0.62, "LCB": 0.151, "MATH500": 0.748, "MMLU-Pro": 0.44, "avg": 0.405}, "ja_mtb": {"avg": 0.735, "coding": 0.727, "extraction": 0.65, "humanities": 0.814, "math": 0.826, "reasoning": 0.482, "roleplay": 0.787, "stem": 0.796, "writing": 0.802}, "ja_post": {"GPQA": 0.246, "JEMHopQA": 0.45, "JHumanEval": 0.604, "MATH100": 0.606, "MMLU-ProX": 0.335, "__MIFEvalJa": 0.473, "avg": 0.448}}, "sortkey": "gemma 3", "url": "https://huggingface.co/google/gemma-3-4b-it"}, "google/medgemma-27b-it": {"active_params": 27, "date": "2025-07-09", "family": "Gemma 3", "id": "google/medgemma-27b-it", "is_post": true, "model_id": "google/medgemma-27b-it", "name": "MedGemma 27B IT", "params": 27, "pre_training": "Gemma 3 27B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.83, "coding": 0.722, "extraction": 0.914, "humanities": 0.884, "math": 0.97, "reasoning": 0.735, "roleplay": 0.819, "stem": 0.858, "writing": 0.737}, "en_post": {"AIME": 0.2, "GPQA": 0.434, "HellaSwag": 0.859, "LCB": 0.001, "MATH500": 0.824, "MMLU-Pro": 0.654, "avg": 0.495}, "ja_mtb": {"avg": 0.778, "coding": 0.799, "extraction": 0.926, "humanities": 0.805, "math": 0.883, "reasoning": 0.646, "roleplay": 0.718, "stem": 0.758, "writing": 0.686}, "ja_post": {"GPQA": 0.35, "JEMHopQA": 0.537, "JHumanEval": 0.001, "MATH100": 0.818, "MMLU-ProX": 0.606, "__MIFEvalJa": 0.624, "avg": 0.463}}, "sortkey": "medgemma", "url": "https://huggingface.co/google/medgemma-27b-it"}, "gpt-4.1-2025-04-14": {"active_params": 0, "date": "2025-04-14", "family": "GPT-4.1", "id": "gpt-4.1-2025-04-14", "is_post": true, "model_id": "gpt-4.1-2025-04-14", "name": "GPT-4.1 (gpt-4.1-2025-04-14)", "params": 0, "pre_training": "(private)", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.908, "coding": 0.898, "extraction": 0.936, "humanities": 0.903, "math": 0.942, "reasoning": 0.863, "roleplay": 0.901, "stem": 0.925, "writing": 0.898}, "en_post": {"AIME": 0.4, "GPQA": 0.667, "HellaSwag": 0.94, "LCB": 0.387, "MATH500": 0.906, "MMLU-Pro": 0.813, "avg": 0.685}, "ja_mtb": {"avg": 0.892, "coding": 0.917, "extraction": 0.911, "humanities": 0.885, "math": 0.98, "reasoning": 0.819, "roleplay": 0.879, "stem": 0.887, "writing": 0.858}, "ja_post": {"GPQA": 0.603, "JEMHopQA": 0.856, "JHumanEval": 0.911, "MATH100": 0.899, "MMLU-ProX": 0.772, "__MIFEvalJa": 0.81, "avg": 0.808}}, "sortkey": "gpt 4.1 (gpt 4.1 2025 04 14)", "url": "https://platform.openai.com/docs/models"}, "gpt-4o-2024-08-06": {"active_params": 0, "date": "2024-08-06", "family": "GPT-4o", "id": "gpt-4o-2024-08-06", "is_post": true, "model_id": "gpt-4o-2024-08-06", "name": "GPT-4o (gpt-4o-2024-08-06)", "params": 0, "pre_training": "(private)", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.922, "coding": 0.943, "extraction": 0.927, "humanities": 0.896, "math": 0.993, "reasoning": 0.976, "roleplay": 0.874, "stem": 0.905, "writing": 0.865}, "en_post": {"AIME": 0.083, "GPQA": 0.556, "HellaSwag": 0.93, "LCB": 0.25, "MATH500": 0.792, "MMLU-Pro": 0.749, "avg": 0.56}, "ja_mtb": {"avg": 0.865, "coding": 0.896, "extraction": 0.929, "humanities": 0.874, "math": 0.895, "reasoning": 0.755, "roleplay": 0.869, "stem": 0.847, "writing": 0.855}, "ja_post": {"GPQA": 0.453, "JEMHopQA": 0.813, "JHumanEval": 0.844, "MATH100": 0.758, "MMLU-ProX": 0.685, "__MIFEvalJa": 0.704, "avg": 0.71}}, "sortkey": "gpt 4o (gpt 4o 2024 08 06)", "url": "https://platform.openai.com/docs/models"}, "gpt-5-2025-08-07": {"active_params": 0, "date": "2025-08-07", "family": "GPT-5", "id": "gpt-5-2025-08-07", "is_post": true, "model_id": "gpt-5-2025-08-07", "name": "GPT-5 (gpt-5-2025-08-07)", "params": 0, "pre_training": "(private)", "reasoning": "on (middle)", "results": {"en_mtb": {"avg": 0.888, "coding": 0.876, "extraction": 0.912, "humanities": 0.923, "math": 0.843, "reasoning": 0.811, "roleplay": 0.906, "stem": 0.927, "writing": 0.904}, "en_post": {"AIME": 0.933, "GPQA": 0.828, "HellaSwag": 0.959, "LCB": 0.677, "MATH500": 0.99, "MMLU-Pro": 0.865, "avg": 0.875}, "ja_mtb": {"avg": 0.882, "coding": 0.893, "extraction": 0.883, "humanities": 0.928, "math": 0.882, "reasoning": 0.758, "roleplay": 0.896, "stem": 0.933, "writing": 0.885}, "ja_post": {"GPQA": 0.786, "JEMHopQA": 0.9, "JHumanEval": 0.943, "MATH100": 0.98, "MMLU-ProX": 0.849, "__MIFEvalJa": 0.907, "avg": 0.891}}, "sortkey": "gpt 5 (gpt 5 2025 08 07)", "url": "https://platform.openai.com/docs/models"}, "llm-jp/llm-jp-3.1-1.8b-instruct4": {"active_params": 1.8, "date": "2025-05-30", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3.1-1.8b-instruct4", "is_post": true, "model_id": "llm-jp/llm-jp-3.1-1.8b-instruct4", "name": "llm-jp-3.1-1.8b-instruct4", "params": 1.8, "pre_training": "llm-jp-3.1-1.8b", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.548, "coding": 0.454, "extraction": 0.482, "humanities": 0.662, "math": 0.521, "reasoning": 0.364, "roleplay": 0.665, "stem": 0.563, "writing": 0.673}, "en_post": {"AIME": 0.0, "GPQA": 0.278, "HellaSwag": 0.45, "LCB": 0.03, "MATH500": 0.146, "MMLU-Pro": 0.163, "avg": 0.178}, "ja_mtb": {"avg": 0.657, "coding": 0.574, "extraction": 0.601, "humanities": 0.809, "math": 0.672, "reasoning": 0.446, "roleplay": 0.767, "stem": 0.697, "writing": 0.693}, "ja_post": {"GPQA": 0.239, "JEMHopQA": 0.342, "JHumanEval": 0.365, "MATH100": 0.212, "MMLU-ProX": 0.195, "__MIFEvalJa": 0.288, "avg": 0.271}}, "sortkey": "llm jp 3.1 4", "url": "https://huggingface.co/llm-jp/llm-jp-3.1-1.8b-instruct4"}, "llm-jp/llm-jp-3.1-13b-instruct4": {"active_params": 14, "date": "2025-05-30", "family": "llm-jp-3", "id": "llm-jp/llm-jp-3.1-13b-instruct4", "is_post": true, "model_id": "llm-jp/llm-jp-3.1-13b-instruct4", "name": "llm-jp-3.1-13b-instruct4", "params": 14, "pre_training": "llm-jp-3.1-13b", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.682, "coding": 0.562, "extraction": 0.681, "humanities": 0.844, "math": 0.625, "reasoning": 0.512, "roleplay": 0.736, "stem": 0.715, "writing": 0.779}, "en_post": {"AIME": 0.0, "GPQA": 0.227, "HellaSwag": 0.717, "LCB": 0.082, "MATH500": 0.188, "MMLU-Pro": 0.252, "avg": 0.244}, "ja_mtb": {"avg": 0.733, "coding": 0.587, "extraction": 0.7, "humanities": 0.87, "math": 0.731, "reasoning": 0.559, "roleplay": 0.831, "stem": 0.775, "writing": 0.807}, "ja_post": {"GPQA": 0.23, "JEMHopQA": 0.698, "JHumanEval": 0.463, "MATH100": 0.232, "MMLU-ProX": 0.296, "__MIFEvalJa": 0.372, "avg": 0.384}}, "sortkey": "llm jp 3.1 4", "url": "https://huggingface.co/llm-jp/llm-jp-3.1-13b-instruct4"}, "meta-llama/Llama-3.3-70B-Instruct": {"active_params": 70, "date": "2024-12-06", "family": "Llama 3", "id": "meta-llama/Llama-3.3-70B-Instruct", "is_post": true, "model_id": "meta-llama/Llama-3.3-70B-Instruct", "name": "Llama 3.3 70B Instruct", "params": 70, "pre_training": "Llama 3.1 70B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.863, "coding": 0.795, "extraction": 0.935, "humanities": 0.891, "math": 0.895, "reasoning": 0.861, "roleplay": 0.858, "stem": 0.822, "writing": 0.847}, "en_post": {"AIME": 0.117, "GPQA": 0.48, "HellaSwag": 0.911, "LCB": 0.303, "MATH500": 0.746, "MMLU-Pro": 0.717, "avg": 0.545}, "ja_mtb": {"avg": 0.735, "coding": 0.672, "extraction": 0.878, "humanities": 0.751, "math": 0.742, "reasoning": 0.638, "roleplay": 0.762, "stem": 0.735, "writing": 0.7}, "ja_post": {"GPQA": 0.453, "JEMHopQA": 0.557, "JHumanEval": 0.752, "MATH100": 0.646, "MMLU-ProX": 0.607, "__MIFEvalJa": 0.65, "avg": 0.603}}, "sortkey": "llama 3.3", "url": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"}, "meta-llama/Llama-4-Scout-17B-16E-Instruct": {"active_params": 17, "date": "2025-04-04", "family": "Llama 4", "id": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "is_post": true, "model_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "name": "Llama 4 Scout Instruct", "params": 109, "pre_training": "Llama 4 Scout", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.857, "coding": 0.722, "extraction": 0.911, "humanities": 0.86, "math": 0.92, "reasoning": 0.904, "roleplay": 0.836, "stem": 0.84, "writing": 0.862}, "en_post": {"AIME": 0.183, "GPQA": 0.606, "HellaSwag": 0.891, "LCB": 0.309, "MATH500": 0.834, "MMLU-Pro": 0.744, "avg": 0.594}, "ja_mtb": {"avg": 0.789, "coding": 0.763, "extraction": 0.923, "humanities": 0.816, "math": 0.879, "reasoning": 0.615, "roleplay": 0.787, "stem": 0.752, "writing": 0.778}, "ja_post": {"GPQA": 0.54, "JEMHopQA": 0.512, "JHumanEval": 0.82, "MATH100": 0.758, "MMLU-ProX": 0.687, "__MIFEvalJa": 0.611, "avg": 0.663}}, "sortkey": "llama 4 scout", "url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"}, "meta-llama/Meta-Llama-3.1-8B-Instruct": {"active_params": 8.0, "date": "2024-07-23", "family": "Llama 3", "id": "meta-llama/Meta-Llama-3.1-8B-Instruct", "is_post": true, "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct", "name": "Llama 3.1 8B Instruct", "params": 8.0, "pre_training": "Llama 3.1 8B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.737, "coding": 0.556, "extraction": 0.816, "humanities": 0.871, "math": 0.697, "reasoning": 0.522, "roleplay": 0.821, "stem": 0.765, "writing": 0.85}, "en_post": {"AIME": 0.033, "GPQA": 0.374, "HellaSwag": 0.769, "LCB": 0.131, "MATH500": 0.526, "MMLU-Pro": 0.489, "avg": 0.387}, "ja_mtb": {"avg": 0.592, "coding": 0.528, "extraction": 0.848, "humanities": 0.585, "math": 0.6, "reasoning": 0.465, "roleplay": 0.569, "stem": 0.562, "writing": 0.577}, "ja_post": {"GPQA": 0.261, "JEMHopQA": 0.482, "JHumanEval": 0.58, "MATH100": 0.384, "MMLU-ProX": 0.306, "__MIFEvalJa": 0.381, "avg": 0.403}}, "sortkey": "llama 3.1", "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"}, "microsoft/phi-4": {"active_params": 15, "date": "2024-12-13", "family": "Phi 4", "id": "microsoft/phi-4", "is_post": true, "model_id": "microsoft/phi-4", "name": "Phi-4", "params": 15, "pre_training": "(private)", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.881, "coding": 0.771, "extraction": 0.904, "humanities": 0.876, "math": 0.928, "reasoning": 0.933, "roleplay": 0.889, "stem": 0.879, "writing": 0.865}, "en_post": {"AIME": 0.217, "GPQA": 0.551, "HellaSwag": 0.859, "LCB": 0.227, "MATH500": 0.8, "MMLU-Pro": 0.63, "avg": 0.547}, "ja_mtb": {"avg": 0.822, "coding": 0.752, "extraction": 0.933, "humanities": 0.862, "math": 0.89, "reasoning": 0.629, "roleplay": 0.83, "stem": 0.845, "writing": 0.835}, "ja_post": {"GPQA": 0.435, "JEMHopQA": 0.589, "JHumanEval": 0.77, "MATH100": 0.798, "MMLU-ProX": 0.638, "__MIFEvalJa": 0.438, "avg": 0.646}}, "sortkey": "phi 4", "url": "https://huggingface.co/microsoft/phi-4"}, "microsoft/phi-4-reasoning-plus": {"active_params": 15, "date": "2025-04-30", "family": "Phi 4", "id": "microsoft/phi-4-reasoning-plus", "is_post": true, "model_id": "microsoft/phi-4-reasoning-plus", "name": "Phi-4-reasoning-plus", "params": 15, "pre_training": "(private)", "reasoning": "on", "results": {"en_mtb": {"avg": 0.426, "coding": 0.281, "extraction": 0.384, "humanities": 0.116, "math": 0.437, "reasoning": 0.322, "roleplay": 0.769, "stem": 0.299, "writing": 0.8}, "en_post": {"AIME": 0.583, "GPQA": 0.611, "HellaSwag": 0.26, "LCB": 0.478, "MATH500": 0.77, "MMLU-Pro": 0.113, "avg": 0.469}, "ja_mtb": {"avg": 0.374, "coding": 0.205, "extraction": 0.376, "humanities": 0.206, "math": 0.379, "reasoning": 0.283, "roleplay": 0.643, "stem": 0.162, "writing": 0.741}, "ja_post": {"GPQA": 0.563, "JEMHopQA": 0.015, "JHumanEval": 0.751, "MATH100": 0.737, "MMLU-ProX": 0.118, "__MIFEvalJa": 0.221, "avg": 0.437}}, "sortkey": "phi 4 reasoning plus", "url": "https://huggingface.co/microsoft/phi-4-reasoning-plus"}, "nvidia/Llama-3.1-Nemotron-Nano-8B-v1": {"active_params": 8.0, "date": "2025-03-18", "family": "Llama 3", "id": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1", "is_post": true, "model_id": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1", "name": "Llama-3.1-Nemotron-Nano-8B-v1", "params": 8.0, "pre_training": "Llama 3.1 8B", "reasoning": "on", "results": {"en_mtb": {"avg": 0.701, "coding": 0.658, "extraction": 0.654, "humanities": 0.696, "math": 0.906, "reasoning": 0.526, "roleplay": 0.712, "stem": 0.738, "writing": 0.72}, "en_post": {"AIME": 0.55, "GPQA": 0.47, "HellaSwag": 0.518, "LCB": 0.478, "MATH500": 0.948, "MMLU-Pro": 0.566, "avg": 0.588}, "ja_mtb": {"avg": 0.363, "coding": 0.374, "extraction": 0.503, "humanities": 0.311, "math": 0.564, "reasoning": 0.27, "roleplay": 0.289, "stem": 0.301, "writing": 0.293}, "ja_post": {"GPQA": 0.339, "JEMHopQA": 0.202, "JHumanEval": 0.802, "MATH100": 0.919, "MMLU-ProX": 0.489, "__MIFEvalJa": 0.186, "avg": 0.55}}, "sortkey": "llama 3.1 nemotron nano v1", "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1"}, "nvidia/Llama-3_3-Nemotron-Super-49B-v1": {"active_params": 50, "date": "2025-03-18", "family": "Llama 3", "id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1", "is_post": true, "model_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1", "name": "Llama-3.3-Nemotron-Super-49B-v1", "params": 50, "pre_training": "Llama 3.3 70B Instruct", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.881, "coding": 0.782, "extraction": 0.915, "humanities": 0.91, "math": 0.963, "reasoning": 0.8, "roleplay": 0.878, "stem": 0.908, "writing": 0.893}, "en_post": {"AIME": 0.567, "GPQA": 0.667, "HellaSwag": 0.885, "LCB": 0.408, "MATH500": 0.96, "MMLU-Pro": 0.783, "avg": 0.711}, "ja_mtb": {"avg": 0.806, "coding": 0.731, "extraction": 0.898, "humanities": 0.821, "math": 0.801, "reasoning": 0.755, "roleplay": 0.804, "stem": 0.809, "writing": 0.828}, "ja_post": {"GPQA": 0.531, "JEMHopQA": 0.541, "JHumanEval": 0.9, "MATH100": 0.919, "MMLU-ProX": 0.687, "__MIFEvalJa": 0.558, "avg": 0.716}}, "sortkey": "llama 3.3 nemotron super v1", "url": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1"}, "o3-2025-04-16": {"active_params": 0, "date": "2025-04-16", "family": "o3", "id": "o3-2025-04-16", "is_post": true, "model_id": "o3-2025-04-16", "name": "o3 (o3-2025-04-16)", "params": 0, "pre_training": "(private)", "reasoning": "on (middle)", "results": {"en_mtb": {"avg": 0.917, "coding": 0.929, "extraction": 0.931, "humanities": 0.945, "math": 0.964, "reasoning": 0.836, "roleplay": 0.9, "stem": 0.938, "writing": 0.892}, "en_post": {"AIME": 0.817, "GPQA": 0.818, "HellaSwag": 0.956, "LCB": 0.649, "MATH500": 0.978, "MMLU-Pro": 0.857, "avg": 0.846}, "ja_mtb": {"avg": 0.903, "coding": 0.935, "extraction": 0.898, "humanities": 0.888, "math": 0.995, "reasoning": 0.809, "roleplay": 0.889, "stem": 0.941, "writing": 0.867}, "ja_post": {"GPQA": 0.766, "JEMHopQA": 0.852, "JHumanEval": 0.929, "MATH100": 0.97, "MMLU-ProX": 0.835, "__MIFEvalJa": 0.85, "avg": 0.87}}, "sortkey": "o3 (o3 2025 04 16)", "url": "https://platform.openai.com/docs/models"}, "o3-mini-2025-01-31": {"active_params": 0, "date": "2025-01-31", "family": "o3", "id": "o3-mini-2025-01-31", "is_post": true, "model_id": "o3-mini-2025-01-31", "name": "o3-mini (o3-mini-2025-01-31)", "params": 0, "pre_training": "(private)", "reasoning": "on (middle)", "results": {"en_mtb": {"avg": 0.901, "coding": 0.876, "extraction": 0.913, "humanities": 0.891, "math": 0.969, "reasoning": 0.865, "roleplay": 0.895, "stem": 0.914, "writing": 0.882}, "en_post": {"AIME": 0.733, "GPQA": 0.747, "HellaSwag": 0.869, "LCB": 0.503, "MATH500": 0.958, "MMLU-Pro": 0.792, "avg": 0.767}, "ja_mtb": {"avg": 0.88, "coding": 0.868, "extraction": 0.937, "humanities": 0.86, "math": 0.952, "reasoning": 0.802, "roleplay": 0.863, "stem": 0.893, "writing": 0.868}, "ja_post": {"GPQA": 0.685, "JEMHopQA": 0.607, "JHumanEval": 0.934, "MATH100": 0.939, "MMLU-ProX": 0.76, "__MIFEvalJa": 0.841, "avg": 0.785}}, "sortkey": "o3 mini (o3 mini 2025 01 31)", "url": "https://platform.openai.com/docs/models"}, "openai/gpt-oss-120b": {"active_params": 5.1, "date": "2025-08-05", "family": "gpt-oss", "id": "openai/gpt-oss-120b", "is_post": true, "model_id": "openai/gpt-oss-120b", "name": "gpt-oss-120b", "params": 120, "pre_training": "(private)", "reasoning": "on (middle)", "results": {"en_mtb": {"avg": 0.918, "coding": 0.947, "extraction": 0.892, "humanities": 0.915, "math": 0.989, "reasoning": 0.871, "roleplay": 0.886, "stem": 0.96, "writing": 0.886}, "en_post": {"AIME": 0.733, "GPQA": 0.727, "HellaSwag": 0.878, "LCB": 0.67, "MATH500": 0.966, "MMLU-Pro": 0.79, "avg": 0.794}, "ja_mtb": {"avg": 0.907, "coding": 0.898, "extraction": 0.924, "humanities": 0.915, "math": 0.999, "reasoning": 0.862, "roleplay": 0.855, "stem": 0.948, "writing": 0.852}, "ja_post": {"GPQA": 0.663, "JEMHopQA": 0.635, "JHumanEval": 0.925, "MATH100": 0.97, "MMLU-ProX": 0.756, "__MIFEvalJa": 0.735, "avg": 0.79}}, "sortkey": "gpt oss", "url": "https://huggingface.co/openai/gpt-oss-120b"}, "openai/gpt-oss-20b": {"active_params": 3.6, "date": "2025-08-05", "family": "gpt-oss", "id": "openai/gpt-oss-20b", "is_post": true, "model_id": "openai/gpt-oss-20b", "name": "gpt-oss-20b", "params": 22, "pre_training": "(private)", "reasoning": "on (middle)", "results": {"en_mtb": {"avg": 0.889, "coding": 0.913, "extraction": 0.881, "humanities": 0.935, "math": 0.913, "reasoning": 0.779, "roleplay": 0.88, "stem": 0.939, "writing": 0.869}, "en_post": {"AIME": 0.617, "GPQA": 0.636, "HellaSwag": 0.847, "LCB": 0.635, "MATH500": 0.944, "MMLU-Pro": 0.741, "avg": 0.737}, "ja_mtb": {"avg": 0.869, "coding": 0.914, "extraction": 0.917, "humanities": 0.853, "math": 0.994, "reasoning": 0.772, "roleplay": 0.772, "stem": 0.909, "writing": 0.824}, "ja_post": {"GPQA": 0.571, "JEMHopQA": 0.506, "JHumanEval": 0.927, "MATH100": 0.929, "MMLU-ProX": 0.702, "__MIFEvalJa": 0.549, "avg": 0.727}}, "sortkey": "gpt oss", "url": "https://huggingface.co/openai/gpt-oss-20b"}, "sbintuitions/sarashina2.2-3b-instruct-v0.1": {"active_params": 3.4, "date": "2025-03-07", "family": "Sarashina2", "id": "sbintuitions/sarashina2.2-3b-instruct-v0.1", "is_post": true, "model_id": "sbintuitions/sarashina2.2-3b-instruct-v0.1", "name": "Sarashina2.2 3B Instruct v0.1", "params": 3.4, "pre_training": "Sarashina2.2 3B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.708, "coding": 0.499, "extraction": 0.642, "humanities": 0.863, "math": 0.747, "reasoning": 0.552, "roleplay": 0.783, "stem": 0.827, "writing": 0.75}, "en_post": {"AIME": 0.017, "GPQA": 0.293, "HellaSwag": 0.613, "LCB": 0.086, "MATH500": 0.57, "MMLU-Pro": 0.329, "avg": 0.318}, "ja_mtb": {"avg": 0.721, "coding": 0.579, "extraction": 0.68, "humanities": 0.862, "math": 0.828, "reasoning": 0.467, "roleplay": 0.832, "stem": 0.766, "writing": 0.752}, "ja_post": {"GPQA": 0.301, "JEMHopQA": 0.434, "JHumanEval": 0.464, "MATH100": 0.465, "MMLU-ProX": 0.335, "__MIFEvalJa": 0.288, "avg": 0.4}}, "sortkey": "sarashina2.2 v0.1", "url": "https://huggingface.co/sbintuitions/sarashina2.2-3b-instruct-v0.1"}, "tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1": {"active_params": 27, "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1", "is_post": true, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1", "name": "Gemma-2-Llama Swallow 27B IT", "params": 27, "pre_training": "Gemma 2 27B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.771, "coding": 0.626, "extraction": 0.789, "humanities": 0.883, "math": 0.751, "reasoning": 0.622, "roleplay": 0.824, "stem": 0.821, "writing": 0.853}, "en_post": {"AIME": 0.033, "GPQA": 0.343, "HellaSwag": 0.786, "LCB": 0.218, "MATH500": 0.544, "MMLU-Pro": 0.436, "avg": 0.393}, "ja_mtb": {"avg": 0.759, "coding": 0.627, "extraction": 0.846, "humanities": 0.868, "math": 0.767, "reasoning": 0.548, "roleplay": 0.796, "stem": 0.785, "writing": 0.833}, "ja_post": {"GPQA": 0.333, "JEMHopQA": 0.681, "JHumanEval": 0.656, "MATH100": 0.465, "MMLU-ProX": 0.452, "__MIFEvalJa": 0.54, "avg": 0.517}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-27b-it-v0.1"}, "tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1": {"active_params": 2.6, "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1", "is_post": true, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1", "name": "Gemma-2-Llama Swallow 2B IT", "params": 2.6, "pre_training": "Gemma2-Llama Swallow 2B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.584, "coding": 0.461, "extraction": 0.534, "humanities": 0.758, "math": 0.376, "reasoning": 0.452, "roleplay": 0.728, "stem": 0.646, "writing": 0.715}, "en_post": {"AIME": 0.0, "GPQA": 0.268, "HellaSwag": 0.495, "LCB": 0.036, "MATH500": 0.138, "MMLU-Pro": 0.169, "avg": 0.184}, "ja_mtb": {"avg": 0.583, "coding": 0.408, "extraction": 0.551, "humanities": 0.774, "math": 0.42, "reasoning": 0.418, "roleplay": 0.725, "stem": 0.655, "writing": 0.709}, "ja_post": {"GPQA": 0.259, "JEMHopQA": 0.274, "JHumanEval": 0.241, "MATH100": 0.263, "MMLU-ProX": 0.19, "__MIFEvalJa": 0.363, "avg": 0.245}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-2b-it-v0.1"}, "tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1": {"active_params": 9.2, "date": "2025-05-19", "family": "Gemma 2", "id": "tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1", "is_post": true, "model_id": "tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1", "name": "Gemma-2-Llama Swallow 9B IT", "params": 9.2, "pre_training": "Gemma 2 9B", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.734, "coding": 0.523, "extraction": 0.831, "humanities": 0.886, "math": 0.72, "reasoning": 0.518, "roleplay": 0.789, "stem": 0.786, "writing": 0.819}, "en_post": {"AIME": 0.017, "GPQA": 0.283, "HellaSwag": 0.801, "LCB": 0.106, "MATH500": 0.438, "MMLU-Pro": 0.296, "avg": 0.323}, "ja_mtb": {"avg": 0.729, "coding": 0.579, "extraction": 0.787, "humanities": 0.88, "math": 0.661, "reasoning": 0.616, "roleplay": 0.788, "stem": 0.735, "writing": 0.783}, "ja_post": {"GPQA": 0.283, "JEMHopQA": 0.465, "JHumanEval": 0.518, "MATH100": 0.374, "MMLU-ProX": 0.372, "__MIFEvalJa": 0.54, "avg": 0.402}}, "sortkey": "gemma 2 llama swallow", "url": "https://huggingface.co/tokyotech-llm/Gemma-2-Llama-Swallow-9b-it-v0.1"}, "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3": {"active_params": 8.0, "date": "2024-12-23", "family": "Llama 3", "id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3", "is_post": true, "model_id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3", "name": "Llama 3.1 Swallow 8B Instruct v0.3", "params": 8.0, "pre_training": "Llama 3.1 Swallow 8B v0.2", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.691, "coding": 0.528, "extraction": 0.714, "humanities": 0.886, "math": 0.562, "reasoning": 0.458, "roleplay": 0.773, "stem": 0.768, "writing": 0.838}, "en_post": {"AIME": 0.0, "GPQA": 0.293, "HellaSwag": 0.725, "LCB": 0.102, "MATH500": 0.338, "MMLU-Pro": 0.287, "avg": 0.291}, "ja_mtb": {"avg": 0.709, "coding": 0.57, "extraction": 0.783, "humanities": 0.869, "math": 0.631, "reasoning": 0.506, "roleplay": 0.782, "stem": 0.716, "writing": 0.813}, "ja_post": {"GPQA": 0.239, "JEMHopQA": 0.549, "JHumanEval": 0.488, "MATH100": 0.364, "MMLU-ProX": 0.306, "__MIFEvalJa": 0.491, "avg": 0.389}}, "sortkey": "llama 3.1 swallow v0.3", "url": "https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3"}, "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5": {"active_params": 8.0, "date": "2025-06-25", "family": "Llama 3", "id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5", "is_post": true, "model_id": "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5", "name": "Llama 3.1 Swallow 8B Instruct v0.5", "params": 8.0, "pre_training": "Llama 3.1 Swallow 8B v0.2", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.753, "coding": 0.576, "extraction": 0.801, "humanities": 0.9, "math": 0.769, "reasoning": 0.499, "roleplay": 0.848, "stem": 0.796, "writing": 0.833}, "en_post": {"AIME": 0.0, "GPQA": 0.318, "HellaSwag": 0.648, "LCB": 0.072, "MATH500": 0.452, "MMLU-Pro": 0.399, "avg": 0.315}, "ja_mtb": {"avg": 0.726, "coding": 0.59, "extraction": 0.843, "humanities": 0.884, "math": 0.47, "reasoning": 0.618, "roleplay": 0.78, "stem": 0.799, "writing": 0.822}, "ja_post": {"GPQA": 0.295, "JEMHopQA": 0.602, "JHumanEval": 0.584, "MATH100": 0.404, "MMLU-ProX": 0.369, "__MIFEvalJa": 0.496, "avg": 0.451}}, "sortkey": "llama 3.1 swallow v0.5", "url": "https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5"}, "tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4": {"active_params": 70, "date": "2025-03-10", "family": "Llama 3", "id": "tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4", "is_post": true, "model_id": "tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4", "name": "Llama 3.3 Swallow 70B Instruct v0.4", "params": 70, "pre_training": "Llama 3.3 Swallow 70B v0.4", "reasoning": "N/A", "results": {"en_mtb": {"avg": 0.816, "coding": 0.672, "extraction": 0.902, "humanities": 0.888, "math": 0.839, "reasoning": 0.706, "roleplay": 0.828, "stem": 0.838, "writing": 0.855}, "en_post": {"AIME": 0.083, "GPQA": 0.409, "HellaSwag": 0.884, "LCB": 0.232, "MATH500": 0.642, "MMLU-Pro": 0.57, "avg": 0.47}, "ja_mtb": {"avg": 0.791, "coding": 0.696, "extraction": 0.856, "humanities": 0.881, "math": 0.807, "reasoning": 0.664, "roleplay": 0.827, "stem": 0.772, "writing": 0.822}, "ja_post": {"GPQA": 0.355, "JEMHopQA": 0.658, "JHumanEval": 0.727, "MATH100": 0.697, "MMLU-ProX": 0.533, "__MIFEvalJa": 0.593, "avg": 0.594}}, "sortkey": "llama 3.3 swallow v0.4", "url": "https://huggingface.co/tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4"}};
const g_tasks = {"en_mtb": {"description": "English MT-Bench evaluates multi-turn dialogue capabilities. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["post"], "taskdict": {"avg": {"category": "Average", "collective": true, "description": "Average score of eight tasks of English MT-bench", "field": "english_mtbench_average", "metric": "Average", "name": "avg", "short": "MTB (en) avg", "title": "Average of English MT-Bench"}, "coding": {"description": "Implementing algorithms in Python or C++, and creating websites using HTML.", "field": "english_mtbench_coding", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "coding", "short": "Code", "title": "Coding"}, "extraction": {"description": "Extracting named entities (such as author names and numerical values) and sentiment (e.g., positive or negative) from text.", "field": "english_mtbench_extraction", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "extraction", "short": "Ext", "title": "Extraction"}, "humanities": {"description": "Creating essays and strategies on topics related to law, economics, history, philosophy, and education.", "field": "english_mtbench_humanities", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "humanities", "short": "Human", "title": "Humanities"}, "math": {"description": "Generating solutions for problems and word problems in algebra, geometry, probability, and number theory.", "field": "english_mtbench_math", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "math", "short": "Math", "title": "Math"}, "reasoning": {"description": "Generating answers to questions by leveraging common knowledge and reasoning skills.", "field": "english_mtbench_reasoning", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "reasoning", "short": "Reason", "title": "Reasoning"}, "roleplay": {"description": "Writing creative texts by assuming the persona of famous individuals or fictional characters and imagining hypothetical scenarios.", "field": "english_mtbench_roleplay", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "roleplay", "short": "Roleplay", "title": "Roleplay"}, "stem": {"description": "Generating answers and explanations on topics related to physics, chemistry, biology, geography, architecture, and machine learning.", "field": "english_mtbench_stem", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "stem", "short": "STEM", "title": "STEM"}, "writing": {"description": "Writing blog articles, email drafts, and fictional narratives.", "field": "english_mtbench_writing", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "writing", "short": "Write", "title": "Writing"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of eight tasks of English MT-bench", "field": "english_mtbench_average", "metric": "Average", "name": "avg", "short": "MTB (en) avg", "title": "Average of English MT-Bench"}, {"description": "Implementing algorithms in Python or C++, and creating websites using HTML.", "field": "english_mtbench_coding", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "coding", "short": "Code", "title": "Coding"}, {"description": "Extracting named entities (such as author names and numerical values) and sentiment (e.g., positive or negative) from text.", "field": "english_mtbench_extraction", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "extraction", "short": "Ext", "title": "Extraction"}, {"description": "Creating essays and strategies on topics related to law, economics, history, philosophy, and education.", "field": "english_mtbench_humanities", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "humanities", "short": "Human", "title": "Humanities"}, {"description": "Generating solutions for problems and word problems in algebra, geometry, probability, and number theory.", "field": "english_mtbench_math", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "math", "short": "Math", "title": "Math"}, {"description": "Generating answers to questions by leveraging common knowledge and reasoning skills.", "field": "english_mtbench_reasoning", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "reasoning", "short": "Reason", "title": "Reasoning"}, {"description": "Writing creative texts by assuming the persona of famous individuals or fictional characters and imagining hypothetical scenarios.", "field": "english_mtbench_roleplay", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "roleplay", "short": "Roleplay", "title": "Roleplay"}, {"description": "Generating answers and explanations on topics related to physics, chemistry, biology, geography, architecture, and machine learning.", "field": "english_mtbench_stem", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "stem", "short": "STEM", "title": "STEM"}, {"description": "Writing blog articles, email drafts, and fictional narratives.", "field": "english_mtbench_writing", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "writing", "short": "Write", "title": "Writing"}], "title": "\u82f1\u8a9e MT-Bench"}, "en_post": {"description": "This benchmark evaluates post-trained LLMs including reasoning models on English benchmark datasets. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["post"], "taskdict": {"AIME": {"category": "Mathematics", "description": "Qualification for the United States Mathematical Olympiad (USAMO)", "field": "aime_2024_2025", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "metric": "Accuracy", "name": "AIME", "short": "AIME 24-25", "title": "AIME 2024-2025"}, "GPQA": {"category": "Science", "description": "Graduate-level Google-proof question answering", "field": "gpqa_diamond", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Rein et al.", "href": "https://openreview.net/forum?id=Ti67584b98", "year": 2024}, "metric": "Accuracy", "name": "GPQA", "short": "GPQA (en)", "title": "GPQA (English)"}, "HellaSwag": {"category": "Natural language inference", "description": "Four-choice questions to predict the next event", "field": "swallow_hellaswag", "format": "Multiple choice", "input": "zero-shot", "link": {"author": "Zellers et al.", "href": "https://aclanthology.org/P19-1472/", "year": 2019}, "metric": "Accuracy", "name": "HellaSwag", "short": "HellaSwag", "title": "HellaSwag"}, "LCB": {"category": "Coding", "description": "Contests across competition platforms (LeetCode, AtCoder, and CodeForces)", "field": "livecodebench_v5_v6_pass@1", "format": "Free text", "input": "zero-shot", "metric": "Pass@1 (n=10)", "name": "LCB", "short": "LCB", "title": "LiveCodeBench"}, "MATH500": {"category": "Mathematics", "description": "Competition-level mathmatics", "field": "math_500", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Hendrycks et al.", "href": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html", "year": 2021}, "metric": "Accuracy", "name": "MATH500", "short": "MATH-500 (en)", "title": "MATH-500 (English)"}, "MMLU-Pro": {"category": "College-level exam", "description": "Proficient-level multi-discipline language understanding and reasoning", "field": "mmlu_pro_english", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Wang et al.", "href": "https://neurips.cc/virtual/2024/poster/97435", "year": 2024}, "metric": "Accuracy", "name": "MMLU-Pro", "short": "MMLU-Pro (en)", "title": "MMLU-Pro (English)"}, "avg": {"category": "Average", "collective": true, "description": "Average score of English datasets for post-trained models", "field": "en_post_avg", "metric": "Average", "name": "avg", "short": "Post (en) avg", "title": "Post-trained (English) average"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of English datasets for post-trained models", "field": "en_post_avg", "metric": "Average", "name": "avg", "short": "Post (en) avg", "title": "Post-trained (English) average"}, {"category": "Natural language inference", "description": "Four-choice questions to predict the next event", "field": "swallow_hellaswag", "format": "Multiple choice", "input": "zero-shot", "link": {"author": "Zellers et al.", "href": "https://aclanthology.org/P19-1472/", "year": 2019}, "metric": "Accuracy", "name": "HellaSwag", "short": "HellaSwag", "title": "HellaSwag"}, {"category": "College-level exam", "description": "Proficient-level multi-discipline language understanding and reasoning", "field": "mmlu_pro_english", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Wang et al.", "href": "https://neurips.cc/virtual/2024/poster/97435", "year": 2024}, "metric": "Accuracy", "name": "MMLU-Pro", "short": "MMLU-Pro (en)", "title": "MMLU-Pro (English)"}, {"category": "Science", "description": "Graduate-level Google-proof question answering", "field": "gpqa_diamond", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Rein et al.", "href": "https://openreview.net/forum?id=Ti67584b98", "year": 2024}, "metric": "Accuracy", "name": "GPQA", "short": "GPQA (en)", "title": "GPQA (English)"}, {"category": "Mathematics", "description": "Competition-level mathmatics", "field": "math_500", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Hendrycks et al.", "href": "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html", "year": 2021}, "metric": "Accuracy", "name": "MATH500", "short": "MATH-500 (en)", "title": "MATH-500 (English)"}, {"category": "Mathematics", "description": "Qualification for the United States Mathematical Olympiad (USAMO)", "field": "aime_2024_2025", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "metric": "Accuracy", "name": "AIME", "short": "AIME 24-25", "title": "AIME 2024-2025"}, {"category": "Coding", "description": "Contests across competition platforms (LeetCode, AtCoder, and CodeForces)", "field": "livecodebench_v5_v6_pass@1", "format": "Free text", "input": "zero-shot", "metric": "Pass@1 (n=10)", "name": "LCB", "short": "LCB", "title": "LiveCodeBench"}], "title": "Post-trained (English)"}, "ja_mtb": {"description": "The Japanese version of MT-Bench (Nejumi LLM Leaderboard edition) evaluates multi-turn dialogue capabilities. The test questions are based on v4, and the reference answers are derived from v2 with corrections to incorrect responses. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["post"], "taskdict": {"avg": {"category": "Average", "collective": true, "description": "Average score of eight tasks of Japanese MT-bench", "field": "japanese_mtbench_average", "metric": "Average", "name": "avg", "short": "MTB (ja) avg", "title": "Average of Japanese MT-Bench"}, "coding": {"description": "Implementing algorithms in Python or C++, and creating websites using HTML.", "field": "japanese_mtbench_coding", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "coding", "short": "Code", "title": "Coding"}, "extraction": {"description": "Extracting named entities (such as author names and numerical values) and sentiment (e.g., positive or negative) from text.", "field": "japanese_mtbench_extraction", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "extraction", "short": "Ext", "title": "Extraction"}, "humanities": {"description": "Creating essays and strategies on topics related to law, economics, history, philosophy, and education.", "field": "japanese_mtbench_humanities", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "humanities", "short": "Human", "title": "Humanities"}, "math": {"description": "Generating solutions for problems and word problems in algebra, geometry, probability, and number theory.", "field": "japanese_mtbench_math", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "math", "short": "Math", "title": "Math"}, "reasoning": {"description": "Generating answers to questions by leveraging common knowledge and reasoning skills.", "field": "japanese_mtbench_reasoning", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "reasoning", "short": "Reason", "title": "Reasoning"}, "roleplay": {"description": "Writing creative texts by assuming the persona of famous individuals or fictional characters and imagining hypothetical scenarios.", "field": "japanese_mtbench_roleplay", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "roleplay", "short": "Roleplay", "title": "Roleplay"}, "stem": {"description": "Generating answers and explanations on topics related to physics, chemistry, biology, geography, architecture, and machine learning.", "field": "japanese_mtbench_stem", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "stem", "short": "STEM", "title": "STEM"}, "writing": {"description": "Writing blog articles, email drafts, and fictional narratives.", "field": "japanese_mtbench_writing", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "writing", "short": "Write", "title": "Writing"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of eight tasks of Japanese MT-bench", "field": "japanese_mtbench_average", "metric": "Average", "name": "avg", "short": "MTB (ja) avg", "title": "Average of Japanese MT-Bench"}, {"description": "Implementing algorithms in Python or C++, and creating websites using HTML.", "field": "japanese_mtbench_coding", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "coding", "short": "Code", "title": "Coding"}, {"description": "Extracting named entities (such as author names and numerical values) and sentiment (e.g., positive or negative) from text.", "field": "japanese_mtbench_extraction", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "extraction", "short": "Ext", "title": "Extraction"}, {"description": "Creating essays and strategies on topics related to law, economics, history, philosophy, and education.", "field": "japanese_mtbench_humanities", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "humanities", "short": "Human", "title": "Humanities"}, {"description": "Generating solutions for problems and word problems in algebra, geometry, probability, and number theory.", "field": "japanese_mtbench_math", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "math", "short": "Math", "title": "Math"}, {"description": "Generating answers to questions by leveraging common knowledge and reasoning skills.", "field": "japanese_mtbench_reasoning", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "reasoning", "short": "Reason", "title": "Reasoning"}, {"description": "Writing creative texts by assuming the persona of famous individuals or fictional characters and imagining hypothetical scenarios.", "field": "japanese_mtbench_roleplay", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "roleplay", "short": "Roleplay", "title": "Roleplay"}, {"description": "Generating answers and explanations on topics related to physics, chemistry, biology, geography, architecture, and machine learning.", "field": "japanese_mtbench_stem", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "stem", "short": "STEM", "title": "STEM"}, {"description": "Writing blog articles, email drafts, and fictional narratives.", "field": "japanese_mtbench_writing", "link": {"author": "Zheng et al.", "href": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html", "year": "2023"}, "metric": "Reference-guided grading by GPT-4o (gpt-4o-2024-08-06)", "name": "writing", "short": "Write", "title": "Writing"}], "title": "Japanese MT-Bench"}, "ja_post": {"description": "This benchmark evaluates post-trained LLMs including reasoning models on Japanese benchmark datasets. The evaluation scores range from 0 (lowest) to 1 (highest).", "for": ["post"], "taskdict": {"GPQA": {"category": "Science", "description": "Graduate-level Google-proof question answering", "field": "gpqa_main_ja", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Huang et al.", "href": "https://arxiv.org/abs/2502.07346", "year": 2025}, "metric": "Accuracy", "name": "GPQA", "short": "GPQA (ja)", "title": "GPQA (Japanese)"}, "JEMHopQA": {"category": "Multihop reasoning", "description": "Japanese explainable multi-hop question answering", "field": "jemhopqa_cot_f1_score_quasi", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Ishii et al.", "href": "https://aclanthology.org/2024.lrec-main.831/", "year": 2024}, "metric": "Character F1 (lenient)", "name": "JEMHopQA", "short": "JEMHopQA", "title": "JEMHopQA", "version": "v1.2"}, "JHumanEval": {"category": "Coding", "description": "Japanese translation of HumanEval (code genration benchmark)", "field": "jhumaneval_pass@1", "format": "Free text", "input": "zero-shot", "link": {"author": "Sato et al.", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P10-9.pdf", "year": 2024}, "metric": "Pass@1 (n=10)", "name": "JHumanEval", "short": "JHumanEval", "title": "JHumanEval"}, "MATH100": {"category": "Mathematics", "description": "Competition-level mathmatics", "field": "mclm_math_100_japanese", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Son et al.", "href": "https://aclanthology.org/2025.acl-long.699/", "year": 2025}, "metric": "Accuracy", "name": "MATH100", "short": "MATH-100 (ja)", "title": "MATH-100 (Japanese)"}, "MMLU-ProX": {"category": "College-level exam", "description": "Proficient-level multi-discipline language understanding and reasoning", "field": "mmlu_prox_japanese", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Xuan et al.", "href": "https://arxiv.org/abs/2503.10497", "year": 2025}, "metric": "Accuracy", "name": "MMLU-ProX", "short": "MMLU-ProX (ja)", "title": "MMLU-ProX (Japanese)"}, "__MIFEvalJa": {"category": "Instruction following", "description": "Controllability of instruction following", "exclude_from_avg": true, "field": "mifeval_ja_inst_level_strict_acc", "format": "Free text", "input": "zero-shot", "link": {"author": "Dussolle et al.", "href": "https://aclanthology.org/2025.findings-naacl.344/", "year": 2025}, "metric": "Accuracy", "name": "__MIFEvalJa", "remark": "Evaluation results of this task are excluded from average calculation.", "short": "M-IFEval-Ja", "title": "M-IFEval-Ja"}, "avg": {"category": "Average", "collective": true, "description": "Average score of Japanese datasets for post-trained models", "field": "ja_post_avg", "metric": "Average", "name": "avg", "short": "Post (ja) avg", "title": "Post-trained (Japanese) average"}}, "tasks": [{"category": "Average", "collective": true, "description": "Average score of Japanese datasets for post-trained models", "field": "ja_post_avg", "metric": "Average", "name": "avg", "short": "Post (ja) avg", "title": "Post-trained (Japanese) average"}, {"category": "Multihop reasoning", "description": "Japanese explainable multi-hop question answering", "field": "jemhopqa_cot_f1_score_quasi", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Ishii et al.", "href": "https://aclanthology.org/2024.lrec-main.831/", "year": 2024}, "metric": "Character F1 (lenient)", "name": "JEMHopQA", "short": "JEMHopQA", "title": "JEMHopQA", "version": "v1.2"}, {"category": "College-level exam", "description": "Proficient-level multi-discipline language understanding and reasoning", "field": "mmlu_prox_japanese", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Xuan et al.", "href": "https://arxiv.org/abs/2503.10497", "year": 2025}, "metric": "Accuracy", "name": "MMLU-ProX", "short": "MMLU-ProX (ja)", "title": "MMLU-ProX (Japanese)"}, {"category": "Science", "description": "Graduate-level Google-proof question answering", "field": "gpqa_main_ja", "format": "Multiple choice", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Huang et al.", "href": "https://arxiv.org/abs/2502.07346", "year": 2025}, "metric": "Accuracy", "name": "GPQA", "short": "GPQA (ja)", "title": "GPQA (Japanese)"}, {"category": "Mathematics", "description": "Competition-level mathmatics", "field": "mclm_math_100_japanese", "format": "Free text", "input": "zero-shot, chain-of-thought prompt", "link": {"author": "Son et al.", "href": "https://aclanthology.org/2025.acl-long.699/", "year": 2025}, "metric": "Accuracy", "name": "MATH100", "short": "MATH-100 (ja)", "title": "MATH-100 (Japanese)"}, {"category": "Coding", "description": "Japanese translation of HumanEval (code genration benchmark)", "field": "jhumaneval_pass@1", "format": "Free text", "input": "zero-shot", "link": {"author": "Sato et al.", "href": "https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/P10-9.pdf", "year": 2024}, "metric": "Pass@1 (n=10)", "name": "JHumanEval", "short": "JHumanEval", "title": "JHumanEval"}, {"category": "Instruction following", "description": "Controllability of instruction following", "exclude_from_avg": true, "field": "mifeval_ja_inst_level_strict_acc", "format": "Free text", "input": "zero-shot", "link": {"author": "Dussolle et al.", "href": "https://aclanthology.org/2025.findings-naacl.344/", "year": 2025}, "metric": "Accuracy", "name": "__MIFEvalJa", "remark": "Evaluation results of this task are excluded from average calculation.", "short": "M-IFEval-Ja", "title": "M-IFEval-Ja"}], "title": "Post-trained (Japanese)"}};
const g_updater = [];

var swallow_leaderboard_models = function(query, sortkey) {
  if ("name" in query) {
    // Return a single model specified by a name.
    return g_models[query.name];
  } else {

    let models = [];
    for (let key in g_models) {
      const model = g_models[key];

      // Filter by the include list.
      if ("includes" in query) {
        if (query.includes.includes(model['id'])) {
          models.push(model);
        }
        continue;
      }

      // Filter by the model family.
      if ("family" in query) {
        const family = model.family.toLowerCase();
        if (Array.isArray(query.family)) {
          let m = false;
          for (let f of query.family) {
            if (family.startsWith(f.toLowerCase())) {
              m = true;
              break;
            }
          }
          if (!m) {
            continue;
          }
        } else {
          if (!family.startsWith(query.family.toLowerCase())) {
            continue;
          }
        }
      }

      // Filter by the model size.
      if ("minp" in query && model['params'] < query.minp) {
        continue;
      }
      if ("maxp" in query && query.maxp <= model['params']) {
        continue;
      }

      // Filter by the exclude list.
      if ("excludes" in query && query.excludes.includes(model['id'])) {
        continue;
      }
      
      models.push(model);
    }

    // Sort models.
    if (sortkey !== undefined) {
      models.sort((a, b) => a.results[sortkey[0]][sortkey[1]] - b.results[sortkey[0]][sortkey[1]]);
    }

    return models;
  }
};

var swallow_leaderboard_get_taskdef = function(query) {
  for (let t of g_tasks[query[0]].tasks) {
    if (query[1] == t.name) {
      return t;
    }
  }
  return null;
};

var swallow_leaderboard_tasks = function(queries) {
  let tasks = [];

  for (let query of queries) {
    for (task of g_tasks[query[0]].tasks) {
      if (task.name == query[1] || (query[1] == "__ALL__" && !task.collective)) {
        tasks.push(task);
      }
    }
  }

  return tasks;
};

var swallow_leaderboard_chart_bar = function(element, config) {
  var onclick = function(chartContext, options) {
    config.sort = (config.sort + 1) % config.tasks.length;
    option = get_option(element, config);
    ApexCharts.exec(element, 'updateOptions', option, false, true);
  };

  var get_option = function(element, config) {
    const models = swallow_leaderboard_models(config.models, config.tasks[config.sort]);

    // Build series.
    let series = [];
    let labels = [];
    for (var query of config.tasks) {
      let data = [];
      for (var model of models) {
        data.push(model.results[query[0]][query[1]]);
      }
      const t = swallow_leaderboard_get_taskdef(query);
      series.push({name: t.title, data: data});
    }

    // Build labels.
    for (var model of models) {
      labels.push(model.name);
    }
  
    // Determine the orientation.
    const horizontal = (window.outerWidth < window.outerHeight);
    const height = horizontal ? Math.max(480, labels.length * 16 + 64) : 480;
    const padding = horizontal ? {} : { left: 48, right: 48, top: 0, bottom: 64 };
    return {
      chart: {
        type: "bar",
        id: element,
        fontFamily: 'inherit',
        height: height,
        parentHeightOffset: 0,
        animations: {
          enabled: true
        },
        events: {
          xAxisLabelClick: onclick,
        },
      },
      plotOptions: {
        bar: {
          horizontal: horizontal,
        },
      },
      grid: {
        padding: padding,
      },
      series: series,
      xaxis: {
        categories: labels,
        labels: {
          hideOverlappingLabels: false,
          style: {
//            fontSize: '9px',
          }
        }
      },
      yaxis: {
        tickAmount: 5,
        min: 0.,
        max: 1.,
        labels: {
          formatter: function(val) {
            return typeof val == "string" ? val : parseFloat(val).toFixed(1);
          }
        },
      },
      dataLabels: {
        enabled: false
      },
      stroke: {
        show: true,
        width: 2,
        colors: ['transparent']
      },
      legend: {
        position: 'top',
      },
      tooltip: {
        marker: {
          show: false,
        },
        y: {
          formatter: function(val) {
            return typeof val == "string" ? val : parseFloat(val).toFixed(4);
          }
        },
      },
      noData: {
        text: " (Select a model)",
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    }
  };

  // Create the bar chart.
  option = get_option(element, config);
  window.ApexCharts && (new ApexCharts(document.getElementById(element), option)).render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      ApexCharts.exec(element, 'updateOptions', option, false, true);
    }
  });
};

var swallow_leaderboard_chart_radar = function(element, config) {
  var get_option = function(element, config) {
    const models = swallow_leaderboard_models(config.models);

    // Build series.
    let series = [];
    var labels = [];
    for (var model of models) {
      let data = [];
      labels = [];

      for (let query of config.tasks) {
        for (let t of g_tasks[query[0]].tasks) {
          if (query[1] == t.name || (query[1] == "*" && !t.collective) || (query[1] == "+" && !t.collective && !t.exclude_from_avg)) {
            data.push(model.results[query[0]][t.name]);
            labels.push(t.short);
          }
        }
      }
      series.push({
        name: model.name,
        data: data,
      });
    }

    return {
      chart: {
        type: "radar",
        id: element,
        fontFamily: 'inherit',
        height: config.height,
        parentHeightOffset: 0,
      },
      series: series,
      xaxis: {
        categories: labels,
      },
      yaxis: {
        show: false,
        tickAmount: 5,
        min: 0.,
        max: 1.,
      },
      tooltip: {
        marker: {
          show: false,
        },
      },
      noData: {
        text: " (Select a model)",
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    };
  };
  
  // Create the radar chart.
  option = get_option(element, config);
  window.ApexCharts && (new ApexCharts(document.getElementById(element), option)).render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      ApexCharts.exec(element, 'updateOptions', option, false, true);
    }
  });
};

var swallow_leaderboard_chart_scatter = function(element, config) {
  var get_option = function(element, config) {
    const models = swallow_leaderboard_models(config.models);
    const xcat = config.xaxis[0];
    const xname = config.xaxis[1]
    const ycat = config.yaxis[0];
    const yname = config.yaxis[1];
    // There seems to be no way to obtain a color palette from ApexCharts,
    // so these colors are hard coded.
    const theme_colors = document.documentElement.getAttribute("data-bs-theme") == "dark" ? 
      ['#4ECDC4', '#C7F464', '#81D4FA', '#FD6A6A', '#546E7A']: 
      ['#008FFB', '#00E396', '#FEB019', '#FF4560', '#775DD0'];

    var format_params = function(val) {
      const v = Math.pow(2, parseFloat(val));
      return v < 10 ? v.toFixed(1) : v.toFixed(0);
    };

    var format_value = function(val) {
      return typeof val == "string" ? val : parseFloat(val).toFixed(4);
    };

    var build_axis = function(category, name) {
      if (category == "params" || category == "active_params") {
        
        title = category == "active_params" ? "Active parameters" : "Parameters";
        
        return {
          min: 0, // 2^0 = 1.
          max: 8, // 2^8 = 256.
          title: {text: title + " [B]"},
          tickAmount: 10,
          labels: {
            formatter: function(val) {
              const v = Math.pow(2, parseFloat(val));
              return v < 10 ? v.toFixed(1) : v.toFixed(0);
            }
          }
        }
      } else {
        return {
          min: 0,
          max: 1,
          title: {text: g_tasks[category].taskdict[name].title },
          tickAmount: 10,
          labels: {
            formatter: function(val) {
              return parseFloat(val).toFixed(1)
            }
          }
        }
      }
    };

    // Build series.
    let series = [];
    for (let model of models) {
      const x = (xcat == "params" || xcat == "active_params") ? Math.log2(model[xcat]) : model.results[xcat][xname];
      const y = (ycat == "params" || ycat == "active_params") ? Math.log2(model[ycat]) : model.results[ycat][yname];
      const family = model.family.toLowerCase();
      let color_index = 4;
      if (family.startsWith("gpt") || family.startsWith("o3") || family.startsWith("o4")) {
        color_index = 0;
      } else if (family.startsWith("llama")) {
        color_index = 1;
      } else if (family.startsWith("gemma")) {
        color_index = 2;
      } else if (family.startsWith("qwen")) {
        color_index = 3;
      }
      series.push({name: model.name, data: [[x, y]], color: theme_colors[color_index]});
    }
    const xaxis = build_axis(xcat, xname);
    const yaxis = build_axis(ycat, yname);

    return {
      chart: {
        type: "scatter",
        fontFamily: 'inherit',
        height: 480,
        parentHeightOffset: 0,
        toolbar: {
          tools: {
            download: true,
            selection: false,
            zoom: false,
            zoomin: false,
            zoomout: false,
            pan: false,
            reset: false,
          },
        },
        zoom: {
          enabled: false,
          allowMouseWheelZoom: false,
        },
      },
      series: series,
      xaxis: xaxis,
      yaxis: yaxis,
      legend: {
        show: false,
      },
      markers: {
        size: 4,
      },
      tooltip: {
        marker: {show: false},
        x: {formatter: (xcat == "params" || xcat == "active_params") ? format_params : format_value},
        y: {formatter: (ycat == "params" || ycat == "active_params") ? format_params : format_value},
      },
      theme: {
        mode: document.documentElement.getAttribute("data-bs-theme") == "dark" ? "dark" : "light",
      },
    };
  };

  // Create the radar chart.
  option = get_option(element, config);
  let obj_scatter = new ApexCharts(document.getElementById(element), option);
  console.log(obj_scatter);
  obj_scatter.render();

  // Register an update function.
  g_updater.push(function(target, updates) {
    if (target == "" || target == element) {
      Object.assign(config, updates);
      option = get_option(element, config);
      obj_scatter.updateOptions(option, false, true);
    }
  });
};

var swallow_leaderboard_chart = function(element, config) {
  //
  if (config.type == "bar") {
    swallow_leaderboard_chart_bar(element, config);
  } else if (config.type == "radar") {
    swallow_leaderboard_chart_radar(element, config);
  } else if (config.type == "scatter") {
    swallow_leaderboard_chart_scatter(element, config);
  }
};

var swallow_leaderboard_update = function(target, updates) {
  for (let i = 0; i < g_updater.length; ++i) {
    g_updater[i](target, updates);
  }
}
